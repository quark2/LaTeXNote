\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[all]{xy}

\newcommand{\NaN}{\mathbb{N}}
\newcommand{\InZ}{\mathbb{Z}}
\newcommand{\RaQ}{\mathbb{Q}}
\newcommand{\ReR}{\mathbb{R}}
\newcommand{\CoC}{\mathbb{C}}

\newcommand{\SBar}{\;|\;}

\newcommand{\tr}[1]{\mathrm{tr}#1}
\newcommand{\tru}[2]{\mathrm{tr}_{#1}#2}

\newcommand{\lie}[1]{\mathfrak{#1}}
\newcommand{\ad}[1]{\mathrm{ad}\; #1}
\newcommand{\adu}[2]{\mathrm{ad}_{#1}\; #2}

\newcommand{\triangleleftneq}{%
  \mathrel{\ooalign{$\lneq$\cr\raise.22ex\hbox{$\lhd$}\cr}}}

\begin{document}

\part{Definitions and basic properties}

\newpage

\textbf{Definition of Lie algebra}

\newpage

\part{Some basic properties for semisimple Lie algebras}

\newpage

\textbf{Root space decomposition}

Let $\lie{g}$ be a finite-dimensional (associative or Lie or whatever) $k$-algebra with product $[\cdot \; \cdot]$ and $D$ be a derivative of $\lie{g}$.
By the primary decomposition theorem we can decompose $\lie{g}$ as $\bigoplus_{c \in k} \lie{g}_c$, where $\lie{g}_c = \{X \in \lie{g} \SBar (D - c1)^r X = 0 \textrm{ for some $r \in \NaN$}\}$.
Now for any $A, B \in \lie{g}$ and $a, b \in k$, $(D - (a + b)1)^m [AB] = \sum_{i = 0}^m [((D - a1)^i A)((D - b1)^{m - i} B)]$.
It gives immediately that $[\lie{g}_a \lie{g}_b] \subseteq \lie{g}_{a + b}$. This property plays an important role of my text.

The first is following. The minimal polynomial of $D$ can be written as $\prod (t - \lambda_i)^{n_i}$.
Let $D = s(D) + n(D)$ be the Jordan decomposition, where $s, n$ are polynomials.
By Chinese remainder theorem we can obtain $s$; $s$ is the (minimal) polynomial satisfying $s(t) \equiv \lambda_i \textrm{ (mod $(t - \lambda_i)^{n_i}$)}$ for all $i$.
Thus $s(D)|_{\lie{g}_c} = c1$ for any $c \in k$.
Then for any $A \in \lie{g}_a$ and $B \in \lie{g}_b$, whenever $[AB]$ is 0 or not, $s(D)[AB] = (a + b)[AB] = [(aA)B] + [A(bB)] = [(s(D)A)B] + [A(s(D)B)]$, which implies that $s(D)$ is a derivative, and so is $n(D) = D - s(D)$.

We can go further.
Let $\mathcal{H}$ be a $l$-dimensional vector space of some derivatives of $\lie{g}$, of which all element commutes with each other.
Also let $(H_i)$ be a basis of $\mathcal{H}$.
We can decompose $\lie{g}$ as a direct sum of $\lie{g}_{c_1}$ ($c_1 \in k$), where $\lie{g}_{c_1} = \{X \in \lie{g} \SBar (H_1 - c_1 1)^r X = 0 \textrm{ for some $r \in \NaN$}\}$.
Since $H_1$ and $H_2$ commute, $\lie{g}_{c_1}$ can be even decomposed as a direct sum of $\lie{g}_{c_1, c_2}$, where $\lie{g}_{c_1, c_2} = \{X \in \lie{g}_{c_1} \SBar (H_2 - c_2 1)^r X = 0 \textrm{ for some $r \in \NaN$}\}$, or $\lie{g}_{c_1, c_2} = \{X \in \lie{g} \SBar (H_i - c_i 1)^{r_i} X = 0 \textrm{ for some $r_i \in \NaN$ and $i = 1, 2$}\}$.
This observation gives a general decomposition: $\lie{g} = \bigoplus_{c_1, \cdots, c_l \in k} \lie{g}_{(c_i)}$, where $\lie{g}_{(c_i)} = \{X \in \lie{g} \SBar (H_i - c_i 1)^{r_i} X = 0 \textrm{ for some $r_i \in \NaN$ and all $i$}\}$.

This result is unconvenient since it depends on the choice of the basis $H_i$'s.
Fortunately, it can be rewritten in a beautiful form.
Let $\alpha \in \mathcal{H}^\ast$ and denote $c_i = \alpha(H_i)$.
Using that $(\sum a_i H_i - \sum a_i c_i 1)^m = (\sum a_i(H_i - c_i 1))^m$ is a linear combination of the products of $(H_i - c_i 1)^{m_i}$, where $m_1 + m_2 + \cdots + m_l = m$, we have that $\lie{g}_{(c_i)} = \{X \in \lie{g} \SBar (H - \alpha(H) 1)^r X = 0 \textrm{ for any $H \in \mathcal{H}$ and some $r \in \NaN$}\} \equiv \lie{g}_\alpha$, of which the notation depends not on the basis but only on $\alpha$.
Furthermore, each $(c_i)$ one-to-one corresponds to $\alpha \in \mathcal{H}^\ast$.
Thus we can write the decomposition as $\lie{g} = \bigoplus_{\alpha \in \mathcal{H}^\ast} \lie{g}_\alpha$.
This decomposition is called the \textbf{root space decomposition}.
Note finally that we can immediately obtain that $[\lie{g}_\alpha \lie{g}_\beta] \subseteq \lie{g}_{\alpha + \beta}$; especially $\lie{g}_0$ ($0 \in \mathcal{H}^*$) is a subalgebra of $\lie{g}$.

\newpage

\textbf{Proof of the proper part of Engel theorem and Lie theorem}

Let $V$ be a finite-dimensional vector space and $\lie{g}$ in $\lie{gl}(V)$. Here are the parts.

- If every element in $\lie{g}$ is nilpotent, then there is $v \in V$ such that $\lie{g}v = 0$
- If $\lie{g}$ is solvable, then there is $v \in V$ such that $v$ is a common eigenvector of all elements of $\lie{g}$.

The proof of them are in very similar procedure.

- Assume that for some $n \in \NaN$ if a condition that $\dim{\lie{g}} < n$ is added, the statement holds, and then let $\dim{\lie{g}} = n$.

- Find an proper ideal $\lie{h}$ such that there is $X \in \lie{g}$ such that $\lie{g} = \lie{h} + kX$ (of course, $\lie{h}$ satisfies the given condition for $\lie{g}$).

- (By induction hypothesis) let $W \subseteq V$ be the set of $w$ satisfying the condition for $v$.

- Using that $\lie{h}$ is an ideal and something more, show that $W$ is $X$-invariant.

- Then there is $v \in W$ which we seek.

That's it. But there are some obstacles.

- For the second, such $\lie{h}$ can be found easily; choose an ideal of $\lie{g}/[\lie{g}, \lie{g}]$ (which is abelian) with codimension 1, and then let $\lie{h}$ be an ideal such that $\lie{h}/[\lie{g}, \lie{g}]$ be the ideal.
But for the first, we need a trick: let $\lie{h}$ be a maximal proper subalgebra.
We can apply the induction hypothesis to $\adu{\lie{g}/\lie{h}}{\lie{g}}$ ($\lie{g}/\lie{h}$ should be just a module) to find $Y$ in $\lie{g}$ such that $Y$ is not in $\lie{h}$ but $[\lie{h}, Y] \in \lie{h}$, which implies that the normalizer $\lie{n}$ of $\lie{h}$ is not equal to (but bigger than) $\lie{h}$.
Then the maximality of $\lie{h}$ says that $\lie{n} = \lie{g}$, so $\lie{h}$ is an ideal.
Now, for any $X \in \lie{g}$ which is not in $\lie{h}$ $\lie{h} + kX$ is a subalgebra containing $\lie{h}$ but not equal to $\lie{h}$, so it must be $\lie{g}$, again using the maximality of $\lie{h}$.
That is for the second step.

- Every step is now almost obvious, but the 'something else' is not.
For the first there is no 'something else'.
But for the second, if we denote for $w \in W$ $Aw = a(A)w$ for all $A \in \lie{h}$, we need to show that $a([A, X]) = 0$ for any $A \in \lie{h}$.
To show this we fix $w \in W$ and a function $a$, and let $n \in \NaN$ be the maximal number such that $w, Xw, ..., X^{n-1} w$ is linearly independent, and let $w_i = X^{i-1} w$ and $W_i$ the subspace spanned by $w_1, ..., w_i$.
By induction on $i$, one can show that $(A - a(A)) X^i w \in W_i$.
Hence $W_n$ is $A$-invariant and the matrix representation of the restriction of $A$ on $W_n$ relative to $(w_1, ..., w_n)$ is upper-diagonal of which all diagonal component is $a(A)$.
We denote $A_w$ be the restriction of $A$ on $W_n$.
Then $\tr{(A_w)} = na(A)$, so $a([A, X]) = (1/n) \tr{([A, X]_w)}$.
But it is direct that $[A, X]_w$ is nilpotent.
Therefore, $a([A, X]) = 0$, which we seek.

- Now the proof of Engel's theorem is obvious.
The above result says that the center $\lie{c}$ is not 0, so $\lie{g}/\lie{c}$ has dimension < $\dim{\lie{g}}$, and satisfies the condition, so we can assume that $\lie{g}/\lie{c}$ is nilpotent.
But from this $\lie{g}$ is nilpotent obviously.
By induction on $\dim{\lie{g}}$, Engel's theorem is proved.

- One more corollary; let $\lie{g}$ be a Lie algebra of which all element is ad-nilpotent, and let $\lie{h}$ be an ideal of $\lie{g}$.
Then the intersection of the center of $\lie{c}$ and $\lie{h}$ is not 0.
By the above proof its proof is easy if $\adu{\lie{h}}{\lie{g}}$ is used.

- Lie's theorem is more obvious. Use induction on $\dim{V}$.
There is one notable stuff; if $X \in [\lie{g}, \lie{g}]$, then all diagonal components of $X$ is 0, so that $X$ is nilpotent.

\newpage

\textbf{Cartan's criterion for solvability}

Although these are not related to semisimple Lie algebras explicitly, Engel's theorem (and its corollary) and Lie's theorem are indispensable ingredient.
There is one more such thing: Cartan's criterion for solvability.
We have to show that.

To do this we need a lemma.
\textit{Let $V$ be a finite-dimensional vector space and $\lie{h} \subseteq \lie{g} \subseteq L(V)$ subspaces (not necessary to be Lie algebras), and $M = \{A \in L(V) \SBar [A, \lie{g}] \subseteq \lie{h}\}$.
If for some $X \in M$ $\tr{XY} = 0$ for all $Y \in M$, then $X$ is nilpotent}.

Its proof is so freaky.
The main part is, if $E$ is $\RaQ$-spanned space by all eigenvalues of X, to show that any linear $f : E \to \RaQ$ is 0 so that $E^* = 0$, or all eigenvalue of $X$ is 0.
Let $(v_i)$ be a basis such that $Xv_i = \lambda_i v_i$.
Then it suffices to show that $f(\lambda_i) = 0$ for all $i$.
It can be archived if one shows that $\sum f(\lambda_i)^2 = 0$, but since $\sum f(\lambda_i)^2 = \sum \lambda_i f(\lambda_i)$, the aim can be to show that $\sum \lambda_i f(\lambda_i) = 0$.
This is the trace of $XY$ if we define $Y \in L(V)$ as $Yv_i = f(\lambda_i) v_i$.
Thus we should show that $Y \in M$, or $(\ad{Y}) \lie{g} \subseteq \lie{h}$ using that $(\ad{X}) \lie{g} \subseteq \lie{h}$.

Let $X = S + N$ be the Jordan decomposition.
If one define $E_{ij} \in L(V)$ as $E_{ij} v_r = \delta_{rj} v_i$, then $\ad{S} E_{ij} = (\lambda_i - \lambda_j) E_ij$ and $\ad{N}$ is nilpotent obviously, and of course $\ad{S}$ and $\ad{N}$ commute, so $\ad{S}$ is the semisimple part of $\ad{X}$.
Since $\ad{S}$ is non-invertible, one of its eigenvalue is 0.
Thus if $s$ is the polynomial which satisfies that $\ad{S} = s(\ad{X})$, then the constant term of $s$ is 0.
From this, $(\ad{S}) \lie{g} = (\textrm{some ploynomial of $\ad{X}$}) (\ad{X}) \lie{g} \in \lie{h}$, so $S \in M$. This gives a hint that we would find a polynomial $r$ with constant term zero, satisfying that $r(\ad{S}) = \ad{Y}$.
Since $\ad{Y}E_{ij} = (f(\lambda_i) - f(\lambda_j)) E_{ij}$, if $r$ is a polynomial with constant term 0 satisfying that $r(\lambda_i - \lambda_j) = f(\lambda_i - \lambda_j)$.
Let $T = {\lambda_i - \lambda_j \SBar i < j, \lambda_i - \lambda_j \ne 0}$, and $t_1, t_2, \cdots, t_m$ be the elements of $T$ ($m = |T|$).
Let $r(t) = \sum_{i = 1}^m a_i t^i$.
If we can find $a_i$ such that $\sum a_j (t_i)^j = f(t_i)$ for all $i$, this $r$ is the polynomial we seek.
It is done if the matrix $((t_i)^j)$ is invertible, which is immediate since all $t_i$ are nonzero and distinct to each other (see the Vandermonde's determinant).
Finally, we can find $r$ and that $(\ad{Y}) \lie{g} = r(\ad{S}) \lie{g} \subseteq \lie{g}$, which finishes our proof.

Now we attack the Cartan's criterion.
It is immediately from the following theorem: \textit{If $\lie{g} \le \lie{gl}(V)$ and $\tr{XY} = 0$ for all $X \in [\lie{g}, \lie{g}]$ and $Y \in \lie{g}$, then $\lie{g}$ is solvable}.
Note that $\lie{g}$ is solvable when $[\lie{g}, \lie{g}]$ is nilpotent.
Thus by Engel's theorem (this is the only usage of this theorem in my text) our aim is to show that every $X \in [\lie{g}, \lie{g}]$ is nilpotent.
Let $M = \{A \in \lie{gl}(V) \SBar [A, \lie{g}] = [\lie{g}, \lie{g}] \}$.
Then for any $X \in [\lie{g}, \lie{g}]$ if $\tr{XY} = 0$ for all $Y \in M$, then by the above lemma the proof is over.
(Note that $M$ is not necessarily equal to $\lie{g}$.)
But for any $A, B \in \lie{g}$ $\tr{[A, B]Y} = \tr{A[B, Y]} = 0$ by the associativity and the assumption.
Therefore, $\tr{XY} = 0$ for all $Y \in M$, which we want.

This theorem is a generalization of the Cartan's criterion: \textit{if for a Lie algebra $\lie{g}$ the Killing form vanishes, then $\lie{g}$ is solvable}.
This generalization will help to find a critical aspect of semisimple Lie algebras.

\newpage

\textbf{Some notes for radicals with derivatives}

This page is spared to a topic which is somewhat unrelated to our main topic, but it will be used in some special topic such as Ado's theorem.\footnote{The proofs are from Fulton, Harris, Representation theory (1991).}
In this page we let $\lie{g}$ be any finite-dimensional Lie algebra and $\lie{r}$ and $\lie{n}$ the radical and nil radical of $\lie{g}$, respectively.
First of all, we show that \textit{for any finite-dimensional representation $\rho : \lie{g} \to \lie{gl}(V)$ $\rho(\lie{g}^2 \cap \lie{r})$ consists of nilpotent elements in $L(V)$.}
It can be proved immediately by induction on $\dim{V}$ with $W$, a non-zero proper submodule of $V$, and $V/W$, except for the case $\rho$ irreducible, so we can assume that $\rho$ is irreducible.
We can assume also that $\rho$ is faithful and $\lie{r} \ne 0$.
If we can show that in this case $\rho(\lie{g}^2 \cap \lie{r}) = 0$, the proof is done, so this is our aim.
We let $r \in \NaN_0$ be such that $\lie{a} = \lie{r}^{(r)}$ becomes Abelian ideal.
Then, it suffice to show that $\rho(\lie{g}^2 \cap \lie{a}) = 0$ since $\lie{a} = \lie{r}$ when $r = 0$, while $\lie{a} \subseteq \lie{g}^2$ when $r > 0$.

Remind that in the main theorem of Engel's theorem, for $\rho(\lie{i}) \trianglelefteq \rho(\lie{g})$ consisting of nilpotent endomorphisms, $\{v \in V \SBar \rho(X)v = 0 \textrm{ for all $\rho(X) \in \rho(\lie{i})$}\}$ is a non-zero proper submodule of $V$.
It implies that there is no $\rho(\lie{i}) \trianglelefteq \rho(\lie{g})$ consisting of nilpotent elements.
The first usage of this fact is $[\rho(\lie{g}), \rho(\lie{a})]$.
For any $\rho(X) \in \rho(\lie{g})$, $\rho(A) \in \rho(\lie{a})$ and $n \in \NaN$, we have that $\tr{([\rho(X), \rho(A)]^n)} = \tr{(\rho(X) \cdot [\rho(A), [\rho(X), \rho(A)]^{n - 1}])} = 0$, so (using the fact that $\lie{a}$ is Abelian) the ideal $[\rho(\lie{g}), \rho(\lie{a})]$ consists of nilpotent elements, which must be 0 as seen.
It is a key to show $\rho(\lie{g}^2 \cap \lie{a}) = 0$; for any $X_i, Y_i \in \lie{g}$ with $\sum [X_i, Y_i] \in \lie{a}$, the above result says that $[\rho(Y_i), \sum_j [\rho(X_j), \rho(Y_j)]] = 0$ for all $i$, so $\tr{((\sum [\rho(X_i), \rho(Y_i)])^n)} = \sum_i \tr{(\rho(X_i) \cdot [\rho(Y_i), (\sum_j [\rho(X_j), \rho(Y_j)])^{n - 1}])} = 0$ for every $n \in \NaN$, which implies that $\rho(\lie{g}^2 \cap \lie{a})$, an ideal consisting of nilpotent elements, is 0, which we want.

To go further, we shall look at $[\lie{g}, \lie{r}]$.
By the above lemma and Engel’s theorem we have that $[\ad{\lie{g}}, \ad{\lie{r}}] \subseteq \ad{(\lie{g}^2 \cap \lie{r})}$ is nilpotent.
Thus $[\lie{g}, \lie{r}] / \lie{c}$, $\lie{c}$ the center of $\lie{g}$, is nilpotent, which implies that \textit{for every $\lie{g}$ $[\lie{g}, \lie{r}]$ is nilpotent.}

To catch our target we need a extension of $\lie{g}$.
Let $D$ be a derivative of $\lie{g}$.
We define a product on $\lie{g} \oplus k$ such that $[(X, a), (Y, b)] = ([X, Y] + aD(Y) - bD(X), 0)$.
It is direct that $\lie{g} \oplus k$ with this product is a Lie algebra and $\lie{g} \oplus 0$ is an ideal of $\lie{g} \oplus k$.
We call this algebra \textbf{holomorph}.

Now, using these we shall prove that \textit{for any finite-dimensional Lie algebra $\lie{g}$ and its radical $\lie{r}$ and derivative $D$, $D(\lie{r})$ is contained in a nilpotent ideal.}
It is direct; let $Z = (0, 1) \in \lie{g} \oplus k$ and $\lie{r}_1$ the radical of $\lie{g} \oplus k$, and then $D(\lie{r}) \oplus 0 = [Z, \lie{r} \oplus 0] \subseteq [\lie{g} \oplus k, \lie{r}_1] \cap (\lie{g} \oplus 0)$, where the last is a nilpotent ideal of $\lie{g} \oplus 0$ by the above lemma.

\newpage

\textbf{Inherits of Killing form and perpendicular of an ideal}

The Killing form $\kappa_{\lie{g}} : \lie{g} \to k$ of a Lie algebra $\lie{g}$ is defined as $\kappa_{\lie{g}} (A, B) = \tr{(\ad{A})(\ad{B})}$.
If no confusing, we just denote $\kappa_{\lie{g}}$ by $\kappa$.
This form is obviously symmetry.
Also, it is \textbf{associative}, in the sense that $\kappa([A, B], C) = \kappa(A, [B, C])$.
This property will play an important role.

Let $\lie{i}$ be an ideal of $\lie{g}$.
Then for any $A, B \in \lie{i}$ $\lie{i}$ is $(\ad{A})(\ad{B})$-invariant.
Thus $\tr{(\ad{A})(\ad{B})} = \tr{(\ad{A})(\ad{B})|_{\lie{i}}}$, which implies that \textit{$\kappa_{\lie{g}} (A, B) = \kappa_{\lie{i}} (A, B)$, or $\kappa_{\lie{g}}|_{\lie{i} \times \lie{i}} = \kappa_{\lie{i}}$}.

The associativity gives an interesting result.
We can think about $\lie{i}^\perp$ (relative to $\kappa$).
Let $X \in \lie{i}^\perp$.
By definition, $\kappa(A, X) = 0$ for all $A \in \lie{i}$.
Now let $B \in \lie{g}$.
Then, for any $A \in \lie{i}$ $\kappa(A, [B, X]) = \kappa([A, B], X) = 0$ since $[A, B] \in \lie{g}$.
Hence $\lie{i}^\perp$ is an ideal.
Note that we used only the associativity, so \textit{if $\beta$ is any associative bilinear form of $\lie{g}$, then $\lie{i}^\perp$ relative to $\beta$ is an ideal}.

\newpage

\textbf{Cartan's criterion for semisimplicity}

Now we shall prove the Cartan's criterion for semisimplicity.
To do this we need a lemma: \textit{A Lie algebra $\lie{g}$ is semisimple if and only if its Abelian ideal is only 0}.

($\Leftarrow$) Let $\lie{r}$ be the radical of $\lie{g}$ and assume that $\lie{r}$ is nonzero, and let $m \in \NaN$ be such that $\lie{r}^{(m)} = 0$ but $\lie{r}^{(m-1)} \ne 0$.
Then $\lie{r}^{(m-1)}$ is Abelian.
Also, by the Jacobi identity, for any ideal $\lie{i}$, $\lie{i}'$ is also an ideal, so $\lie{r}^{(m-1)}$ is an ideal.
Therefore, the assumption for nontriviality of $\lie{r}$, or for non-semisimplicity of $\lie{g}$ implies the existence of nonzero Abelian ideal.

($\Rightarrow$) Since Abelian algebra is solvable, Abelian ideal can be the radical of $\lie{g}$, or at least is contained in the radical of $\lie{g}$, which means that if there is an Abelian ideal then $\lie{g}$ is not semisimple.

Before beginning to the proof of Cartan's criterion for semisimplicity, we need some more.
Let $\rho : \lie{g} \to \lie{gl}(V)$ be a representation.
We can define a symmetric bilinear form $\beta(A, B) = \tr{\rho(A) \rho(B)}$.
It is easy to see that $\beta$ is associative.
Actually, the Killing form is one of the case for this bilinear form, with $\rho = \textrm{ad}$.
We can generalize the one part of the criterion by using this.

Now here is the criterion.
\textit{If the Killing form $\kappa$ of $\lie{g}$ is non-degenerate, then $\lie{g}$ is semisimple.
Conversely, but more generally, if $\lie{g}$ is semisimple, then for any faithful representation $\rho$ of $\lie{g}$ the associative symmetry bilinear form $\beta$, as defined, is non-degenerate.}

Assume that $\kappa$ is non-degenerate.
By the above lemma it is sufficient to show that any Abelian ideal is in $\lie{g}^\perp$.
Let $\lie{r}$ be an Abelian ideal and $A \in \lie{g}$ and $X \in \lie{r}$.
Then $(\ad{A})(\ad{X})$ sends $\lie{g}$ into $\lie{r}$, so $\tr{(\ad{A})(\ad{X})} = \tr{((\ad{A})(\ad{X}))|_{\lie{r}}}$.
But for any $Y \in \lie{r}$ $(\ad{A})(\ad{X})Y = 0$ since $\lie{r}$ is Abelian.
Thus $\kappa(A, X) = 0$, which means that $\lie{r} \subseteq \lie{g}^\perp = 0$.
Therefore, $\lie{g}$ is semisimple.

Now conversely assume that $\lie{g}$ is semisimple.
Consider $\lie{g}^{\perp}$ relative to $\beta$.
Since $\lie{g}$ is also an ideal, $\lie{g}^{\perp}$ is an ideal.
Now the general form of Cartan's criterion for solvability is used.
Since for any $\rho(X), \rho(Y) \in \rho(\lie{g}^{\perp})$ $\tr{\rho(X) \rho(Y)} = 0$, by the criterion $\rho(\lie{g}^{\perp})$ is solvable.
On the other hand, since $\rho$ is faithful, $\lie{g}^{\perp} \cong \rho(\lie{g}^{\perp})$.
Thus $\lie{g}^{\perp}$ is solvable so that it is contained the radical of $\lie{g}$, which is 0.
Therefore, $\beta$ is non-degenerate.

There is a corollary.
One might guess what $[\lie{g}, \lie{g}]^\perp$ (relative to the $\kappa_\lie{g}$) is if $\lie{g}$ is semisimple.
By definition, for any $X \in [\lie{g}, \lie{g}]^\perp$ and $A, B \in \lie{g}$ we have that $\kappa_\lie{g}([A, B], X) = 0$.
But $\kappa_\lie{g}([A, B], X) = \kappa_\lie{g}(A, [B, X])$, so $[B, X] = 0$ by the non-degeneracy, or $[\lie{g}, \lie{g}]^\perp$ is in the center of $\lie{g}$, which is 0.
Therefore, we obtain that \textit{$\lie{g} = [\lie{g}, \lie{g}]$.}

\newpage

\textbf{Decomposition of semisimple Lie algebras}

Let $\lie{g}$ be a semisimple Lie algebra.
Note that if $\lie{g}$ is simple, then $\lie{g}$ is also semisimple.
How about when $\lie{g}$ is not simple?

Let $\lie{i}$ be an ideal of $\lie{g}$.
We know that $\lie{i}^\perp$ (relative to the Killing form $\kappa$) is an ideal.
Thus $\lie{i} \cap \lie{i}^\perp$ is also an ideal.
It is immediate that $\kappa(A, B) = 0$ for all $A, B \in \lie{i} \cap \lie{i}^\perp$, so $\kappa_{\lie{i} \cap \lie{i}^\perp}$ vanishes since $\lie{i} \cap \lie{i}^\perp$ is an ideal so that $\kappa_{\lie{i} \cap \lie{i}^\perp} = \kappa_{(\lie{i} \cap \lie{i}^\perp) \times (\lie{i} \cap \lie{i}^\perp)}$.
Using Cartan's criterion for solvability we obtain that $\lie{i} \cap \lie{i}^\perp$ is a solvable ideal of $\lie{g}$, so $\lie{i} \cap \lie{i}^\perp = 0$.
Because $\dim{\lie{i}} + \dim{\lie{i}^\perp} = \dim{\lie{g}}$, we have that $\lie{g} = \lie{i} \oplus \lie{i}^\perp$.
Finally, $\kappa|_{\lie{i} \times \lie{i}}$ is non-degenerate obviously.
Since $\kappa_\lie{i} = \kappa|_{\lie{i} \times \lie{i}}$, we have that $\lie{i}$ is semisimple, and so is $\lie{i}^\perp$ since we choose an ideal $\lie{i}$ arbitrary so that it could be $\lie{i}^\perp$ at first.
Therefore, we have a decomposition $\lie{g} = \lie{i} \oplus \lie{i}^\perp$ of semisimple ideals.

From this it is immediate that \textit{every finite-dimensional semisimple Lie algebra is uniquely decomposed as a direct sum of simple ideals.}
The uniqueness (up to permutation) of this decomposition is obvious.
Hence, if we know all about simple Lie algebras, then we know all about semisimple Lie algebras.

\newpage

\textbf{Derivative of semisimple Lie algebra is inner; abstract decomposition}

Let $\lie{g}$ be a semisimple Lie algebra and $\textrm{Der }\lie{g}$ the derivative algebra of $\lie{g}$.
We will show that actually \textit{$\textrm{Der }\lie{g}$ is equal to $\ad{\lie{g}}$.}

To do this we need a formula.
Let $D \in \textrm{Der }\lie{g}$ and $A, X \in \lie{g}$.
Then $[D, \ad{A}]X = D[A, X] - [A, DX] = [DA, X] = (\ad{DA})X$, so $[D, \ad{A}] = \ad{DA}$.

Here we start the proof.
By the above formula $\ad{\lie{g}}$ is an ideal of $\textrm{Der }\lie{g}$.
Let $\kappa_D$ be the Killing form of $\textrm{Der }\lie{g}$.
Then $\kappa|_{\ad{\lie{g}} \times \ad{\lie{g}}} = \kappa_{\ad{\lie{g}}}$ is non-degenerate.
Thus $(\ad{\lie{g}})^\perp \cap \ad{\lie{g}} = 0$, so we can write $\textrm{Der }\lie{g} = \ad{\lie{g}} \oplus (\ad{\lie{g}})^\perp$.

Now let $D \in (\ad{\lie{g}})^\perp$.
Then for any $A, B \in \lie{g}$ $\kappa_D(\ad{DA}, \ad{B}) = \kappa_D([D, \ad{A}], \ad{B}) = \kappa_D(D, [\ad{A}, \ad{B}]) = \kappa_D(D, \ad{[A, B]}) = 0$.
By the non-degeneracy $\ad{DA} = 0$, thus $DA = 0$, or $D = 0$ since $\textrm{ad}$ is a faithful representation.
Therefore we have that $\textrm{Der }\lie{g} = \ad{\lie{g}}$.

This theorem gives an important property of semisimple Lie algebras.
Let $A \in \lie{g}$.
$\ad{A}$, a derivative, can be viewed as an element of $L(\lie{g})$.
Thus there is the Jordan decomposition $\ad{A} = s(\ad{A}) + n(\ad{A})$.
On the other hand, we showed that $s(\ad{A})$ and $n(\ad{A})$ are also derivatives.
Then by the above theorem there are $A_S, A_N \in \lie{g}$ such that $\ad{A_S} = s(\ad{A})$ and $\ad{A_N} = n(\ad{A})$, and $A = A_S + A_N$.
Hence, \textit{there is a decomposition of elements in $\lie{g}$ like the Jordan decomposition.}
We call these $A_S$ and $A_N$ the \textbf{(abstract) semisimple part} and the \textbf{(abstract) nilpotent part} of $A$.
In many case, fortunately, this terminology does not make any ambiguity.
Furthermore, we will see that if $\lie{g}$ is in some $\lie{gl}(V)$, the abstract decomposition for each element in $\lie{g}$ coincides exactly with the Jordan decomposition.

\newpage

\textbf{Casimir element}

Let $\lie{g}$ be a semisimple Lie algebra and $\rho : \lie{g} \to \lie{gl}(V)$ be a faithful representation, and $(U_i)$ be a basis of $\lie{g}$.
Since $\lie{g}$ is semisimple, the associative and symmetry bilinear form $\beta$ defined as $\beta(A, B) = \tr{\rho(A)\rho(B)}$ is non-degenerate.
Then there is another basis $(U'_i)$, the dual of $(U_i)$, such that $\beta(U_i, U'_j) = \delta_{ij}$.

These bases has an interesting property.
For $A \in \lie{g}$ let $[U_i, A] = \sum a_{ij} U_j$ and $[U'_i, A] = \sum b_{ij} U'_j$.
Then $a_{ij} = \sum a_{ir} \delta{rj} = \beta(\sum a_{ir} U_r, U'_j) = \beta([U_i, A], U'_j) = \beta(U_i, [A, U'_j]) = \beta(U_i, -\sum b_{jr} V_r) = -\sum b_{jr} \delta_{ir} = -b_{ji}$.
This involves an important result.
Let $C_\rho = \sum \rho(U_i) \rho(U'_i)$.
Then for any $A \in \lie{g}$ we have that $[C_\rho, \rho(A)] = \sum ([\rho(U_i), \rho(A)] \rho(U'_i) + \rho(U_i) [\rho(U'_i), \rho(A)]) = \sum (a_{ij} \rho(U_j) \rho(U'_i) + b_{ij} \rho(U_i) \rho(U'_j)) = \sum (a_{ij} \rho(U_j) \rho(U'_i) - a_{ji} \rho(U_i) \rho(U'_j)) = 0$, that is, \textit{$C_\rho$ commutes with all $\rho(A) \in \rho(\lie{g})$.}
Such $C_\rho$ is called the Casimir operator.
This operator will play important roles in many areas.
Note that since $\tr{C_\rho} = \sum \tr{\rho(U_i) \rho(U'_i)} = \dim{\lie{g}}$, if $\rho$ is faithful, then $C_\rho$ is not 0.

One might guess the well-definedness of $C_\rho$; it depends on the choosing the basis $U_i$.
To see what happened when we choose another basis, let $(V_i)$ be another basis and $(V'_i)$ the dual basis of $(V_i)$.
We can write $V_i = \sum c_{ij} U_j$ and $V'_i = \sum d_{ij} U'_j$.
Then $\sum \rho(V_i) \rho(V'_i) = \sum_{irs} c_{ir} d_{is} \rho(U_r) \rho(U'_s)$.
On the other hand, $\sum_i c_{ir} d_{is} = \sum_{ij} c_{ir} d_{js} \beta(U_i, U'_j) = \beta(V_r, V'_s) = \delta_{rs}$.
Therefore, $\sum \rho(V_i) \rho(V'_i) = \sum \rho(U_i) \rho(U'_i) = C_\rho$, so $C_\rho$ is independent of the choosing basis.

This page is ended introducing a pretty proof of Schur's first and second lemma: \textit{If $\mathcal{A}$ be an (associative or Lie or whatever) algebra and $V$ and $W$ irreducible $\mathcal{A}$-modules, then 1. any $\mathcal{A}$-homomorphism of $V$ into $W$ is zero or isomorphism, 2. when the base field is algebraically closed, any $\mathcal{A}$-homomorphism of $V$ into $V$ is a scalar multiplication.}

pf) The first one is immediate; the kernel of the homomorphism must be $V$ or 0 because of the irreducibility, where in the former case the mapping must be 0, otherwise it is a monomorphism, but then the image must be $W$ because of the irreducibility.
The proof of second one: If $A$ is a $\mathcal{A}$-homomorphism of $V$ into $V$ and $v$ is an eigenvector of $A$ such that $Av = \lambda v$, then a $\mathcal{A}$-homomorphism $A - \lambda 1$ have a non-zero kernel, which must be $V$ by the irreducibility of $V$.

(N.B. Note that the condition of algebraically closed-ness is crucial, because on some field extension $V$ may become not irreducible! --- See $\ReR^2$, a module of $SO(2)$, which is irreducible but after the field extension to $\CoC$ not irreducible.
But if $V$ is still irreducible after a suitable field extension, then the lemma can hold.)

\newpage

\textbf{Weyl's complete reducibility theorem}

Now we let $V$ be a module of a semisimple $\lie{g}$, and $\rho : \lie{g} \to \lie{gl}(V)$ the associative representation of the action (i.e., for any $A \in \lie{g}$ and $v \in V$ we have that $\rho(A)v = Av$).
We also assume that the action of $\lie{g}$ on $V$ is faithful.
This assumption is actually quite general because $\rho(\lie{g})$ must be isomorphic to an ideal of $\lie{g}$, which is also semisimple, so the assumption is same as to change $\lie{g}$ to the ideal, which is irrelevant to our story.
The aim is to show that \textit{if $W$ is a submodule of $V$, then there is another submodule $U$ of $V$ such that $V$ is the direct sum of $W$ and $U$.}
(We call $U$ the \textbf{complementary module} of $W$.)
The proof is constructed by the following steps:

(1) $\dim{V/W} = 1$, $W$ is irreducible

(2) $\dim{V/W} = 1$, $W$ is not necessarily irreducible

(3) $\dim{V/W}$ is not 1, necessarily

Here are the important points of proofs of each steps.

(1) Now let $C (=C_\rho)$ be the Casimir operator of $\rho$.
What we should do is to check the kernel of $C$, which can be checked seeing the induced map of $C$ on $V/W$.
From this we have that $\ker{C} \ne 0$.
But the intersection of $W$ and $\ker{C}$, which can be shown that $C|_W$ is an isomorphism of $W$ onto $W$, by Schur’s first lemma.
Therefore, with $CW = W$ and dimension theorem, we have that $V = W \oplus \ker{C}$, and $\ker{C}$ is a $\lie{g}$-module since $C$ is a $\lie{g}$-module homomorphism.

(2) Use induction on $\dim{V}$. Let $W'$ be a proper submodule of $W$.
Then $W/W'$ is a submodule of $V/W'$ with codimension 1.
By induction, find a submodule $W''/W'$ of $V/W'$ such that $V/W'$ is the direct sum of $W/W'$ and $W''/W'$.
Show that there is $W''$ which is a submodule of $W$ having $W'$ as a submodule with codimension 1 [we can write $W''/W' = k(u + W')$ so that $W'' = ku + W'$].
Thus, by induction there is a one-dimensional submodule $U$ of $W''$, which is the complementary we seek.

(3) Let $\rho$ be a representation of $\lie{g}$ on $V$ and let $K$ be the set of all element of $L(V, W)$, as a $\lie{g}$-module with action $(A \cdot X)(v) = \rho(A)(Xv) - X(\rho(A)v)$, of which the restriction on $W$ is just a scalar multiplication.
If $X$ is in $K$, for $w$ in $W$, $(A \cdot X)(w) = 0 \cdot w$, so that $K$ is a submodule of $L(V, W)$. Let $M = \{Y \in K \SBar Y \cdot W=0\}$.
Then $M$ is a submodule of $K$ with codimension 1.
Thus by (2) $K = M + kY$ where $Yw = cw$ with non-zero $c$ in $k$ and $\lie{g} \cdot Y = [\rho(\lie{g}), Y] = 0$, so that $\ker{Y}$ is a submodule of $V$.
Now by the dimension theorem $V$ is the direct sum of $W$ and $\ker{Y}$, which completes the proof.

\newpage

\textbf{the three-dimensional simple Lie algebra}

There is an elementary ingredient for the theory of semisimple Lie algebras: The three-dimensional simple Lie algebra.
It is $\lie{s} = kH + kE + kF$, where $[H, E] = 2E, [H, F] = -2F, [E, F] = H$.
We shall see that this Lie algebra is simple.
Let $\lie{i}$ be a non-zero ideal of $\lie{s}$.
It is immediate that if any one of $H, E, F$ is in $\lie{i}$, then $\lie{i} = \lie{s}$.
Let $0 \ne X = aH + bE + cF \in \lie{i}$.
Then $[E, [E, X]] = [E, -2aE + cH] = -2cE$ and $[F, [F, X]] = [F, 2aF - bH] = -2bF$.
Thus if $a \ne 0$ or $b \ne 0$, then $E$ or $F$ is in $\lie{i}$.
If $a = b = 0$, then $X = aH$, but it means that $H \in \lie{i}$.
In any case $\lie{i} = \lie{s}$.
Therefore, we obtain that \textit{$\lie{s}$ is simple.}
(Actually it is the smallest simple Lie algebra and there is no three-dimensional simple Lie algebra different from this (up to isomorphism) if the base field is algebraically closed.
For more detail, see Jacobson.)

There is a simple representation of this Lie algebra: $H = \left( \begin{array}{rr} 1 & 0 \\ 0 & -1 \end{array} \right)$, $E = \left( \begin{array}{rr} 0 & 1 \\ 0 & 0 \end{array} \right)$, $F = \left( \begin{array}{rr} 0 & 0 \\ 1 & 0 \end{array} \right)$.
In this representation we obtain that $\lie{s} \cong \lie{sl}(2, k)$.

\newpage

\textbf{Irreducible modules of the three-dimensional simple Lie algebra}

We have a simple Lie algebra $\lie{s} = kH + kE + kF$ with $[H, E] = 2E$, $[H, F] = -2F$, $[E, F] = H$.
This is the smallest simple Lie algebra.
What we will do is to find all finite-dimensional irreducible modules of $\lie{s}$.
We will see that $\lie{s}$ and this result play the central roles in the whole semisimple Lie algebra theory.

Let $V$ be a finite-dimensional irreducible module of $\lie{s}$.
Also we let $w$ be an eigenvector of $H$ with $Hw = \lambda' w$.
Then $H(Ew) = E(Hw) + [H, E]w = (\lambda' + 2)Ew$ and $H(Fw) = F(Hw) + [H, F]w = (\lambda' - 2)Fw$.
From this, by induction it is immediate that $H(E^i w) = (\lambda' + 2i) (E^i w)$ and $H(F^i w) = (\lambda' - 2i) (F^i w)$.
Hence we have many of eigenvectors of $H$.
But since $V$ is finite-dimensional, there is $m \in \NaN$ such that $E^m w \ne 0$ but $E^{m + 1} w = 0$.
Now we denote $v = E^m w$ (thus $Ev = 0$) and $\lambda = \lambda + 2m$ so that $Hv = \lambda v$.
The finite-dimensionality also implies that there is $n \in \NaN$ such that $F^n v \ne 0$ but $F^{n + 1} v = 0$.
Thus we have eigenvectors $v_i$'s of $H$ where $v_i = (1/i!) F^i v$ ($i = 0, 1, \cdots, n$) so that $Hv_i = (\lambda - 2i) v_i$.

Note that from the fact that $Ev_0 = 0$, we obtain that $Ev_i = (1/i!) [E, F^i]v = \frac{1}{i!} \sum_{j = 0}^{i - 1} F^j [E, F](F^{i - j - 1} v) = \frac{1}{i!} i \sum_{j = 0}^{i - 1} (\lambda - 2(i - j - 1)) (F^{i - 1} v) = (\lambda - i + 1) v_{i - 1}$ (this is the reason that the extra coefficient $(1/i!)$ is added), and that $Fv_i = (i + 1) v_{i + 1}$. Then the subspace spanned by all $v_i$ is a submodule of $\lie{g}$.
Thus, by the irreducibility, $(v_i)$ is a basis of $V$, and $\dim{V} = n + 1$.

Here is the remaining question: what is $\lambda$?
To answer this we need to check the consitency of Lie products.
We do not know yet that the rules for $E$ and $F$ is consistent to the formula $H = [E, F]$.
Here is the calculation for this: for $i = 1, 2, \cdots, n - 1$, $(EF - FE)v_i = E((i + 1) v_{i + 1}) - F((\lambda - i + 1) v_{i - 1}) = (i + 1)(\lambda - i) v_i - (\lambda - i + 1) iv_i = (\lambda - 2i) v_i$.
Also, $(EF - FE)v_0 = EFv_0 = (0 + 1)(\lambda - 0) v_0 = \lambda v_0$.
Thus the rules is automatically consistent for $v_0, v_1, \cdots, v_{n - 1}$.
The remaining is for $v_n$: $(EF - FE)v_n = -FEv_n = -n(\lambda - n + 1) v_n$.
It must coincides with $Hv_n = (\lambda - 2n) v_n$.
The only one way to make this consistent is setting $\lambda = n$.
Hence, \textit{the $(n + 1)$-dimensional irreducible module of $\lie{s}$ is only one.
Especially, we can always find a basis $(v_i)$ such that }
\begin{eqnarray*}
  Hv_i &=& (n - 2i) v_i \\
  Ev_i &=& (n - i + 1) v_{i - 1} \\
  Fv_i &=& (i + 1) v_{i + 1}.
\end{eqnarray*}

We call the eigenvalues of $H$ the \textbf{weights}.
Note that, although we start with an algebraically closed base field, the result says that this result holds for any base field (of characteristic 0).
Also it is important that \textit{every weight is an integer, but not all nonnegative or nonpositive.}

There is another important property.
Let $V$ be a $\lie{g}$-module which is not necessarily irreducible and $V_a = \{v \in V \SBar Hv = av\}$.
We call such subspace the \textbf{weight space}.
The above argument immediately implies that \textit{there is a basis of $V_a$ of which each element is contained in a distinct irreducible module.}

\newpage

\textbf{Preservation of Jordan decomposition}

We mentioned above that \textit{if the base field is algebraically closed, in a semisimple Lie algebra the abstract decomposition coincides with the Jordan decomposition.}
This page is spared to show this.

First, we consider a semisimple subalgebra $\lie{g} \le \lie{gl}(V)$.
For $A \in \lie{g}$ we can Jordan-decompose $A = s(A) + n(A)$.
The abstract decomposition is denoted by $A = A_S + A_N$.
We shall show that $s(A) = A_S$ (hence $n(A) = A_N$).
One may confuse that since $\adu{\lie{g}}{A} = \adu{\lie{g}}{A_S} + \adu{\lie{g}}{A_N}$ is the Jordan decomposition, while $\adu{\lie{gl}(V)}{A} = \adu{\lie{gl}(V)}{s(A)} + \adu{\lie{gl}(V)}{n(A)}$ is also the Jordan decomposition, $A_S$ coincides automatically with $s(A)$.
But these formulae solve the confuse.
Each of these decompositions indicates a distinct stuff: one is $\adu{\lie{g}}{A}$, while the other is $\adu{\lie{gl}(V)}{A}$.
But if one proves that $s(A)$ (and $n(A)$) is in $\lie{g}$, then $\adu{\lie{g}}{A} = \adu{\lie{g}}{s(A)} + \adu{\lie{g}}{n(A)}$ becomes the Jordan-decomposition.
Hence the uniqueness of Jordan decomposition shows that $s(A) = A_S$ and $n(A) = A_N$.
Thus our aim is to show that $s(A) \in \lie{g}$.

Since $\adu{\lie{gl}(V)}{s(A)}$ is a polynomial of $\adu{\lie{gl}(V)}{A}$, it leaves $\lie{g}$ invariant. 
Thus $s(A)$ is in the normalizer $\lie{n}$ of $\lie{g}$.
If we find a certain subalgebra containing $\adu{\lie{gl}(V)}{s(A)}$ and show that the subalgebra coincides with $\lie{g}$, our proof is done.
Note that $\lie{n}$ contains scalar multiplications, or non-traceless elements, it is larger than $\lie{g}$.
Hence we consider a smaller subalgebra: $\lie{g}_1 = \{X \in \lie{n} \SBar XW \subseteq W, \; \tr{X|_W} = 0 \textrm{ for any $W \le V$}\}$.
Of course, $\lie{g}_1$ contains $\lie{g}$, and it is $\adu{\lie{gl}(V)}{\lie{g}}$-invariant.
Thus $\lie{g}_1$ is a $(\adu{\lie{gl}(V)}{\lie{g}})$-module.
By the complete reducibility, there is the complementary $\lie{h}$ of $\lie{g}$ in $\lie{g}_1$.
Thus $(\adu{\lie{gl}(V)}{\lie{g}})\lie{h} \subseteq \lie{h}$.
But since $(\adu{\lie{gl}(V)}{\lie{g}}) \lie{n} \subseteq \lie{g}$, $(\adu{\lie{gl}(V)}{\lie{g}})\lie{h} = 0$.
Hence for any $X \in \lie{h}$ $[A, X] = 0$ for all $A \in \lie{g}$, that is, $A$ and $X$ commute, which implies that $A$ is a $\lie{g}$-homomorphism of $V$ into $V$.
Since $X \in \lie{g}_1$, for any irreducible submodule $W \le V$ $X|_W$ is a $\lie{g}$-homomorphism of $W$ into $W$, which must be a scalar multiplication by Schur's second lemma (this is the only one point we need the condition that the base field is algebraically closed).
But since $\tr{X|_W} = 0$, $X|_W = 0$.
Therefore, we obtain that $X = 0$ for any $X \in \lie{h}$, so that $\lie{g}_1 = \lie{g}$.
Of course, by the definition $s(A) \in \lie{g}_1$, so our proof is done.

This result can be extended immediately.
\textit{Let $\lie{g}$ be a (abstract) semisimple Lie algebra and $\rho : \lie{g} \to \lie{gl}(V)$ a faithful representation.
Then for $A \in \lie{g}$ with the abstract decomposition $A = A_S + A_N$, $\rho(A) = \rho(A_S) + \rho(A_N)$ is the Jordan decomposition.}
This result will not play any role in the classification (and construction) of (semi)simple Lie algebras.
But it will be a base of representation theory.

\newpage

\textbf{Abelian-ness of toral subalgebra}

We call a subalgebra of a Lie algebra, in which all element is semisimple, i.e., its adjoint is semisimple (or diagonalizable), a \textbf{toral subalgebra}.
Note that if we have a semisimple element, we can construct at least one-dimensional toral subalgebra, which is just the linear span of our one semisimple element.
We shall show that \textit{any toral subalgebra is Abelian.}

pf) Let $\lie{h}$ be the toral subalgebra and $X$ and $Y$ in $\lie{h}$.
The closedness (which is from the fact that $\lie{h}$ is an subalgebra) implies that h is $(\ad{X})$- and $(\ad{Y})$-invariant subspace.
Since these operators are, by the definition of $\lie{h}$, diagonalizable, it is sufficient to investigate what the eigenvalue of $\ad{X}$ is.
For this purpose, we let $(\ad{X})Y = aY$.

Now the diagonalizability of $\ad{Y}$ plays a crucial roles.
Let $U_i$ be a basis of h such that each $U_i$ is an eigenvector of $\ad{Y}$, where $(\ad{Y})U_i = \lambda_i U_i$.
We can write $X = \sum a_i U_i$.
On the other hand, $Y$ is also an eigenvector of $\ad{Y}$ with eigenvalue 0, obviously.
Now by linear-combining $U_i$'s with eigenvalue 0 and reordering the indices, we can assume that $U_1 = Y$, so $\lambda_1 = 0$, without any lose of generality.
Then, $-aY = [Y, X] = \sum a_i b_i U_i$, so $aY + \sum a_i b_i U_i = (a+a_1 b_1)U_1 + \sum_{i \ne 1} a_i b_i U_i = 0$.
The linear independence of $U_i$'s says that $a+a_1 b_1 = a = 0$.
Therefore, $\ad{X}$ has only zero eigenvalue, which implies that $\ad{X}$ sends $\lie{h}$ to 0, so $\lie{h}$ is Abelian.

\newpage

\textbf{Levi's decomposition theorem}

Our main target is semisimple Lie algebras, of which the radical is 0.
(Actually, the contents in this page is not needed for the theory of semisimple Lie algebras, so it can be skipped in the first reading.)
Meanwhile, one may wonder that for any finite-dimensional Lie algebra $\lie{g}$ there is $\lie{g}_1 \le \lie{g}$ such that $\lie{g}_1 \cong \lie{g}/\lie{r}$, where $\lie{r}$ is the radical of $\lie{g}$ so that $\lie{g} = \lie{g}_1 \oplus \lie{r}$.
We call such $\lie{g}_1$ a \textbf{Levi factor}.
Now we shall prove that \textit{every finite-dimensional Lie algebra has a Levi factor}.

The proof\footnote{\scriptsize{http://www.maths.adelaide.edu.au/thomas.leistner/2010-1-honours/LA10-handout01-Levi.pdf}} is starting from the case for $\lie{r}$ having non-zero proper ideal $\lie{a}$ of $\lie{g}$.
It is good to use the induction on $\dim{\lie{g}}$; we assume that if $\dim{\lie{g}} < n$ for some $n \in \NaN$ then $\lie{g}$ has a Levi factor and we let $\dim{\lie{g}} = n$, where the case for $n = 1$ is trivial.
Then, $\lie{g}/\lie{a}$ has a Levi factor $\lie{g}'_1$, i.e., $\lie{g}/\lie{a} = \lie{g}'_1 \oplus \lie{r}/\lie{a}$.
Let $\lie{g}'$ be the preimage of $\lie{g}'_1$ under the canonical projection $\lie{g} \to \lie{g}/\lie{r}$.
Then it is immediate that $\lie{g}' \lneq \lie{g}$ and that $\lie{a}$ is the radical of $\lie{g}'$.
By induction hypothesis, $\lie{g}'$ has a Levi factor $\lie{g}_1$, and it is obviously a Levi factor of $\lie{g}$.

We assumed that $\lie{r}$ has a non-zero proper ideal of $\lie{g}$.
Now we shall handle the case for $\lie{r}$ with no such $\lie{a}$.
In this case we shall construct a Levi factor explicitly.
Since $\lie{g}/\lie{r}$ is semisimple, we can set our strategy to use the Weyl's theorem.
Considering $[\lie{r}, \lie{r}]$, in this case $\lie{r}$ is Abelian.
This case is splitted again into two cases: $[\lie{g}, \lie{r}] = 0$ or $[\lie{g}, \lie{r}] = \lie{r}$.
In the former case, $\lie{r}$ is actually the center of $\lie{g}$, so one can consider a representation $\lie{g}/\lie{r} \to L(\lie{g})$ ($X + \lie{r} \mapsto \ad{X}$).
It is of course faithful and $\lie{g}/\lie{r}$ is semisimple, and $\lie{r}$ is a $\lie{g}/\lie{r}$-submodule of $\lie{g}$.
Then, by Weyl's theorem, $\lie{r}$ has a complementary $\lie{g}_1$, and obviously this is a Levi factor.

Now consider the case for $[\lie{g}, \lie{r}] = \lie{r}$.
To construct a $\lie{g}/\lie{r}$-module we use a method similar to a way which is used in the proof of Weyl's theorem.
We define a representation $\rho : \lie{g} \to \lie{gl}(\lie{g})$ as $\rho(X) \phi = [\ad{X}, \phi]$ ($X \in \lie{g}$), and let $\lie{K} = \{\phi \in \lie{gl}(\lie{g}) \SBar \phi(\lie{g}) \subseteq \lie{r}, \phi|_{\lie{r}} = c1 \; (c \in k)\}$ and $\lie{L} = \{\phi \in \lie{K} \SBar \phi|_\lie{r} = 0\}$.
As in the proof of Weyl's theorem, $\dim{\lie{K}} - \dim{\lie{L}} = 1$ and $\rho(\lie{g}) \lie{K} \subseteq \lie{L}$.
They are not $\lie{g}/\lie{r}$-modules, but we can modify them by considering another set; $\ad{\lie{r}}$.
Of course, $\ad{\lie{r}} \subseteq \lie{L}$.
For any $X \in \lie{r}$, $\phi \in \lie{K}$ and $A \in \lie{g}$ we have that $(\rho(X)(\phi))(A) = [X, \phi(A)] - \phi([X, A]) = -c[X, A] = (\ad{(-cX)})(A)$ for some $c \in k$ since $\phi(A)$ and $[X, Y]$ are in $\lie{r}$ and $\lie{r}$ is Abelian, so $\rho(\lie{r})(\lie{K}) \subseteq \ad{\lie{r}}$.
It implies that we can regard $\lie{K}/\lie{L}$ and $\lie{K}/(\ad{\lie{r}})$ as $\lie{g}/\lie{r}$-modules.
Furthermore, if $\pi : \lie{K}/(\ad{\lie{r}}) \to \lie{K}/\lie{L}$ is the canonical projection, it is immediate that $\pi$ is a $\lie{g}/\lie{r}$-module homomorphism, so $\ker{\pi}$ is a $\lie{g}/\lie{r}$-submodule of $\lie{K}/(\ad{\lie{r}})$.
Since $\lie{g}/\lie{r}$ is semisimple, by Weyl's theorem again, we can find the complementary of $\ker{\pi}$, which is one-dimensional so that it can be written as $k(\psi + \ad{\lie{r}})$ ($\psi \in \lie{K} \setminus (\lie{L})$).
On the other hand, since $k(\psi + \ad{\lie{r}}$ is one-dimensional, $\lie{g}/\lie{r}$ acts on this module trivially, which implies that $\rho(\lie{g})(\psi) \subseteq \ad{\lie{r}}$.
Now let $\lie{g}_1 = \{X \in \lie{g} \SBar \rho(X)(\psi) = 0\}$, which is obviously a subalgebra.
First, since $\psi \notin \ad{\lie{r}}$, $\lie{g}_1 \cap \lie{r}$ must be 0.
Next, we choose $X \in \lie{g}$ and denote $Y = \psi(X)$.
Then $\psi(Y) = cY$ for some $0 \ne c \in k$, so $\psi(X - \frac{1}{c} Y) = 0$, or $X - \frac{1}{c} Y \in \lie{g}_1$.
Hence $X = (X - \frac{1}{c} Y) + \frac{1}{c} Y$, which implies that $\lie{g} = \lie{g}_1 \oplus \lie{r}$.
Therefore, we can say that $\lie{g}_1$ is a Levi factor.
Finally, induction on $n$ finishes the proof.

\newpage

\part{Classification of semisimple Lie algebras}

\newpage

\textbf{Root space decomposition of a semisimple Lie algebra}

We now start to the classification of finite-dimensional (semi)simple Lie algebras.
The first is the root space decomposition of a maximal toral subalgebra $\lie{h} \le \lie{g}$; $\lie{g} = \bigoplus_{\alpha \in \lie{h}^*} \lie{g}_\alpha$.
Let $\Phi$ be the set of all nonzero $\alpha \in \lie{h}^*$ such that $\lie{g}_\alpha \ne 0$.
Then $\lie{g} = \lie{g}_0 \oplus \bigoplus_{\alpha \in \Phi} \lie{g}_\alpha$.
We call an element in $\Phi$ a \textbf{root}.
Note that since $\lie{g}$ is finite-dimensional, $|\Phi| < \infty$.
Also note that since all $H \in \lie{h}$ is semisimple, $\lie{g}_\alpha = \{A \in \lie{g} \SBar (\ad{H})A = \alpha(H)A\}$.

We define a terminology.
For a subspace $\lie{r}$ of $\lie{g}$ if $\kappa|_{\lie{r} \times \lie{r}}$ is non-degenerate, where $\kappa$ is the Killing form, then we say that there is non-degeneracy on $\lie{r}$.

The first proposition is that for $\alpha, \beta \in \lie{h}^*$ with $\beta \ne -\alpha$ $\lie{g}_\alpha$ and $\lie{g}_\beta$ are orthogonal relative to $\kappa$.
Since $\alpha + \beta \ne 0$, there is $H \in \lie{h}$ such that $(\alpha + \beta)(H) \ne 0$.
Then for any $A \in \lie{g}_\alpha$ and $B \in \lie{g}_\beta$ we have that $(\alpha + \beta)(H) \kappa(A, B) = \kappa(\alpha(H) A, B) + \kappa(A, \beta(H) B) = \kappa([H, A], B) + \kappa(A, [H, B]) = 0$, which implies the orthogonality.
Especially, $\lie{g}_0$ is orthogonal to all root spaces.
In this case if we do not have the non-degeneracy on $\lie{g}_0$, then $\kappa$ will be not non-degenerate.
Hence, we have \textbf{the non-degeneracy on $\lie{g}_0$}.

The next step is to show the non-degeneracy on $\lie{h}$.
Note that $X \in \lie{g}$ is in $\lie{g}_0$ if and only if $\ad{X}$ sends $\lie{h}$ to 0.
If so is some $X \in \lie{g}$, then any polynomial of $\ad{X}$ with constant term 0 sends $\lie{h}$ to 0, and in the decomposition $\ad{X} = \ad{X_S} + \ad{X_N} = s(\ad{X}) + n(\ad{X})$ $s$ and $n$ are such polynomials.
Thus every semisimple part and nilpotent part of $\lie{g}_0$ is in $\lie{g}_0$.
Since it is obvious that for any semisimple element $X \in \lie{g}_0$, which commutes with all $H \in \lie{h}$, $\lie{h} + kX$ is a toral subalgebra, hence $X \in \lie{h}$.
Now let $X \in \lie{h}$ be such that $\kappa(H, X) = 0$ for all $H \in \lie{h}$.
Since all semisimple $A_S \in \lie{g}_0$ is in $\lie{h}$ and for any nilpotent $A_N \in \lie{g}_0$ $(\ad{A_N})(\ad{X})$ is nilpotent, we have that $\kappa(A, X) = 0$ for all $A \in \lie{g}_0$.
Then by the non-degeneracy on $\lie{g}_0$ implies that $X = 0$, hence the non-degeneracy on $\lie{h}$ is obtained.

Now let $\lie{c}$ be the center of $\lie{g}_0$.
Obviously, $\lie{h}$ is contained in $\lie{c}$.
Let $X \in \lie{c}$ and $X_N$ the nilpotent part of $X$.
Then $\ad{X_N}$ commutes with $\ad{A}$ for all $A \in \lie{g}_0$, so $\kappa(A, X_N) = 0$, which implies that $X_N = 0$, or $\lie{c} = \lie{h}$ by the non-degeneracy on $\lie{g}_0$.
On the other hand, it is then obvious that for any $A \in \lie{g}_0$ $\adu{\lie{g}_0}{A}$ is nilpotent.
(Note that since for any $\alpha, \beta \in \lie{h}^*$ $[\lie{g}_\alpha, \lie{g}_\beta] \subseteq \lie{g}_{\alpha + \beta}$, $\lie{g}_0$ is a subalgebra so that $\adu{\lie{g}}{A}$ leaves $\lie{g}_0$ invariant.)
By the corollary of the main part of Engel's theorem, since $[\lie{g}_0, \lie{g}_0]$ is an ideal of $\lie{g}_0$, $\lie{h} \cap [\lie{g}_0, \lie{g}_0] \ne 0$ if $\lie{g}$ is not Abelian.
But for $X, Y \in \lie{g}_0$ $\kappa([X, Y], A) = \kappa(X, [Y, A]) = 0$ for all $A \in \lie{h}$, which implies that for any $X \in \lie{h} \cap [\lie{g}_0, \lie{g}_0]$ $X = 0$.
Thus $\lie{g}_0$ is Abelian.
Finally, then $\lie{g}_0 = \lie{c} = \lie{h}$.

Now \textit{the decomposition can be written as $\lie{g} = \lie{h} + \sum_{\alpha \in \Phi} \lie{g}_\alpha$.}

\newpage

\textbf{Orthogonality properties}

This page is spared to find an extremely useful ingredient; for each root $\alpha$ a subalgebra $\lie{s}_\alpha$ which is isomorphic to the three-dimensional simple Lie algebra.
Before the next process we need some notation.
Since we have a non-degeneracy on $\lie{h}$, for any $\lambda \in \lie{h}^*$ we can find $T_\lambda \in \lie{h}$ such that $\kappa(T_\lambda, H) = \lambda(H)$.
From this notation we define a bilinear form of $\lie{h}^*$ as $(\lambda, \mu) = \kappa(T_\lambda, T_\mu)$.

First, one can immediately see that $-\alpha \in \Phi$ for each $\alpha \in \Phi$.
This is because if not, by the orthogonality of each two every root spaces (with $\lie{g}_0$) which we saw implies the degeneracy of $\kappa$.
In the same reason $\lie{g}_\alpha$ must not be orthogonal to $\lie{g}_{-\alpha}$.
Thus we can find nonzero $X_\alpha \in \lie{g}_\alpha$ and nonzero $Y_\alpha \in \lie{g}_{-\alpha}$ such that $\kappa(X_\alpha, Y_\alpha) \ne 0$.


If we let $S_\alpha = [X_\alpha, Y_\alpha]$, then $S_\alpha \in \lie{h}$. Also, we obtain that for any $H \in \lie{h}$ $\kappa([X_\alpha, Y_\alpha], H) = \kappa(X_\alpha, [Y_\alpha, H]) = \kappa(X_\alpha, Y_\alpha) \alpha(H)$, so $[X_\alpha, Y_\alpha] = \kappa(X_\alpha, Y_\alpha) T_\alpha$.
Then, if we denote $c = \kappa(X_\alpha, Y_\alpha) (\ne 0)$, we have that $[S_\alpha, X_\alpha] = c \alpha(T_\alpha) X_\alpha$ and $[S_\alpha, Y_\alpha] = -c \alpha(T_\alpha) Y_\alpha$ and $[X_\alpha, Y_\alpha] = S_\alpha$.
This rules says that $\lie{s}_\alpha = kX_\alpha + kY_\alpha + kS_\alpha$ is a subalgebra.

Suppose that $\alpha(T_\alpha) = 0$.
We have that $[\lie{s}_\alpha, \lie{s}_\alpha] = kS_\alpha$, which says that $\lie{s}_\alpha$ is solvable.
Since $\textrm{ad}$ is faithful, $\lie{s}_\alpha \cong \adu{\lie{g}}{\lie{s}_\alpha}$ so that $\adu{\lie{g}}{\lie{s}_\alpha} \le \lie{gl}(\lie{g})$ is solvable.
Then by Lie's theorem we have that, since $\adu{\lie{g}}{S_\alpha} \in [\adu{\lie{g}}{\lie{s}_\alpha}, \adu{\lie{g}}{\lie{s}_\alpha}]$, $\adu{\lie{g}}{S_\alpha}$ is nilpotent.
But we know that $\adu{\lie{g}}{S_\alpha}$ is semisimple.
Hence $S_\alpha = 0$, a contradiction.
Therefore, $\alpha(T_\alpha) \ne 0$.

Now we define $E_\alpha = X_\alpha$ and $F_\alpha = \frac{2}{c\alpha(T_\alpha)} Y_\alpha$ and $H_\alpha = \frac{2}{c\alpha(T_\alpha)} S_\alpha$.
Then, we have that $[H_\alpha, E_\alpha] = 2E_\alpha$ and $[H_\alpha, F_\alpha] = -2F_\alpha$ and $[E_\alpha, F_\alpha] = H_\alpha$.
This coincides exactly with the production rules of the three-dimensional simple Lie algebra.
Furthermore, $\lie{s}_\alpha = kH_\alpha + kE_\alpha + kF_\alpha$.
Therefore, $\lie{s}_\alpha$ is isomorphic to the three-dimensional simple Lie algebra.
(In addition, it yields that the three-dimensional simple Lie algebra is the smallest simple Lie algebra and there is no other three-dimensional (semi)simple Lie algebra, if the base field is algebraically closed\footnote{Otherwise, it does not hold; the Lie algebra $kX + kY + kZ$ with $[X, Y] = Z$ and $[Y, Z] = X$, $[Z, X] = Y$ is also simple but it is not isomorphic to $\lie{s}$.}.)
This subalgebra will play crucial roles for the theory of semisimple Lie algebras.

\newpage

\textbf{$\lie{s}_\alpha$-submodules; Integrality (1)}

We have the subalgebra $\lie{s}_\alpha$ for each $\alpha \in \Phi$ which is isomorphic to the three-dimensional simple Lie algebra.
It can be viewed in another way: $\ad{\lie{s}_\alpha}$ (If there is no any ambiguity, $\textrm{ad}$ means $\textrm{ad}_{\lie{g}}$), which is of course isomorphic to the three-dimensional simple Lie algebra.
This aspect gives that we can view $\lie{g}$ as a $\ad{\lie{s}_\alpha}$(or $\lie{s}_\alpha$)-module.
Since this module is completely reducible and we know the structure of all irreducible module of $\lie{s}_\alpha$, this view is an effective tool for the analysis of $\lie{g}$.
We will use this to certain submodules of $\lie{g}$.

The first is $\lie{p} = \bigoplus_{c \in k} \lie{g}_{c\alpha}$.
Since $\lie{g}_{c\alpha}$ is sent to $\lie{g}_{c\alpha}$, $\lie{g}_{(c+1)\alpha}$, $\lie{g}_{(c-1)\alpha}$ by $\ad{H_\alpha}$, $\ad{E_\alpha}$, $\ad{F_\alpha}$, respectively, $\lie{p}$ is a $\lie{s}_\alpha$-submodule.
Note that $\lie{s}_\alpha$ is an irreducible submodule of $\lie{p}$, and that each $\lie{g}_{c\alpha}$ is a weight space of $\lie{p}$.

Now suppose that $\dim{\lie{g}_{-\alpha}} > 1$.
Then, there is a subspace of $\lie{g}_{-\alpha}$ which does not contains $F_\alpha$, so there is another irreducible submodule $\lie{w}$ of $\lie{p}$ of which the intersection of $\lie{g}_{-\alpha}$ is not 0.
Since, by complete reducibility, we have that $\lie{w} \cap \lie{s}_\alpha = 0$.
It implies that for any $X \in \lie{w} \cap \lie{g}_{-\alpha}$ $[E_\alpha, X] = 0$.
Thus the subspace spanned by $((\ad{F_\alpha})^i X)$ ($i = 0, 1, 2, \cdots$) forms an irreducible submodule, which must be equal to $\lie{w}$.
But then it is immediate that its weights are negative all, a contradiction.
This implies that $\dim{\lie{g}_\alpha} = 1$ for all $\alpha \in \Phi$.


The next is supposing that there is  $c' \in k$ which is not an integer or a negative integer smaller than -1 such that $\lie{g}_{c\alpha} \ne 0$.

Then, since $\lie{g}_{c'\alpha}$ is a weight space, there is an irreducible submodule $\lie{u}$ different from $\lie{s}_\alpha$ which contains $\lie{g}_{c'\alpha}$.
Let $\lie{g}_{c'\alpha} = kX'$ for some $X' \in \lie{g}$.
We argue that there is $c = c' + m$ for some $m \in \NaN$ such that for all $i = 0, 1, 2, \cdots, m$ $(\ad{E_\alpha})^i X' \ne 0$ but $(\ad{E_\alpha})^{m + 1} X' = 0$.
When $c'$ is not an integer, since $|\Phi| < \infty$, there must be such $m$.
Assume that $c'$ is a negative integer.
In this case $c$ cannot excess -1 since, if not, $(\ad{E_\alpha})^i X'$ is in $\lie{s}_1$, a contradiction.
Now we denote $X = (\ad{E_\alpha})^m X'$.
Then $X \in \lie{u}$ and $(\ad{E_\alpha})X = 0$.
Thus the subspace spanned by $((\ad{F_\alpha})^i X)$ ($i = 0, 1, 2, \cdots$) forms an irreducible submodule, which must be equal to $\lie{u}$.
But then it is immediate that its weights are not integers or negative integers all, a contradiction.
Thus, such $\lie{u}$ cannot exist.
We did not consider the case for positive integer $c$ bigger than 1, however, this case also can be banned by replacing $\alpha$ by $-\alpha$.
Finally, we obtain that $\lie{p} = \lie{s}_\alpha$, so if $c\alpha \in \Phi$ for some $c \in k$, then $c = \pm 1$.
This is the first result of integrality of $\lie{g}$.

\newpage

\textbf{$\lie{s}_\alpha$-submodules; Integrality (2)}

The second target for $\lie{s}_\alpha$ is, for non-propositional $\alpha, \beta \in \Phi$ $\lie{q} = \bigoplus_{i \in \InZ} \lie{g}_{\beta + i\alpha}$.
It is immediate that for any $i$ $\lie{g}_{\beta + i\alpha}$ is sent to $\lie{g}_{\beta + i\alpha}$, $\lie{g}_{\beta + (i + 1)\alpha}$, $\lie{g}_{\beta + (i - 1)\alpha}$ by $\ad{H_\alpha}$, $\ad{E_\alpha}$, $\ad{F_\alpha}$, respectively.
Of course, every $\lie{g}_{\beta + i\alpha}$ is a weight space, so for each of them there is a submodule containing it.
Let $\lie{u}$ be the irreducible module containing $\lie{g}_\beta$, or $E_\beta$, and let $r \in \NaN_0$ be such that for any $i = 0, 1, \cdots, r$ $(\ad{F_\alpha})^i E_\beta \ne 0$ but $(\ad{F_\alpha})^{r + 1} E_\beta = 0$, and $s \in \NaN_0$ be such that for any $i = 0, 1, \cdots, s$ $(\ad{E_\alpha})^i E_\beta \ne 0$ but $(\ad{E_\alpha})^{s + 1} E_\beta = 0$.
(In the worst case, $r$ and $s$ is zero, so they are well-defined.)
Then $\lie{u}$ contains all $(\ad{F_\alpha})^i E_\beta$ and $(\ad{E_\alpha})^j E_\beta$ for $i = 0, 1, 2, \cdots, r$ and $j = 0, 1, 2, \cdots, s$.
Also, since $\lie{u}$ is irreducible, for any $i = 0, 1, 2, \cdots, r$ and $j = 0, 1, 2, \cdots, s$ $(\ad{E_\alpha})(\ad{F_\alpha})^i E_\beta$ and $(\ad{F_\alpha})(\ad{E_\alpha})^j E_\beta$ are not 0.
Thus, $\lie{u} = \bigoplus_{i = -r}^s \lie{g}_{\beta + i\alpha}$, which is $(r + s + 1)$-dimensional.
This result and the fact that $(\ad{H_\alpha})E_{\beta + s\alpha} = (\beta + s\alpha)(H_\alpha) E_{\beta + s\alpha}$ and $(\ad{H_\alpha})E_{\beta - r\alpha} = (\beta - r\alpha)(H_\alpha) E_{\beta - r\alpha}$ say that $-(\beta + s\alpha)(H_\alpha) = (\beta - r\alpha)(H_\alpha)$, or $r - s = \frac{2 \beta(H_\alpha)}{\alpha(H_\alpha)} = \frac{2(\beta, \alpha)}{(\alpha, \alpha)}$.
Hence we obtain that $\langle \beta, \alpha \rangle \equiv \frac{2(\beta, \alpha)}{(\alpha, \alpha)}$ is an integer for any root $\alpha$ and $\beta$.

Suppose that there is a root $\beta + q'\alpha$ where $q'$ is smaller than $-r$.
Applying some power of $\ad{E_\alpha}$, we can find $q \in \InZ$ such that $\beta + q\alpha$ is a root but $(\ad{E_\alpha})E_{\beta + q\alpha} = 0$.
Let $\lie{w}$ be the irreducible module containing $E_{\beta + q\alpha}$.
It is then immediate that the subspace spanned by all $(\ad{F_\alpha})^i E_{\beta + q\alpha}$ must be equal to $\lie{u}$.
But all of the weight of this module is negative since $(\beta + q\alpha)(H_\alpha) < (\beta - r\alpha)(H_\alpha)$, a contradiction.
Thus for any $q < -r$ $\beta + q\alpha \notin \Phi$.
Also the same argument with $\alpha$ replaced by $-\alpha$ shows that for any $q > s$ $\beta + q\alpha \notin \Phi$.
Therefore, $\beta + i\alpha$ is a root if and only if $i = -r, -r + 1, -r + 2, \cdots, s - 1, s$.
We call such sequence $\beta + i\alpha$ of roots \textbf{the $\alpha$-string through $\beta$}.

Now with this result we show some properties.
At first, if $\beta + \alpha$ is a root, then in the above process $(\ad{E_\alpha}) E_\beta \ne 0$, which implies that $[\lie{g}_\alpha, \lie{g}_\beta] = \lie{g}_{\alpha + \beta}$.
Also, in any case $-\langle \beta, \alpha \rangle = s - r$ is one of $-r, -r + 1, -r + 2, \cdots, s - 1, s$, $\beta - \langle \beta, \alpha \rangle \alpha$ is a root.

\newpage

\textbf{Rationality; Prepare to root system}

In this page we abstract the essential structure of semisimple Lie algebra.
At first, let $X \in \lie{h}$ such that $\alpha(X) = 0$ for all root $\alpha$.
Then $[X, E_\alpha] = 0$ for any root $\alpha$.
Since we have $[X, \lie{h}] = 0$, we have that $[X, \lie{g}] = 0$, so $X$ is in the center of $\lie{g}$, which is 0.
This result implies that $\Phi$ spans whole $\lie{h}^*$.

There is one more thing to prepare the abstraction.
Let $(H_i)$ be a basis of $\lie{h}$.
Then the set of all $H_i$ and all $E_\alpha$ for $\alpha \in \Phi$ forms a basis of $\lie{g}$, where each of them is an eigenvector of $\ad{H}$ for any $H \in \lie{h}$.
Thus we obtain that for $H, H' \in \lie{h}$ $\kappa(H, H') = \tr{(\ad{H})(\ad{H'})} = \sum_{\alpha \in \Phi} \dim{\lie{g}_\alpha} \alpha(H) \alpha(H') = \sum_{\alpha \in \Phi} \alpha(H) \alpha(H')$.
Especially, $(\lambda, \mu) = \sum_{\alpha \in \Phi} \alpha(T_\lambda) \alpha(T_\mu) = \kappa(T_\lambda, T_\mu) = \sum_{\alpha \in \Phi} (\alpha, \lambda) (\alpha, \mu)$ for $\lambda, \mu \in \lie{h}^*$.

Now let's start to the abstraction.
Since $\Phi$ spans $\lie{h}^*$, we can find a basis $(\alpha_1, \cdots, \alpha_l)$ in $\Phi$.
With this we can write any $\alpha \in \Phi$ as $\sum a_i \alpha_i$.
We obtain that $\langle \alpha, \alpha_i \rangle = \sum \langle \alpha_j, \alpha_i \rangle a_j$.
Note that since $(\cdot, \cdot)$ is non-degenerate the matrix $(\alpha_j, \alpha_i)$ is invertible, thus so is the matrix $\langle \alpha_j, \alpha_i \rangle$.
Hence we can obtain $a_i$ as a rational function of $\langle \alpha, \alpha_i \rangle$'s and $\langle \alpha_j, \alpha_i \rangle$'s.
However, these components are all integer.
Thus all $a_i$ must be rational, that is, every $\alpha \in \Phi$ is a $\RaQ$-linear combination of the basis.

We return to the formula $(\lambda, \mu) = \sum_{\alpha \in \Phi} (\alpha, \lambda) (\alpha, \mu)$.
From this we have that for any $\beta \in \Phi$ $(\beta, \beta) = \sum_{\alpha \in \Phi} (\beta, \alpha)^2$.
Multiplying $4 / (\beta, \beta)^2$, we obtain that $\frac{4}{(\beta, \beta)} = \sum_{\alpha \in \Phi} \langle \beta, \alpha \rangle^2$.
From this, we have that $(\beta, \beta)$ is rational.
Also, from the definition $\langle \alpha, \beta \rangle = \frac{2(\alpha, \beta)}{(\beta, \beta)}$ we also obtain that $(\alpha, \beta)$ is also rational.

Furthermore, the formula for $(\beta, \beta)$ says that every $(\beta, \beta)$ is nonnegative.
Since we saw that $\alpha(T_\alpha) \ne 0$, $(\beta, \beta)$ is even positive.
Therefore, the restriction of our bilinear form on $\lie{h}^*_\RaQ$, which is $\RaQ$-spanned subspace by $\alpha_i$'s, must be positive-definite.

Now we summarize the essential properties of $\Phi$.

(RS1) $\Phi$ spans whole of the given vector space ($\lie{h}^*_\RaQ$) but it is finite and $0 \notin \Phi$

(RS2) for $\alpha \in \Phi$ $-\alpha \in \Phi$ and if $c\alpha \in \Phi$ for a scalar $c$, then $c = \pm 1$

(RS3) for $ \alpha, \beta \in \Phi$ $\frac{2(\beta, \alpha)}{(\alpha, \alpha)} \in \InZ$

(RS4) for $ \alpha, \beta \in \Phi$ $\beta - \frac{2(\beta, \alpha)}{(\alpha, \alpha)} \alpha \in \Phi$

And we call the set of $\langle \beta, \alpha \rangle$ the \textbf{root configuration}.
Also, the set $\Phi$ is, if exists, a \textbf{root system}.
We found a set $\Phi$ in $\lie{h}^*_\RaQ$ for a semisimple Lie algebra.
Now let $E$ be a $l$-dimensional vector space over $k'$, where $k'$ is any of $\RaQ$, $\ReR$ and any ordered fields, with a positive-definite symmetric bilinear form $(\cdot, \cdot)$.
We then get a reverse: If for a given root configuration there is no subset of $E$ for any $k'$ satisfying (RS1)$\sim$(RS4) with the root configuration, then there is no semisimple Lie algebra with the root configuration.
Therefore, the classification of semisimple Lie algebras is reduced to finding all root configuration to which the corresponding root system exists.

\newpage

\textbf{Basic properties of root systems}

(In this and the following pages $E$ is a $l$-dimensional vector space over an ordered field $k'$ (especially $\RaQ$ or $\ReR$) with a positive-definite symmetric bilinear form $(\cdot, \cdot)$, and $\Phi$ is a root system in $E$.)

We defined roots systems.
To do more with this we need something.
We have (RS3); for $\alpha, \beta \in \Phi$ $\langle \beta, \alpha \rangle = \frac{2(\beta, \alpha)}{(\alpha, \alpha)} \in \InZ$.
This condition will give some interesting properties automatically.
We have that $\langle \beta, \alpha \rangle \langle \alpha, \beta \rangle = 4 \cdot \frac{(\beta, \alpha)^2}{(\alpha, \alpha) (\beta, \beta)}$.
Remind that with every positive-definite bilinear form for any non-proportional two vector $\alpha, \beta$ $(\alpha, \beta)^2 < (\alpha, \alpha) (\beta, \beta)$ (Cauchy-Schwartz inequality).
Thus $\langle \beta, \alpha \rangle \langle \alpha, \beta \rangle < 4$.
Furthermore, by (RS3), it can be only 0, 1, 2, 3.
Some of direct result of this is that the signs of $\langle \beta, \alpha \rangle$ and $\langle \alpha, \beta \rangle$ are same and that if one of $\langle \beta, \alpha \rangle$ and $\langle \alpha, \beta \rangle$ is nonzero but not $\pm 1$, then the other must be $\pm 1$.

This result gives a useful property.
Let $\alpha, \beta \in \Phi$ and assume that $(\alpha, \beta) > 0$.
Then, by the above discussion one of $\langle \beta, \alpha \rangle$ and $\langle \alpha, \beta \rangle$ is 1.
Thus by (RS4) one of $\beta - \alpha$ and $\alpha - \beta$ is in $\Phi$.
But by (RS2) in either case $\alpha - \beta$ is in $\Phi$.
In summary, we have that \textit{for $\alpha, \beta \in \Phi$ if $(\alpha, \beta) > 0$, then $\alpha - \beta \in \Phi$}.
By the same method in which all $\beta$ is replaced by $-\beta$, we can obtain that \textit{for $\alpha, \beta \in \Phi$ if $(\alpha, \beta) < 0$, then $\alpha + \beta \in \Phi$}.

\newpage

\textbf{Base}

We declared that we attack the classification of root system due to that of semisimple Lie algebras.
But actually it can be more reduced, which means that there is a stuff which contains the essential features for root systems.
We know that root system must have a something like a basis.
But there is a something more in any root space, called a \textbf{base}.
We define a base $\Delta$ which is a subset of a root system $\Phi$ of if it satisfies the followings:

(B1) $\Delta$ is a basis of $E$

(B2) Every element in $\Phi$ is a nonnegative or nonpositive $\InZ$-linear combination of $\Delta$.

(which means that when $\alpha = \sum_{\alpha \in \Delta} a_\alpha \alpha$, $a_\alpha \in \InZ_+ \cup \{0\}$ or $a_\alpha \in \InZ_- \cup \{0\}$.)

The (B2) seems too strong.
But our conditions (RS1)$\sim$(RS4) guarrantee the existence of base.
To see this we need a useful ingredient.
We define $P_\alpha = \{\lambda \in E \SBar (\lambda, \alpha) = 0\}$.
Since $\Phi$ is finite, $E - \bigcup_{\alpha \in \Phi} P_\alpha$ is not empty.
We call an element in this set \textbf{regular}.
By definition, of course, for any regular $\gamma$ $(\gamma, \alpha) \ne 0$ for all $\alpha \in \Phi$.

We fix a regular $\gamma$, and then we define $\Phi^+(\gamma) = \{\alpha \in \Phi \SBar (\alpha, \gamma) > 0\}$ and $\Phi^-(\gamma) = \{\alpha \in \Phi \SBar (\alpha, \gamma) < 0\}$.
Note that $\Phi$ is the disjoint union of $\Phi^+(\gamma)$ and $\Phi^-(\gamma)$, and $-\Phi^+(\gamma) = \Phi^-(\gamma)$.
We also define something more.
We call $\alpha \in \Phi^+(\gamma)$ \textbf{decomposable} if $\alpha$ is a sum of two elements in $\Phi^+(\gamma)$, or \textbf{indecomposable} if it is not decomposable.

Now we denote the set of all indecomposable elements by $\Delta(\gamma)$.
We shall show that \textit{$\Delta(\gamma)$ is a base}.
First, we shall show that $\Delta(\gamma)$ satisfies (B2).
Obviously, it is sufficient to show that every $\alpha \in \Phi^+(\gamma)$ can be written as a nonnegative $\InZ$-linear combination of $\Delta(\gamma)$.
That is, if $M$ is the set of such $\alpha \in \Phi^+(\gamma)$, we shall show that $\Phi^+(\gamma) = M$.
Note that $\Delta(\gamma)$ is contained in $M$, obviously.
Suppose that $\Phi^+(\gamma) - M$ is not empty.
Since $\Phi$ is finite, we can choose $\alpha$ in this set so that $(\alpha, \gamma)$ is minimal.
As mentioned, $\alpha$ is decomposable, so there are $\beta, \beta' \in \Phi^+(\gamma)$ such that $\alpha = \beta + \beta'$.
If $\beta$ and $\beta'$ are all in $M$, then obviously $\alpha \in M$.
Hence, at least one of them must not be in $M$.
It does not harm the generality if we assume that $\beta$ is not in $M$.
But, then $(\beta, \gamma) = (\alpha, \gamma) - (\beta', \gamma) < (\alpha, \gamma)$, a contradiction to the minimality of $(\alpha, \gamma)$.
Therefore, $\Phi^+(\gamma) = M$, which we want.

By (RS1), if (B2) holds, then $\Delta(\gamma)$ spans $E$.
Thus the remaining is to show the linear independence of $\Delta(\gamma)$.
First, we need to show that for any distinct $\alpha, \beta \in \Delta(\gamma)$ $(\alpha, \beta) \le 0$.
But it is direct from the result of the previous page; if not, that is, $(\alpha, \beta) > 0$, then $\alpha - \beta \in \Phi$, a contradiction to (B2).
Now we consider scalars $a_{\alpha}$ making $\sum_{\alpha \in \Delta(\gamma)} a_{\alpha} \alpha = 0$.
We denote $\Delta_+ = \{\alpha \in \Delta(\gamma) \SBar a_{\alpha} \ge 0\}$ and $\Delta_- = \{\alpha \in \Delta(\gamma) \SBar a_{\alpha} < 0\}$.
Then $\beta = \sum_{\alpha \in \Delta_+} a_{\alpha} \alpha = -\sum_{\alpha \in \Delta_-} a_{\alpha} \alpha$.
But $(\beta, \beta) = \sum_{\alpha \in \Delta_+, \alpha' \in \Delta_-} a_{\alpha} (-a_{\alpha'}) (\alpha, \alpha')$, which is nonpositive, so $\beta = 0$.
Finally, $0 = (\beta, \gamma) = \sum_{\alpha \in \Delta_+} a_{\alpha} (\alpha, \gamma) = -\sum_{\alpha \in \Delta_-} a_{\alpha} (\alpha, \gamma)$ requires $a_{\alpha} = 0$.
Therefore, $\Delta(\gamma)$ is linearly independent, so (B1) holds.
Then $\Delta(\gamma)$ is a base of $\Phi$, so we have found a basis.

For the later, we define an ordering in $E$: for $\lambda , \mu \in E$ we denote $\lambda \succ \mu$ when $\lambda - \mu$ can be written as a nonnegative linear combination of $\Delta$.

\newpage

\textbf{Weyl chamber}

We have seen that there exists a base.
We said that bases contain the essentials of root systems.
It will be seen that if we 'have a base' (we will explain what this words mean), then we can construct whole root system.
But there is an obscure; a root system may not contain a unique base.
Actually, every root system does, since for a base $\Delta$ $-\Delta$ is also a base, and the situation is in general even complicate.
Hence we have to find an essential information of bases which is unique in all bases of a fixed root system.
This page and some of the next pages are spared to treat this issue.

We can go further.
Let $\Delta = (\alpha_1, \alpha_2, \cdots, \alpha_l)$ be a base.
Then we can show that there is a regular $\gamma$ such that $\Delta = \Delta(\gamma)$.
First, we denote the set of $\alpha \in \Phi$ which is a nonnegative $\InZ$-linear combination of $\Delta$ by $\Phi^+(\Delta)$ and the set of $\alpha \in \Phi$ which is a nonpositive $\InZ$-linear combination of $\Delta$ by $\Phi^-(\Delta)$.
Then by (B2) $\Phi$ is the disjoint union of $\Phi^+(\Delta)$ and $\Phi^-(\Delta)$, and $\Phi^-(\Delta) = -\Phi^+(\Delta)$.
Next, we need to find a regular $\gamma$ such that $(\alpha_i \gamma) > 0$ for all $i$.
To do this we let $c_i$ be arbitrary positive scalars.
If we can find $\gamma = \sum_i a_i \alpha_i$ such that $\sum_j (\alpha_i, \alpha_j) a_j = c_i$, then our purpose is archived, but it is obvious since $(\alpha_i)$ is a basis so that the matrix $(\alpha_i, \alpha_j)$ is invertible.
Now then it is immediate that $\Phi^+(\gamma) = \Phi^+(\Delta)$ and $\Phi^-(\gamma) = \Phi^-(\Delta)$.
The remaining is to show that $\Delta = \Delta(\gamma)$.
Since these cardinalities are same, it is sufficient to show that all $\alpha_i$ is indecomposable.
Suppose that there is a decomposable $\alpha_i$.
Then $\alpha_i = \beta + \beta'$ for some $\beta, \beta' \in \Phi^+(\gamma)$.
But then $\beta, \beta' \in \Phi^+(\Delta)$, so they are nonnegative $\InZ$-linear combination of $\Delta$.
In this situation the only way to make $\alpha_i = \beta + \beta'$ is to set one of $\beta$ and $\beta'$ to be 0, a contradiction.
Therefore, all element in $\Delta$ is indecomposable, so $\Delta = \Delta(\gamma)$.

We have that for every regular $\gamma$ there is a base $\Delta(\gamma)$ and, conversely, for every base $\Delta$ there is a regular $\gamma$ such that $\Delta = \Delta(\gamma)$.
Now we let $\mathcal{W}_\Delta = \{\gamma \in E \SBar \textrm{$\gamma$ is regular, } \Delta(\gamma) = \Delta\}$.
We call this set a \textbf{(fundamental) Weyl chamber relative to $\Delta$}.
Since every regular $\gamma$ is contained in $\mathcal{W}_{\Delta(\gamma)}$, the union of all Weyl chambers is the set of all regular elements.
Also, it is obvious that the family of all Weyl chambers is a partition of the set of all regular elements, or the mapping $\Delta \mapsto \mathcal{W}_\Delta$ is injective.
Of course, this mapping is surjective.
Hence, we have a one-to-one correspondence of the set of all bases to the family of all Weyl chambers.
This correspondence will help us to analysis the structure of bases.

For the representation theory we need something more.
Note that, actually, $\mathcal{W}_\Delta = \{\gamma \in E \SBar (\alpha, \gamma) > 0 \textrm{ for all $\alpha \in \Delta$}\}$ (the definition says that every element in this set is automatically regular).
We also denote $\overline{\mathcal{W}_\Delta} = \{\gamma \in E \SBar (\alpha, \gamma) \ge 0 \textrm{ for all $\alpha \in \Delta$}\}$.
This set is called a \textbf{the closure of Weyl chamber relative to $\Delta$}.

\newpage

\textbf{Weyl group (1) Some basic properties}

Remind (RS4): For every $\alpha, \beta \in \Phi$ $\beta - \frac{2(\beta, \alpha)}{(\alpha, \alpha)} \alpha \in \Phi$.
But we know that $\sigma_\alpha : \lambda \mapsto \lambda - \frac{2(\lambda, \alpha)}{(\alpha, \alpha)} \lambda$ is a reflection.
We call this reflection the \textbf{Weyl reflection}.
Also, we can consider a subgroup $W$ of $GL(E)$ generated by all $\sigma_\alpha$ ($\alpha \in \Phi$), called the \textbf{Weyl group}.

Note that every element in the Weyl group leaves $\Phi$ invariant, and that, since $\Phi$ spans $E$, any linear mapping fixing $\Phi$ is identity.
Thus every element in the Weyl group is determined uniquely by how it permutes $\Phi$, for which the number of the way is finite.
Hence, $W$ is finite.

To go further, we need some lemma.
Let $\sigma$ be a linear map of $E$ leaving $\Phi$ invariant and fixing $P_\lambda$ for a $\lambda \in E$ and sending $\alpha$ to $-\alpha$ (of course, $\alpha$ is not in $P$).
Then $\sigma = \sigma_\alpha$ (so $\lambda = \alpha$).
To show this let $\tau = \sigma \sigma_\alpha$.
Since $\tau$ permutes $\Phi$ and some power of any permutation must be identity, there is $r \in \NaN$ such that $\tau^r = 1$.
Thus the minimal polynomial of $\tau$ must divide $t^r - 1$.
On the other hand, the assumption gives that the induced mapping of $\tau$ on $E/k'\alpha$ must be identity and that $\tau(\alpha) = \alpha$, so the minimal polynomial of $\tau$ must be in the form of $(t - 1)^m$.
Therefore, the minimal polynomial of $\tau$ must be $t - 1$, which implies that $\tau = 1$, so that $\sigma = \sigma_\alpha$.

By this lemma we can prove the following lemma: 
If $\sigma \in L(V)$ leaves $\Phi$ invariant (so, directly, invertible), then for every $\alpha \in \Phi$ $\sigma \sigma_\alpha \sigma^{-1} = \sigma_{\sigma(\alpha)}$, and moreover for every $\alpha, \beta \in \Phi$ $\langle \sigma(\beta), \sigma(\alpha) \rangle = \langle \beta, \alpha \rangle$.
Let $\lambda \in E$.
Then $(\sigma \sigma_\alpha \sigma^{-1})(\sigma(\lambda)) = \sigma(\lambda) - \frac{2(\lambda, \alpha)}{(\alpha, \alpha)} \sigma(\alpha)$.
We can see that $\sigma \sigma_\alpha \sigma^{-1}$ leaves $\Phi$ invariant and fixes the inverse image of $\sigma$ from $P_\alpha$, which is $P_\lambda$ for some $\lambda \in E$, and sends $\sigma(\alpha)$ to $-\sigma(\alpha)$.
Thus by the above lemma $\sigma \sigma_\alpha \sigma^{-1} = \sigma_{\sigma(\alpha)}$.
Also, for any $\alpha, \beta \in \Phi$ we have that $\sigma(\beta) - \langle \sigma(\beta), \sigma(\alpha) \rangle \sigma(\alpha) = \sigma_{\sigma(\alpha)} (\sigma(\beta)) = \sigma(\beta) - \langle \beta, \alpha \rangle \sigma(\alpha)$, so that $\langle \sigma(\beta), \sigma(\alpha) \rangle = \langle \beta, \alpha \rangle$.

We introduce another lemma.
If a base $\Delta$ is fixed, we call $\alpha \in \Phi$ \textbf{positive} if $\alpha \in \Phi^+(\Delta)$ and \textbf{negative} if $\alpha \in \Phi^-(\Delta)$.
Let $\alpha \in \Delta$.
Then for any $\beta = \sum_{\alpha' \in \Delta} a_{\alpha'} \alpha' \in \Phi^+(\Delta)$ but $\beta \ne \alpha$ we have that $\sigma_\alpha(\beta) = \sum_{\alpha \ne \alpha' \in \Delta} a_{\alpha'} \alpha' - (\cdots) \alpha$.
Since $\beta \ne \alpha$, (with (RS2)) one of $a_{\alpha'}$ ($\alpha' \ne \alpha$) must be positive, which implies that $\sigma_\alpha(\beta) \in \Phi^+(\Delta)$, whatever the coefficient of $\alpha$ is (of course, it must be positive).
Hence, for every $\alpha \in \Delta$ $\sigma_\alpha$ permutes all positive roots except for $\alpha$.

There is one interesting corollary.
Let $\delta' = \frac{1}{2} \sum_{\alpha \ne \alpha' \in \Phi^+(\Delta)} \alpha'$.
The above lemma says that $\sigma_\alpha(\delta') = \delta'$.
Now we let $\delta = \frac{1}{2} \sum_{\alpha' \in \Phi^+(\Delta)} \alpha'$.
Then $\delta = \delta' + \frac{1}{2} \alpha$.
From this we obtain that $\sigma_\alpha(\delta) = \delta - \alpha$.

\newpage

\textbf{Weyl group (2) Structure of bases}

The aim of this page is to show that for any bases $\Delta$ and $\Delta'$ there is $\sigma \in W$ such that $\sigma(\Delta') = \Delta$.
To do this we first see the action of $W$ on the set of bases.
Let $\gamma \in E$ be regular such that $\Delta(\gamma) = \Delta$ and $\sigma \in W$.
Now we investigate $\sigma(\Delta) = \sigma(\Delta(\gamma))$.
Of course, since $\sigma$ is isometry, $\sigma(\Delta(\gamma)) \subseteq \Phi^+(\sigma(\gamma))$.
Also, it is immediate that $\sigma(\Phi^+(\gamma)) = \Phi^+(\sigma(\gamma))$.
If for some $\alpha \in \Delta(\gamma)$ $\sigma(\alpha)$ is decomposable in $\Phi^+(\sigma(\gamma))$, that is, $\sigma(\alpha) = \beta + \beta'$ for some $\beta, \beta' \in \Phi^+(\sigma(\gamma))$, then $\alpha = \sigma^{-1} (\beta) + \sigma^{-1} (\beta')$ but $\sigma^{-1} (\beta), \sigma^{-1} (\beta') \in \Phi^+(\gamma)$, a contradiction, so $\sigma(\Delta(\gamma)) \subseteq \Delta(\sigma(\gamma))$.
Therefore, since they have same cardinality, $\sigma(\Delta(\gamma)) = \Delta(\sigma(\gamma))$.
We obtain then that $W$ acts on the set of bases as a permutation.
Also, using the the correspondence of the bases to Weyl chambers one can say that \textit{$W$ acts on the set of Weyl chambers as a permutation, where $\sigma(\mathcal{W}_\Delta) = \mathcal{W}_{\sigma(\Delta)}$}.

It is not just what we want yet.
What we really want is the transitivity of this action.
Now we shall show this.
It is more convenient to send a regular element than a base.
Note that if $(\alpha, \gamma) > 0$ for all $\alpha \in \Delta$, then $\gamma \in \mathcal{W}_\Delta$, so that $\Delta(\gamma) = \Delta$.
It implies that if $\gamma \in \mathcal{W}_{\Delta'}$ and we can find $\sigma \in W$ such that $(\alpha, \sigma(\gamma)) > 0$ for all $\alpha \in \Delta$, then $\sigma(\Delta') = \sigma(\Delta(\gamma)) = \Delta$, which we want.
Thus our aim is to show that \textit{for any regular $\gamma$ and base $\Delta$ there is $\sigma \in W$ such that $(\alpha, \sigma(\gamma)) > 0$ for all $\alpha \in \Delta$ (or $\sigma(\gamma) \in \mathcal{W}_\Delta$)}.

To do this we remind that for any $\alpha \in \Delta$ $\sigma_\alpha(\delta) = \delta - \alpha$, where $\delta = \frac{1}{2} \sum_{\alpha' \in \Phi^+(\Delta)} \alpha'$.
Since $W$ is finite, we can choose $\sigma \in W$ which is generated by $\sigma_\alpha$ for $\alpha \in \Delta$, such that $(\delta, \sigma(\gamma))$ is maximal.
(The condition for the generators are not necessary, but it is needed for the other purpose.)
Now for any $\alpha \in \Delta$ we have that, by the maximality, $(\delta, \sigma(\gamma)) \ge (\delta, (\sigma_\alpha \sigma)(\gamma)) = (\sigma_\alpha(\delta), \sigma(\gamma)) = (\delta - \alpha, \sigma(\gamma))$, so that $(\alpha, \sigma(\gamma)) \ge 0$ for all $\alpha \in \Delta$, where the equality cannot hold since $\sigma(\gamma)$ is regular.
Therefore, $\sigma$ is what we want, so \textit{the action of $W$ on the set of bases (or on the set of the Weyl chambers) is transitive}.

In addition, let $\lambda \in E$ be arbitrary, and let M = $\{\sigma(\lambda) \SBar \sigma(\lambda) \succ \lambda, \sigma \in W\}$ and $\sigma \in W$ be such that $\sigma(\lambda)$ is maximal of $M$ in the ordering (such $\sigma$ exists since $\sigma$ can be $1$.)
If $(\sigma(\lambda), \alpha_i) < 0$, then $\sigma_i(\sigma(\lambda)) \succ \sigma(\lambda) \succ \lambda$, so $\sigma_i(\sigma(\lambda)) \in M$, a contradiction, so we can say that \textit{there is $\sigma$ such that $\sigma(\lambda) \in \overline{\mathcal{W}_\Delta}$ and $\sigma(\lambda) \succ \lambda$}.

We found $\sigma \in W$ such that $\Delta' = \sigma(\Delta)$ for any bases $\Delta$ and $\Delta'$.
Let $\Delta = (\alpha_1, \alpha_2, \cdots, \alpha_l)$.
Then, as we noted, $\langle \sigma(\alpha_i), \sigma(\alpha_j) \rangle = \langle \alpha_i, \alpha_j \rangle$.
Thus we can say that the matrix $(\langle \alpha_i, \alpha_j \rangle)$ is preserved on all bases.
That is, the matrix $(\langle \alpha_i, \alpha_j \rangle)$, called the \textbf{Cartan matrix}, is a character of base in the given root system.
We will see that it is the essence of whole structure of root systems.

One more thing: Let $\alpha \in \Phi$.
Then we can always find a base containing $\alpha$.
To see this let $\gamma' \in E$ be in $P_\alpha$ but not in $P_\beta$ for all $\pm \alpha \ne \beta \in \Phi$ (such $\gamma'$ can be found always).
Next, let $\gamma = \gamma + \frac{\epsilon}{(\alpha, \alpha)} \alpha$, so that $(\alpha, \gamma) = \epsilon$, where $\epsilon$ is chosen to be $|(\beta, \gamma)| < \epsilon$ for all $\pm \alpha \ne \beta \in \Phi$ (since $|\Phi|$ is finite, we can choose such $\epsilon$).
Then $\gamma$ is regular and, by the minimality of $(\alpha, \gamma)$, $\alpha$ must be indecomposable in $\Phi^+(\gamma)$, so that $\alpha \in \Delta(\gamma)$, which we seek.
The above argument says then that \textit{for every $\alpha \in \Phi$ and base $\Delta$ there is $\sigma \in W$ (especially, which is generated by $\sigma_\beta$ for $\beta \in \Delta$) such that $\sigma(\alpha) \in \Delta$}.

\newpage

\textbf{Weyl group (3) More on}

We can see a pretty result with the results of the previous page.
In this page we fix a base $\Delta$ and denote $\Delta = (\alpha_1, \alpha_2, \cdots, \alpha_l)$.
Also, we denote $\sigma_i = \sigma_{\alpha_i}$.
Let $\alpha \in \Phi$ and $\Delta$ a base.
We found that there is $\sigma \in W$ which is generated by $\sigma_i$'s and satisfies that $\sigma(\alpha) = \sigma_j$ for some $j$.
Now we consider $\sigma_\alpha$, which is $\sigma_{\sigma^{-1} \alpha_j} = \sigma^{-1} \sigma_j \sigma$.
Then the right hand side is a product of $\sigma_i$'s.
Therefore, we obtain that \textit{$W$ is generated by $\sigma_i$'s}.

We shall see one more stuff: \textit{simplicity of the action of $W$ on bases}.
To see this we need a lemma.
\textit{If $(\sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_{m - 1}})(\alpha_{i_m})$ is negative, then $\sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_m} = \sigma_{i_1} \cdots\sigma_{i_{s - 1}} \sigma_{i_{s + 1}} \cdots \sigma_{i_{m - 1}}$ for some $1 \le s \le m - 1$}.
Note that we have seen that $\sigma_i$ permutes all positive roots except $\alpha_i$.
In the same reason, we have that $\sigma_i$ permutes all negative roots except $-\alpha_i$.
We use induction.
When $m = 2$, that is, $\sigma_{i_1}(\alpha_{i_2})$ is negative, the only way for it to be possible is $i_1 = i_2$, so $ \sigma_{i_1} \sigma_{i_2} = 1$.
Now we assume that for some $m \in \NaN$ our statement holds.
We consider $(\sigma_{i_0} \sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_{m - 1}})(\alpha_{i_m})$ which is negative.
Then, by the above note, we have two cases: $(\sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_{m - 1}})(\alpha_{i_m}) \equiv \sigma(\alpha_{i_m})$ is negative or equal to $\alpha_{i_0}$.
The first case is immediate by the induction hypothesis.
We see the second case.
In this case $\alpha_{i_0} = \sigma(\alpha_{i_m})$.
Thus $\sigma_{\alpha_{i_0}} = \sigma_{\sigma(\alpha_{i_m})} = \sigma \sigma_{i_m} \sigma^{-1}$, or $\sigma_{i_0} \sigma \sigma_{i_m} = \sigma$.
This is the form which we want, where $s = 0$.
Therefore, by induction, we have the lemma.

From this we have a corollary.
If $\sigma \in W$ is written as $\sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_m}$, where $m$ is as short as possible, then $(\sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_m})(\alpha_{i_m})$ must be negative: if not, $(\sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_{m - 1}})(\alpha_{i_m})$ is negative, thus $\sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_m}$ can be reduced, a contradiction to the minimality of $m$.
This corollary implies that if $\sigma \in W$ is not 1, then it sends at least one of $\alpha_i$ to a negative root.
Thus, the only element in $W$ leaving any given base invariant is 1.
Therefore, we have the simplicity.

The above lemma says something more.
For any $1 \ne \sigma \in W$ there is the minimal $m \in \NaN$ such that $\sigma = \sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_m}$.
Such $m$ is denoted by $l(\sigma)$, called the \textbf{length} of $\sigma$.
We define $l(1) = 0$.
On the other hand, we denote $n(\sigma) = |\{\alpha \in \Phi^+ \SBar \sigma(\alpha) < 0\}|$.
We shall show that $l(\sigma) = n(\sigma)$ for all $\sigma \in W$.
We assume that for any $\sigma \in W$ with length $m - 1$ we have $l(\sigma) = n(\sigma) = m - 1$, and let $\sigma \in W$ be with length $m$, and $\sigma = \sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_m}$.
We saw above that $\sigma_{i_m}$ preserves the set of all negative roots except for $-\alpha_{i_m}$ and that $(\sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_m})(\alpha_{i_m})$ is negative.
Thus $n(\sigma \sigma_{i_m}) = n(\sigma) - 1$, but by the induction hypothesis $n(\sigma \sigma_{i_m}) = \sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_{m - 1}} = m - 1$, so that $n(\sigma) = m$.
Therefore, by the induction on $l(\sigma)$, we obtain the lemma.

We introduce another lemma which is relative to the closure of a Weyl chamber.
\textit{Let $\lambda, \mu \in \overline{\mathcal{W}_\Delta}$.
If $\sigma(\lambda) = \mu$ for $\sigma \in W$, then $\sigma$ is a product of $\sigma_i$'s which fix $\lambda$ so that $\lambda = \mu$.}
First, let $l(\sigma) = 1$, i.e., $\sigma = \sigma_i$.
Since $(\lambda, \alpha_i) \ge 0$ and also $(\sigma(\lambda), \alpha_i) \ge 0$, $(\lambda, \alpha_i) = 0$, so $\sigma_i(\lambda) = \lambda$.
We assume that for some $r \in \NaN$ if $l(\sigma) < r$, then $\sigma$ is a product $(l(\sigma))$-$\alpha_i$'s which fix $\lambda$, and we let $l(\sigma) = r$.
We write $\sigma = \sigma_{i_1} \sigma_{i_2} \cdots \sigma_{i_r}$ and denote $i = i_1$.
Note that by the above lemma $\sigma^{-1}(\alpha_i)$ must be negative.
Thus $(\lambda, \sigma^{-1}(\alpha_i) \le 0$ so that $(\sigma(\lambda), \alpha_i) = 0$, or $\sigma_i$ fixes $\sigma(\alpha_i)$.
Hence $(\sigma_i \sigma)(\lambda) = \mu$.
But since $l(\sigma_i \sigma) = r - 1$, by the induction hypothesis, we can say that all $\sigma_{i_j}$ ($j>1$) fix $\lambda$.
Thus $\sigma(\lambda) = \sigma_{i_1}(\lambda) = \mu$, so $\sigma_{i_1}$ also fixes $\lambda$.
Finally, by the induction, we obtain the lemma.

\newpage

\textbf{Irreducible root system}

Before abtracting the essence we need to investigate something.
If a root system $\Phi$ has a non-empty subset such that it and its complementary are relatively orthogonal, we call such subset a \textbf{component}.
We call a root system $\Phi$ \textbf{irreducible} if there is no proper component.
We can also say irreducibility for a base in the same sense.
Here is a theorem.
Fix a base $\Phi$.
$\Phi$ is irreducible if and only if $\Delta$ is irreducible.

Proof: Assume that $\Phi$ is not irreducible so that $\Phi_1$ and $\Phi_2$ is relatively orthogonal and their disjoint union is $\Phi$.
Then, considering $\Delta = (\Delta \cap \Phi_1) \cup (\Delta \cap \Phi_2)$ and the fact that for any $\beta \in \Phi$ but $\beta \notin \Delta$ there is $\alpha \in \Delta$ such that $(\beta, \alpha) > 0$, $\Delta$ is obviously not irreducible.

Conversely, assume that $\Delta$ is not irreducible so that $\Delta_1$ and $\Delta_2$ is relatively orthogonal and their disjoint union is $\Delta$.
Now we denote $\Delta_1 = (\alpha_1, \cdots, \alpha_r)$ and $\Delta_2 = (\alpha'_1, \cdots, \alpha'_s)$, and $\sigma_i = \sigma_{\alpha_i}$ and $\sigma'_i = \sigma_{\alpha'_i}$.
Choose $\alpha \in \Phi$.
We know that there is $\beta \in \Delta$ and $\sigma \in W$ such that $\alpha = \sigma(\beta)$.
We assume that $\beta$ is in $\Delta_1$.
Of course, any sequence of $\sigma_i$'s sends $\beta$ to a (nonnegative or nonpositive $\InZ$-)linear combination of $\Delta_1$.
On the other hand, since all $\sigma'_i$ fixes $\Delta_1$, it fixes any linear combination of $\Delta_1$.
Since $\sigma$ is generated by $\sigma_i$'s and $\sigma'_i$'s but, as seen, every $\sigma'_i$ in $\sigma$ does not work, we have that $\alpha = \sigma(\beta)$ is a linear combination of only $\Delta_1$.
Similar result holds if $\beta$ is in $\Delta_2$.
Therefore, $\Phi$ is not irreducible.

One more thing can be seen.
In the case that $\Phi = \Phi_1 \cup \Phi_2$ is not irreducible, each $\Phi_i$ is a root system, obviously.
Also, in this case $\Delta_i = \Delta \cap \Phi_1$ is a base of $\Phi_i$.
Therefore, the irreducibility of a root sysyem (or a base) says whether the root system can be separable into two root systems or not.

To understand what this means, for a moment, we return to the theory of semisimple Lie algebras.
Let $\lie{g}$ be a semisimple Lie algebra.
We can find a root system $\Phi \subseteq \lie{h}^*$.
Assume that $\lie{g}$ is not simple.
Then $\lie{g} = \lie{g}_1 \oplus \lie{g}_2$, where $\lie{g}_i$ is all semisimple and $[\lie{g}_1, \lie{g}_2] = 0$.
Let $\lie{h}_i = \lie{h} \cap \lie{g}_i$.
It is easy to see that for each $i$ $\lie{h}$ is a maximal toral subalgebra of $\lie{g}_i$ so that $\Phi_i = \Phi \cap \lie{h}_1^*$ is the root system of $\lie{g}_i$.
But for any $\alpha \in \Phi_1$ and $\beta \in \Phi_2$ $0 = [T_\beta, E_\alpha] = \alpha(T_\beta) E_\alpha$, so $(\alpha, \beta) = 0$.
Thus $\Phi = \Phi_1 \cup \Phi_2$ is not irreducible.

Conversely, assume that $\Phi$ is a disjoint union of relatively orthogonal subsets $\Phi_1$ and $\Phi_2$.
Let $\Delta$ be a base of $\Phi$ and $\Delta_i = \Delta \cap \Phi_i$, and $\lie{h}_i$ the subspace of $\lie{h}$ spanned by all $H_\alpha$ for $\alpha \in \Delta_i$.
Since $\Delta$ is a basis of $\lie{h}^*$, $\lie{h} = \lie{h}_1 \oplus \lie{h}_2$.
Also, of course, $[\lie{h}_1, \lie{h}_2] = 0$.
Let $\alpha \in \Delta_1$ and $\beta \in \Phi_2$.
Then $[T_\alpha, E_\beta] = \beta(T_\alpha) E_\beta = (\beta, \alpha) E_\beta = 0$, so $[\lie{h}_1, E_\beta] = 0$.
In the same way if $\beta \in \Phi_1$, then $[\lie{h}_2, E_\beta] = 0$.
Now let $\alpha_i \in \Phi_i$.
Since $\alpha_1 + \alpha_2$ is a linear combination of not only $\Delta_1$ but also $\Delta_2$, $\alpha_1 + \alpha_2$ is not a root, so $[E_{\alpha_1}, E_{\alpha_2}] = 0$.
Finally, if we let $\lie{g}_i = \lie{h}_i + \sum_{\alpha \in \Phi_i} \lie{g}_\alpha$, $\lie{g} = \lie{g}_1 + \lie{g}_2$ and $[\lie{g}_1, \lie{g}_2] = 0$.
Furthermore, from this all $\lie{g}_i$ is an ideal, so they are semisimple, to which $\Phi_i$ corresponds.

In summary, we have two results.
One is that if the root system of a given semisimple Lie algebra is not irreducible, then an othogonal component (such as $\Phi_1$) corresponds to a (semisimple) ideal and the complementary corresponds to the complementary of the ideal, which is also semisimple.
The other is, every irreducible root system corresponds to a simple Lie algebra.
Therefore, our process for classifying root systems is reduced to the case for irreducible cases.

\newpage

\textbf{Some Properties of Irreducible root system; Maximal root}

In this page we introduce some properties of irreducible root system.
We consider a \textbf{maximal root} relative to the ordering $\succ$.
We write it as $\beta = \sum a_i \alpha_i$, where we denote elements of $\Delta$ by $\alpha_i$.
Suppose that $\Delta_2 = \{\alpha_i \SBar a_i = 0\}$ is not empty, and let $\Delta_1 = \Delta \setminus \Delta_2$.
For $\alpha \in \Delta_2$, since $\beta - \alpha$ is not a root, we have that $(\beta, \alpha) \le 0$.
On the other hand, if $(\beta, \alpha) = 0$ for every $\alpha \in \Delta_2$, since $\beta$ is a positive $\InZ$-linear combination of $\Delta_1$, we obtain that $\Delta_1$ and $\Delta_2$ are orthogonal, which is impossible due to the irreducibility of the given root system.
Thus there is $\alpha \in \Delta_2$ such that $(\beta, \alpha) < 0$ so that $\beta + \alpha$ is a root, a contradiction to the maximality of $\beta$.
Hence $\Delta_2$ is empty, or \textit{a maximal root must be a positive $\InZ$-linear combination of $\Delta$.}
This gives another interesting property.
First, note that for every $\alpha \in \Delta$ $(\beta, \alpha) \ge 0$ due to the maximality of $\beta$ and that there is $\alpha \in \Delta$ such that $(\beta, \alpha) > 0$; otherwise $(\beta, \beta) = 0$, a contradiction.
Now let $\beta'$ be another maximal root and we suppose that $\beta' \ne \beta$.
We shall calculate $(\beta, \beta')$.
It is equal to $\sum a_i (\alpha_i, \beta')$.
Since all $a_i > 0$ and there is $\alpha_i$ such that $(\alpha_i, \beta) > 0$, we obtain that $(\beta, \beta') > 0$ so that $\beta - \beta'$ and $\beta' - \beta$ are roots.
Thus $\beta' \succ \beta$ or $\beta \succ \beta'$, a contradiction to the maximalities of $\beta$ and $\beta'$.
Therefore, \textit{the maximal root is unique.}
It immediately implies that \textit{for every $\alpha \in \Phi$ $\beta \succ \alpha$}.

We investigate another property.
Suppose that there is nonzero proper subspace $E_1$ of $E$ which is leaved invariant by $W$ and $E_2 = E_1^\perp$.
For any $\lambda \in E_1$ and $\mu \in E_2$ and $\sigma \in W$ we obtain that $(\lambda, \sigma(\mu)) = (\sigma^{-1}(\lambda), \mu) = 0$ since $\sigma^{-1}(\lambda) \in E_1$ so that $E_2$ is leaved invariant by $W$.
Let $\alpha \in \Phi$.
One can show that, for any $\sigma_\alpha$-invariant subspace $E'$ of $E$, $\alpha \in E'$ or else $E' \in P_\alpha$ (N.B., for any $\lambda \in E' \setminus P_\alpha$ $\lambda - \left( \lambda - \langle \lambda, \alpha \rangle \alpha \right) \in E'$).
Thus if $\alpha \notin E_i$ for all $i$, then $E_1, E_2 \subseteq P_\alpha$ so that $E \subseteq P_\alpha$, which is absurd.
Hence $\alpha \in E_1$ or else $\alpha \in E_2$.
Let $\Phi_i = \{\alpha \in \Phi \SBar \alpha \in E_i\}$.
Then $\Phi_1$ and $\Phi_2$ are orthogonal and $\Phi$ is the disjoint union of $\Phi_i$'s, a contradiction.
Therefore, we obtain that \textit{$W$ acts irreducibly on $E$}.
This yields immediately that \textit{for every root $\alpha$ $W\alpha$ spans $E$.}

Now we talk about the (squared) 'length' of roots, i.e., $(\alpha, \alpha)$.
We want to get ratio $(\alpha, \alpha) / (\beta, \beta)$ for any non-propositional roots $\alpha$ and $\beta$.
We know that the ratio is one of $1, 2^{\pm 1}, 3^{\pm 1}$, only when $(\alpha, \beta) \ne 0$.
But if $\Phi$ is irreducible, the above result says that there is $\sigma \in W$ such that $(\sigma(\alpha), \beta) \ne 0$.
Thus we can get the ratio for any two roots.
It immediately implies that there are no three roots with distinct length since it yields that a ratio can be 3/2.
Therefore, \textit{at most two distinct length occur in $\Phi$}.
We then call a root \textbf{long} if the length is maximal; otherwise \textbf{short}.
Now let $\alpha$ and $\beta$ be roots with same length.
We can again choose $\beta' \in W\beta$ such they $(\alpha, \beta') \ne 0$.
Assume that $\alpha$ and $\beta'$ are distinct.
Since they have equal length, we have that $\langle \alpha, \beta' \rangle = \langle \alpha, \beta' \rangle = \pm 1$.
In the case for $\langle \alpha, \beta' \rangle = 1$, one can show easily that $(\sigma_\alpha \sigma_{\beta'} \sigma_\alpha)(\beta') = \alpha$.
This says that in the case for $\langle \alpha, \beta' \rangle = -1$ $(\sigma_\alpha \sigma_{\beta'} \sigma_\alpha \sigma_{\beta'})(\beta') = \alpha$.
In any case, we obtain that \textit{any two roots having equal length are $W$-conjugate.}
We return to the maximal root $\beta$.
We shall show that for any root $\alpha$ $(\beta, \beta) \ge (\alpha, \alpha)$, or \textit{the maximal root is long}.
To do this we replace $\alpha$ by a $W$-conjugate which is in $\overline{\mathcal{W}_\Delta}$.
Since $\beta - \alpha$ is the summation of some positive roots, for any $\gamma \in \overline{\mathcal{W}_\Delta}$ $(\gamma, \beta - \alpha) \ge 0$, which implies that $(\beta, \beta) \ge (\beta, \alpha) \ge (\alpha, \alpha)$, which we seek.

\newpage

\textbf{string; reconstruction of root system from base}

In the theory of semisimple Lie algebra we found $\alpha$-string.
We can find an exactly same thing in root systems.
First, let $\alpha, \beta \in \Phi$, which are non-propotional.
We consider $\beta + i\alpha \in \Phi$ for $i \in \InZ$.
Let $r, s \in \NaN_0$ be the maximums such that $\beta - r\alpha, \beta + s\alpha \in \Phi$.
Suppose that there is $-r < i < s$ such that $\beta + i\alpha \notin \Phi$.
Then we can find $-r < p < q < s \in \InZ$ such that $\beta + p\alpha, \beta + (q + 1)\alpha \in \Phi$ but $\beta + (p + 1)\alpha, \beta + q\alpha \notin \Phi$.
It implies that $(\beta + p\alpha, \alpha) \ge 0$ and $(\beta + q\alpha, \alpha) \le 0$.
From this we obtain that $p - q \ge 0$, a contradiction.
Therefore, we obtain that all $\beta - r\alpha, \beta - (r - 1)\alpha, \cdots, \beta + (s - 1)\alpha, \beta + s\alpha$ are in $\Phi$.
We obtain the \textbf{$\alpha$-string through $\beta$}.
Now, it is obvious that $\sigma_\alpha$ preserves any $\alpha$-string, but the form of $\sigma_\alpha$ says that it reverses the order of string.
Thus, we obtain that $\beta + s\alpha = \sigma_\alpha(\beta - r\alpha) = \beta + (r - \langle \beta, \alpha \rangle)\alpha$.
It implies that $r - s = \langle \beta, \alpha \rangle$.
This is exactly same as the case for the roots in semisimple Lie algebras.

We can go further.
Consider $\alpha$-string through $\beta' = \beta - r\alpha$, which must be $\beta, \beta + \alpha, \cdots, \beta + (r + s)\alpha$.
By the above result we have that $0 - (r + s) = \langle \beta', \alpha \rangle$.
But remind that $-\langle \beta', \alpha \rangle < 4$.
Hence, the length of every $\alpha$-string is at most 4.

To start the main dish, we need a lemma.
Let $\beta \in \Phi^+(\Delta)$ but $\beta \notin \Delta$.
Suppose that for every $\alpha \in \Delta$ $(\beta, \alpha) \le 0$.
We can write $\beta = \sum_{\alpha \in \Delta} a_\alpha \alpha$, where all $a_\alpha$ is nonnegative.
Thus $\sum_{\alpha \in \Delta} a_\alpha (\beta, \alpha) \le 0$.
But the left hand side is actually $(\beta, \beta)$ which is positive, a contradiction.
Therefore, there is at least one $\alpha \in \Delta$ such that $\beta - \alpha \in \Phi$.

Now it is time to reconstruct all roots from a base.
Let $\Delta = (\alpha_1, \cdots, \alpha_l)$ be a base and we only know the Cartan matrix.
It doest not harm any generality if we assume that $\Delta$ is irreducible.
We denote, for $\alpha = \sum a_i \alpha_i$, $\textrm{ht }\alpha = \sum a_i$, called the \textbf{height of $\alpha$}.
We use induction on height to find all $\Phi^+(\Delta)$.
We already know all $\sum a_i \alpha_i$ in $\Phi^+(\Delta)$ with height 1.
Now assume that for some $m \in \NaN$ what $\sum a_i \alpha_i$ with height $< m$ is in $\Phi^+(\Delta)$ or not.
For $\alpha \in \Phi^+(\Delta)$ with height $(m - 1)$ and $i$, by seeing the $\alpha_i$-string through $\alpha$, we can determine that $\alpha + \alpha_i$ is a root or not.
In such method we can construct positive roots with height $m$.
Conversely, assume that we have a positive root $\alpha$ with height $m$.
By the above lemma we can find $i$ such that $\alpha - \alpha_i$ is a root, of which the height is $m - 1$.
Thus, $\alpha$ must be constructed by the above method.
Therefore, we can determine, for any nonnegative $\InZ$-linear combination $\alpha$ of $\Delta$ with height $m$, whether $\alpha$ is a root or not.
Note that the determination uses only the information from the Cartan matrix.
By induction, we conclude that with just only the Cartan matrix we can construct whole root space.
Therefore, the Cartan matrix is the essential information of any root system.

We will list the classes of Cartan matrices which makes sense (we will discuss what this means).
Unfortunately, we cannot definitely say that the set of $E$ from one of the Cartan matrices, which we construct from the base, is a root system.
Even we do not know that the set is finite.
But here is a good news and bad news.
The good news is that we can see that the constructed set by a Cartan matrix of the list we will get is a root system.
The bad news is that only the way to see this is constructing the set for each possible Cartan matrix by hand.
We will see the proof for the existence, but I hope that one day we can find an elegant way for this.

\newpage

\textbf{Dual system}

We denote $\alpha^\vee = \frac{2}{(\alpha, \alpha)} \alpha$ for each $\alpha \in \Phi$ and for $M \subseteq \Phi$ $M^\vee = \{ \alpha^\vee \SBar \alpha \in M \}$.
We call $\Phi^\vee$ the \textbf{dual} of $\Phi$.
One can show that \textit{$\Phi^\vee$ is a root system}.
The finiteness, multiplication conditions are immediate, and the condition $\frac{2(\beta^\vee, \alpha^\vee)}{(\alpha^\vee, \alpha^\vee)} \in \InZ$ is easy from the definition.
The condition for $\beta^\vee - \frac{2(\beta^\vee, \alpha^\vee)}{(\alpha^\vee, \alpha^\vee)} \alpha^\vee \in \Phi^\vee$ seems not easy to show, but remind that $(\beta^\vee, \beta^\vee) = (\beta^\vee - \frac{2(\beta^\vee, \alpha^\vee)}{(\alpha^\vee, \alpha^\vee)} \alpha^\vee, \beta^\vee - \frac{2(\beta^\vee, \alpha^\vee)}{(\alpha^\vee, \alpha^\vee)} \alpha^\vee)$, and from this we can show that the condition holds.

One of useful property of dual is that \textit{for any base $\Delta$ of $\Phi$ $\Delta^\vee$ is a base of $\Phi^\vee$}.
It is sufficient to consider only the case for irreducible $\Phi$.
To show this, first, it is immediate that $\gamma \in E$ is regular relative to $\Phi$ if and only if it is regular relative to $\Phi^\vee$ and that for $\gamma \in \mathcal{W}_\Delta$ (thus $\Phi^+(\Delta) = \Phi^+(\gamma)$) $(\Phi^+(\gamma))^\vee = (\Phi^\vee)^+(\gamma)$, so that $\Delta^\vee \subseteq (\Phi^\vee)^+(\gamma)$.
We denote the base of $(\Phi^\vee)^+(\gamma)$ by $\Delta^\vee(\gamma)$.
Now we suppose that there is $\alpha^\vee \in \Delta^\vee$ such that $\alpha^\vee$ is decomposable in $(\Phi^\vee)^+(\gamma)$.
That is, there are $\beta_1^\vee, \beta_2^\vee \in (\Phi^\vee)^+(\gamma)$ such that $\alpha^\vee = \beta_1^\vee + \beta_2^\vee$.
It can be written as $\frac{1}{(\alpha, \alpha)} \alpha = \frac{1}{(\beta_1, \beta_1)} \beta_1 + \frac{1}{(\beta_2, \beta_2)} \beta_2$, where $\beta_1, \beta_2 \in \Phi^+(\gamma)$.
Since $\Phi$ is irreducible, as seen, $\frac{(\alpha, \alpha)}{(\beta_1, \beta_1)}$ and $\frac{(\alpha, \alpha)}{(\beta_2, \beta_2)}$ are well-defined and rational, so we can say that there are positive integers $a, b, c$ such that $\alpha = (b/a)\beta_1 + (c/a)\beta_2$.
One can write this as a linear combination of $\Delta$, which is the unique way because of (B1), but this obviously yields a contradiction to (B2).
Therefore, $\Delta^\vee \subseteq \Delta^\vee(\gamma)$, so that we have the lemma, i.e., $\Delta^\vee$ is a base of $\Phi^\vee$.

\newpage

\textbf{Abstract base}

We have seen that when the Cartan matrix $(\langle \alpha_i, \alpha_j \rangle)$ is given, we can construct whole root system.
The remaining problem is: what Cartan matrix does not give a base?
(Note that our work is for classification, not for existence, yet.)
This question can be changed.
We know some important properties of bases, then what is the possible forms of Cartan matrix which satisfy the properties?

We list the properties for $\Delta = (e_1, e_2, \cdots, e_l)$ (instead denoting $\alpha_i$) in $E$:

(C1) $\Delta$ is a basis of $E$.

(C2) There is no subset of $\Delta$ which is orthogonal to the complementary

(C3) For every $i \ne j$ $(e_i, e_j) \le 0$

(C4) For every $i \ne j$ $\langle e_i, e_j \rangle \langle e_j, e_i \rangle$ is one of 0, 1, 2, 3.

Our aim is then to find the forms of $(\langle e_i, e_j \rangle)$ which makes the conditions consistent.
Only these conditions, however, are not convenient.
Instead, we add a condition:

(C5) For every $i$ $(e_i, e_i) = 1$.

Of course, for given $(\langle e_i, e_j \rangle)$ if we can find such $\Delta$ satisfying (C1)$\sim$(C4), then we can find $\Delta'$ satisfying (C1)$\sim$(C5) over some sufficiently extended field, just normalizing all elements in $\Delta$.
Conversely, if we cannot find $\Delta'$ satisfying (C1)$\sim$(C5) over $\ReR$, then then we cannot find $\Delta$ satisfying (C1)$\sim$(C4) over any (smaller) ordered field, even $\RaQ$.
Hence, our playground will be, for a while, over $\ReR$.

\newpage

\textbf{Dynkin diagram}

We mentioned that we shall investigate $(e_i)$ satisfying all (C1)$\sim$(C5).
Before doing this we need to introduce a tool to visualize the configuration of Cartan matrix $(\langle \alpha_i, \alpha_j \rangle)$ of a base $(\alpha_i)$.
It is simple: Match a vertex (on a plane or paper) to each $\alpha_i$, which is called the $i$-th vertex, and draw lines connecting each $i$-th vertex and $j$-th vertex, where the number of line drawn is equal to $\langle \alpha_i, \alpha_j \rangle \langle \alpha_j, \alpha_i \rangle$, equal to one of 0, 1, 2, 3, and add an arrow on a multiple line, pointing the shorter. For example, 

\begin{displaymath}
  \left( \begin{array}{rrr}
     2 & -1 &  0 \\
    -1 &  2 & -1 \\
     0 & -1 &  2
  \end{array} \right)
  \;\; : \;\; 
  \xymatrix@1{\circ \ar@{-}[r] & \circ \ar@{-}[r] & \circ}
\end{displaymath}

\begin{displaymath}
  \begin{array}{cc}
    \left( \begin{array}{rrrr}
       2 & -1 &  0 &  0 \\
      -1 &  2 & -1 & -1 \\
       0 & -1 &  2 &  0 \\
       0 & -1 &  0 &  2
    \end{array} \right)
    \;\; : &  
    \xymatrix@1{ & & \circ \\ \circ \ar@{-}[r] & \circ \ar@{-}[ur] \ar@{-}[dr] \\ & & \circ}
  \end{array}
\end{displaymath}

\begin{displaymath}
  \left( \begin{array}{rrrr}
     2 & -1 &  0 &  0 \\
    -1 &  2 & -2 &  0 \\
     0 & -1 &  2 & -1 \\
     0 &  0 & -1 &  2
  \end{array} \right)
  \;\; : \;\; 
  \xymatrix@1{\circ \ar@{-}[r] & \circ \ar@2{->}[r] & \circ \ar@{-}[r] & \circ}
\end{displaymath}

Such diagrams are called the \textbf{Dynkin diagrams}.
Remember that the second vertex of the second diagram and the second and third vertices of the third diagram are attatched to 3 lines.
When we remove all information of the length of each $e_i$, or applying (C5), every arrow in Dynkin diagrams are irrelevant, so it will be removed.
We call such diagram the \textbf{Coxeter diagram}.

\newpage

\textbf{Classification theorem (1)}

All ingredients are ready.
It is time to prove the classification theorem.
It will be archived by finding all possible cases of Coxeter diagrams, thus Dynkin diagram.
The proof goes in a few steps.
(Note: (1) and (4) are the cores.)

(1) \textit{If one removes some vertices (and lines attached to them) in an admissible diagram, then the remaining diagram is admissible.}

pf) Clearly satisfying all (C1)$\sim$(C5) (remember that the dimension of $E$ must be reduced sufficiently).

(2) \textit{The number of edges of any admissible diagram is at most $l - 1$.}

pf) Let $e = \sum e_i$.
Then $(e, e) = \sum (e_i, e_j) = \sum (e_i, e_i) + \sum_{i < j} 2(e_i, e_j) = l + \sum_{i < j} 2(e_i, e_j)$.
It is immediate that $\sum_{i < j} 2(e_i, e_j) \le -\textrm{(the number of all edges)}$.
Since $(e, e)$ must be positive, the number of edges must be less than $l$.

(3) \textit{Any admissible diagram cannot contain a cycle.}

pf) Any loop has $l$-edges.
Thus (3) and (2) implies the lemma.

(4) \textit{In any admissible diagram any vertex is connected by at most 3 lines.}

pf) Let $d$ be one of $e_i$'s and $d_1, d_2, \cdots, d_m$ all of $e_i$'s connected with $d$.
But any of $d_i$'s are connected each other, otherwise there is a cycle, a contradiction to (3).
Thus we have that $(d_i, d_j) = 0$ if $i \ne j$, so the set of $d_i$'s is an orthonormal set.
Now, since $d, d_1, \cdots, d_m$ is linearly independent, we can make $d_0$, which is a linear combination of $d, d_1, \cdots, d_m$ such that the set of $d_0, d_1, \cdots, d_m$ is an orthonormal set.
We can, then, denote $d = \sum_{i = 0}^m (d, d_i) d_i$.
From this, $1 = (d, d) = \left( \sum_{i = 0}^m (d, d_i) d_i, \sum_{j = 0}^m (d, d_j) d_j \right) = \sum_{i = 0}^m (d, d_i)^2$.
Finally, we have that $\sum_{i = 1}^m 4(d, d_i)^2 < 4$, but the left hand side is actually the total number of lines attached to $d$.
Therefore, our statement holds.

(5) \textit{In any admissible diagram if one reduces a simple subline (a subdiagram forming a straight line, in which every edge is with only one line and in which every vertex, except for the first and the last, does not connected to other vertex out of the simple line) and attachs the first vertex and the last vertex into an one vertex, then the reduced diagram is admissible.}

pf) Reordering the indices, let the base be $(s_1, s_2, \cdots, s_m, e_1, \cdots, e_n)$, where $s_1, \cdots, s_m$ forms a simple line.
Actually, we can give an ordering to $s_1, \cdots, s_m$ satisfying that $(s_i, s_j)$ is 1 if $i = j$ or -1 if $i = j \pm 1$ or 0 otherwise and $(s_i, e_j) = 0$ for all $i$ and $j$ except for $i = 1\textrm{ or }m$.
Now we denote $s = \sum s_i$, and we remove all $s_i$'s but attach $s$.
In other words, we consider $(s, e_1, e_2, \cdots, e_n)$.
From the fact that $(s, s) = \sum (s_i, s_j) = \sum (s_i, s_i) + \sum 2(s_i, s_{i + 1}) = m - (m - 1) = 1$ and that $(s, e_i)$ is only one of $(s_1, e_i)$ and $(s_m, e_i)$ and 0 (using (3)), it is easily seen that all (C1)$\sim$(C5) hold.
Therefore, the reduced diagram is admissible.

(6) \textit{Any admissible diagram cannot contain two or more vertices with 3 lines, except for the case that there are two such vertices and they are connected by 2 or 3 lines.}

pf) If there are three or more vertices with 3 lines or two such vertices but not connected by only one edge with 2 or 3 lines.
Then there must be two such vertices connected by a simple subline.
It can be reduced, and after reducing the 2 remaining lines of the first vertex and that of the last vertex are attached to the new vertex.
Thus there is a vertex with 4 lines, a contradiction to (4).

\newpage

\textbf{Classification theorem (2)}

We can summary our results.
Note that the number of vertices attached to 3 lines is at most 2, where the case for 2 is highly restricted.
Here is the list of admissible Coxeter diagrams satisfying the results.

The number of vertices with 3 lines = 0 : 
\begin{displaymath}
  \xymatrix{\circ \ar@{-}[r] & \circ \ar@{-}[r] & \cdots \ar@{-}[r] & \circ \ar@{-}[r] & \circ}
\end{displaymath}

The number of vertices with 3 lines = 1 : 
\begin{displaymath}
  \xymatrix{\circ \ar@{-}[r] & \cdots \ar@{-}[r] & \circ \ar@{-}[r] & \circ \ar@2{-}[r] & \circ}
\end{displaymath}
\begin{displaymath}
  \xymatrix{
    & & & & & \overset{y_q}{\circ} \ar@{-}[r] & \cdots & \overset{y_2}{\circ} \ar@{-}[r] & \overset{y_1}{\circ} \\
    \overset{x_1}{\circ} \ar@{-}[r] & \overset{x_2}{\circ} \ar@{-}[r] & \cdots \ar@{-}[r] & \overset{x_p}{\circ} \ar@{-}[r] & \overset{w}{\circ} \ar@{-}[ru] \ar@{-}[rd] \\
   & & & & & \overset{z_r}{\circ} \ar@{-}[r] & \cdots & \overset{z_2}{\circ} \ar@{-}[r] & \overset{z_1}{\circ}}
\end{displaymath}

The number of vertices with 3 lines = 2 : 
\begin{displaymath}
  \xymatrix{\overset{g_1}{\circ} \ar@{-}[r] & \overset{g_2}{\circ} \ar@{-}[r] & \cdots \ar@{-}[r] & \overset{g_m}{\circ} \ar@2{-}[r] & \overset{h_n}{\circ} \ar@{-}[r] & \cdots \ar@{-}[r] & \overset{h_2}{\circ} \ar@{-}[r] & \overset{h_1}{\circ} }
\end{displaymath}
\begin{displaymath}
  \xymatrix{\circ \ar@3{-}[r] & \circ}
\end{displaymath}

There is no more restriction on the first two classes and the last diagram.
(We will see that all of these diagrams corresponds to a simple Lie algebras.)
But the remaining diagrams are not complete.
There is restrictions on $m, n, p, q, r$.
We will investigate it.

We discuss it briefly, but filling the detail is so easy.
The key is to let $g = \sum_{i = 1}^m ig_i$, $h = \sum_{i = 1}^n ih_i$, $x = \sum_{i = 1}^p ix_i$, $y = \sum_{i = 1}^q iy_i$, $z = \sum_{i = 1}^r iz_i$.
To get the restriction on $m$ and $n$, it is sufficient to the Cauchy-Schwartz inequality $(g, h)^2 < (g, g) (h, h)$, which gives $m = n = 2$ (the case for $m = 1$ or $n = 1$ is also given, but it is in the second class).
To get the restriction on $p, q, r$, let $w'$ be a linear combination of $w, x, y, z$ so that $w' x, y, z$ are relatively orthonormal.
Then writting $w = \frac{(w, w')}{(w', w')}w' + \frac{(w, x)}{(x, x)}x + \frac{(w, y)}{(y, y)}y + \frac{(w, z)}{(z, z)}z$ and calculating $(w, w)$, we have that $\frac{(w, x)^2}{(x, x)} + \frac{(w, y)^2}{(y, y)} + \frac{(w, z)^2}{(z, z)} < 1$.
This gives 4 cases: $q = r = 1$ and $p$ arbitrary, $q = 1$ and $r = 2$ and $p = 2, 3, 4$.
Therefore, we finally have all admissible Coxeter diagrams.

The remaining is to get all possible Dynkin diagram.
By symmetry, giving a direction on the edge with multiple line of the last two Coxeter diagrams is not irrelevant.
But the second diagram is not.
The way to give the direction makes a difference.
Hence, we have another class.
Finally, we have all possible Dynkin diagram.

\newpage

\textbf{Classification of Root systems; the list}

$A_l$ $(l \ge 1)$ : 
\begin{displaymath}
  \xymatrix{\overset{\alpha_1}{\circ} \ar@{-}[r] & \overset{\alpha_2}{\circ} \ar@{-}[r] & \cdots \ar@{-}[r] & \overset{\alpha_{l - 1}}{\circ} \ar@{-}[r] & \overset{\alpha_l}{\circ}}
\end{displaymath}

$B_l$ $(l \ge 2)$ : 
\begin{displaymath}
  \xymatrix{\overset{\alpha_1}{\circ} \ar@{-}[r] & \overset{\alpha_2}{\circ} \ar@{-}[r] & \cdots \ar@{-}[r] & \overset{\alpha_{l - 2}}{\circ} \ar@{-}[r] & \overset{\alpha_{l - 1}}{\circ} \ar@2{->}[r] & \overset{\alpha_l}{\circ}}
\end{displaymath}

$C_l$ $(l \ge 3)$ : 
\begin{displaymath}
  \xymatrix{\overset{\alpha_1}{\circ} \ar@{-}[r] & \overset{\alpha_2}{\circ} \ar@{-}[r] & \cdots \ar@{-}[r] & \overset{\alpha_{l - 2}}{\circ} \ar@{-}[r] & \overset{\alpha_{l - 1}}{\circ} \ar@2{<-}[r] & \overset{\alpha_l}{\circ}}
\end{displaymath}

$D_l$ $(l \ge 4)$ : 
\begin{displaymath}
  \xymatrix{
    & & & & & \overset{\alpha_{l - 1}}{\circ} \\
    \overset{\alpha_1}{\circ} \ar@{-}[r] & \overset{\alpha_2}{\circ} \ar@{-}[r] & \cdots \ar@{-}[r] & \overset{\alpha_{l - 3}}{\circ} \ar@{-}[r] & \overset{\alpha_{l - 2}}{\circ} \ar@{-}[ru] \ar@{-}[rd] \\
   & & & & & \overset{\alpha_l}{\circ}}
\end{displaymath}

$E_6$ : 
\begin{displaymath}
  \xymatrix{
    & & \overset{\alpha_2}{\circ} \\
    \overset{\alpha_1}{\circ} \ar@{-}[r] & \overset{\alpha_3}{\circ} \ar@{-}[r] & \overset{\alpha_4}{\circ} \ar@{-}[r] \ar@{-}[u] & \overset{\alpha_5}{\circ} \ar@{-}[r] & \overset{\alpha_6}{\circ}}
\end{displaymath}

$E_7$ : 
\begin{displaymath}
  \xymatrix{
    & & \overset{\alpha_2}{\circ} \\
    \overset{\alpha_1}{\circ} \ar@{-}[r] & \overset{\alpha_3}{\circ} \ar@{-}[r] & \overset{\alpha_4}{\circ} \ar@{-}[r] \ar@{-}[u] & \overset{\alpha_5}{\circ} \ar@{-}[r] & \overset{\alpha_6}{\circ} \ar@{-}[r] & \overset{\alpha_7}{\circ}}
\end{displaymath}

$E_8$ : 
\begin{displaymath}
  \xymatrix{
    & & \overset{\alpha_2}{\circ} \\
    \overset{\alpha_1}{\circ} \ar@{-}[r] & \overset{\alpha_3}{\circ} \ar@{-}[r] & \overset{\alpha_4}{\circ} \ar@{-}[r] \ar@{-}[u] & \overset{\alpha_5}{\circ} \ar@{-}[r] & \overset{\alpha_6}{\circ} \ar@{-}[r] & \overset{\alpha_7}{\circ} \ar@{-}[r] & \overset{\alpha_8}{\circ}}
\end{displaymath}

$F_4$ : 
\begin{displaymath}
  \xymatrix{\overset{\alpha_1}{\circ} \ar@{-}[r] & \overset{\alpha_2}{\circ} \ar@2{->}[r] & \overset{\alpha_3}{\circ} \ar@{-}[r] & \overset{\alpha_4}{\circ} }
\end{displaymath}

$G_2$ : 
\begin{displaymath}
  \xymatrix{\overset{\alpha_1}{\circ} \ar@3{<-}[r] & \overset{\alpha_2}{\circ}}
\end{displaymath}

\newpage

\textbf{Construction of root systems of type A, B, C, D, G}

We have done the classification of possible root systems (and thus finite-dimensional semisimple Lie algebras), but this does not say the existence of a root system in a type of one of our list.
Unfortunately, we do not have package proof of the existence.
What we can do for the proof is attacking each of the types one by one.
There is, however, a good news; we have a useful tool.

We consider $\ReR^m$ ($m \ge l$) and a subgroup $I = \{\sum a_i e_i \SBar a_i \in \InZ\} + \InZ e'$, where $(e_1, \cdots, e_m)$ is the standard (orthonormal) basis of $\ReR^m$ and $e' \in \ReR^m$ (it may be 0).
Also we let $J$ be a certain subgroup of $I$ where $(\alpha, \beta) \in \InZ$ for any $\alpha, \beta \in J$.
We let $\Phi$ be a subset of vectors in $J$ having one certain (squared) length or two lengths.
It is immediate that $Phi$ is finite so that $\Phi$ satisfies (RS1).
Also due to the ratio of the lengths of long and short and the lattice-structure of $J$, (RS2) is immediate.
Now (RS3) and (RS4) seems hard.
But the lattice-structure and group property of $J$ and the length-preservation of $\sigma_\alpha$ say that if (RS3) holds then (RS4) holds automatically.
For example, if we choose the squared lengths to be 1 or 2 and $(\beta, \alpha) \in \InZ$, since $\langle \beta, \alpha \rangle = 2(\beta, \alpha)/(\alpha, \alpha)$, (RS3) holds.
Therefore, what we have to do for $\Phi$ is to find a subgroup $J$ in a certain $\ReR^m$ and to make (RS3) hold and to matching the Cartan matrix (to do this we need to find a base of $\Phi$; it is the hardest process during our task).
From now on, with this tool we construct root systems.

$A_l$ : Let $m = l + 1$ and $e' = 0$ and $E = P_{\sum e_i}$ and $J = I \cap E$, and $\Phi = \{\alpha \in J \SBar (\alpha, \alpha) = 2\}$, which is clearly a root system by the above example.
One can see that $\Phi$ consists of vectors in the form of $e_i - e_j$ ($i \ne j$), so $|\Phi| = l(l + 1)$.
Let $\alpha_i = e_i - e_{i + 1}$.
Then clearly $(\alpha_i)$ is a basis of $E$ and $e_i - e_j = e_i + e_{i + 1} + \cdots + e_j$ if $i < j$ so that $(\alpha_i)$ is a base.
It is immediate that $\langle \alpha_i, \alpha_j \rangle$ ($i \ne j$) is -1 if $j - i = \pm 1$, 0 otherwise, thus the Cartan matrix is of type $A_l$.

$B_l$ : Let $m = l$ and $e' = 0$ and $E = \ReR^l$ and $J = I$, and $\Phi = \{\alpha \in J \SBar (\alpha, \alpha) = \textrm{1 or 2}\}$, which is again a root system.
One can easily see that $\Phi$ consists of all $\pm e_i$ (length 1) and $\pm(e_i \pm e_j)$ (length 2), so $|\Phi| = 2l^2$, and that $\{e_1 - e_2, e_2 - e_3, \cdots, e_{l - 1} - e_l, e_l\}$ is a base, and that the Cartan matrix calculated from this base is of type $B_l$.

$C_l$ : Instead of our tool, it is better to use that $C_l$ is the dual of $B_l$; it is immediate that the Cartan matrix calculated from the dual base of the above base of type $B_l$ is of type $C_l$.

$D_l$ : Let $m = l$ and $e' = 0$ and $E = \ReR^l$ and $J = I$, and $\Phi = \{\alpha \in J \SBar (\alpha, \alpha) = 2\}$, which is again a root system with $|\Phi| = 2l(l - 1)$.
It is also immediate that $\Phi = \{\pm(e_i \pm e_j) \SBar i \ne j\}$ and that $\{e_1 - e_2, e_2 - e_3, \cdots, e_{l - 1} - e_l, e_{l - 1} + e_l\}$ is a base, where the Cartan matrix is of type $D_l$.

$G_2$ : Let $m = 2 + 1$ and $e' = 0$ and $E = P_{e_1 + e_2 + e_3}$ and $J = I \cap E$, and $\Phi = \{\alpha \in J \SBar (\alpha, \alpha) = \textrm{2 or 6}\}$.
Then it is immediate that $\Phi = \pm\{e_1 - e_2, e_2 - e_3, e_1 - e_3, 2e_1 - e_2 - e_3, 2e_2 - e_1 - e_3, -2e_3 + e_1 + e_2\}$ (hence $|\Phi| = 12$).
Note that since we choose vectors with squared length 6, we do not have (RS3) yet as for $A_l$.
However, if $\beta \in \Phi$ has squared length 6, one can show that for any $\alpha \in \Phi$ $(\alpha, \beta)$ is 0 or $\pm 3$, which implies (RS3) so that $\Phi$ is a root system,and  that $\{e_1 - e_2, 2e_2 - e_1 - e_3\}$ is a base of $\Phi$, where its Cartan matrix is of type $G_2$.

\newpage

\textbf{Construction of root systems of type E, F}

In the previous page we have found root systems of type A-D and G.
The remaining is type E and F.
But in the type E the construction is more difficult than the preceed types.

$F_4$ : Let $m = 4$ and $E = \ReR^4$.
Now we let $e' = (1/2)(e_1 + e_2 + e_3 + e_4)$ and $J = I$, and let $\Phi = \{\alpha \in J \SBar (\alpha, \alpha) = \textrm{1 or 2}\}$.
Then $\Phi$ is the set of $\pm e_i$, $\pm(e_i \pm e_j)$ and $\pm(1/2)(e_1 \pm e_2 \pm e_3 \pm e_4)$.
We have an obsure; if $\alpha = e_i$ and $\beta = \pm(1/2)(e_1 \pm e_2 \pm e_3 \pm e_4)$, $(\alpha, \beta) = 1/2$, not an integer.
Fortunately, first, no any other case yields a non-integral scalar product, and since $(\alpha, \alpha) = (\beta, \beta) = 1$ $\langle \alpha, \beta \rangle = \langle \beta, \alpha \rangle = 1$, so (RS3) holds.
Thus $\Phi$ is a root system, and $|\Phi| = 48$.
It is almost immediate that $\{(1/2)(e_1 - e_2 - e_3 - e_4), e_2 - e_3, e_3 - e_4, e_4\}$ is a base of which the Cartan matrix is of type $F_4$.

Before treating type $E$, we see that if we have a root system corresponding to an admissible Dynkin diagram, for any subdiagram which is obtained by removing one vertex there is a root system corresponding to the subdiagram.
(It is then obvious that this lemma holds for any subdiagram.)
It is almost obvious; let $\Phi$ be a root system in an Euclidean space $E$ and $\Delta = \{\alpha_i\}$ a base of $\Phi$.
Now let $\Delta'$ be the set of all $\alpha_i$ except for the one corresponding to the removed vertex and $E'$ be the subspace spanned by $\Delta'$ and $\Phi'$ be the set of elements in $\Phi$ which is a (nonnegative or nonpositive $\InZ$-)linear combination of only $\Delta'$.
Then it is obvious that $\Phi'$ satisfies (RS1)$\sim$(RS4) and $\Delta' \subset \Phi'$ satisfies (B1) and (B2) so that $\Phi'$ is a root system in $E'$ with a base $\Delta'$ and that the Cartan matrix of $\Delta'$ coincides with the Cartan matrix corresponding to the subdiagram.
Therefore, we obtain a new root system.
This theorem helps to construct root systems of type $E$.

$E_6, E_7, E_8$ : Note that the diagram of $E_6$ and $E_7$ is subdiagrams of $E_8$.
Due to the above theorem it is sufficient to construct only $E_8$.
As for $F_4$, we let $e' = (1/2) \sum_{i = 1}^8 e_i$.
In this time, however, we choose $J \in I$ in a somewhat different way.
Let $J$ be the set of $\sum a_i e_i + (a/2) \sum e_i$ such that $a + \sum a_i$ is an even integer (we call this the a \textbf{$E_8$-lattice}).
Then $J$ is a subgroup; to show this it is sufficient to show that $J$ is closed under the additional, which is easy to see.
Now we let $\Phi = \{\alpha \in J \SBar (\alpha, \alpha) = 2\}$.
Then $\Phi$ consists of $\pm(e_i \pm e_j)$ and $(1/2) \sum_{i = 1}^8 (-1)^{r(i)} e_i$, where $r(i) = \textrm{0 or 1}$.
Fortunately, we can easily see that $(\alpha, \beta) \in \InZ$ for all $\alpha, \beta \in \Phi$.
Thus, as we mentioned, $\Phi$ is a root system, where $|\Phi| = 240$.
We can choose a base of $\Phi$ as $\{(1/2)(e_1 + e_8 - (e_2 + \cdots + e_7), e_1 + e_2, e_2 - e_1, e_3 - e_2, e_4 - e_3, e_5 - e_4, e_6 - e_5, e_7 - e_6\}$ and it gives the Cartan matrix of type $E_8$, as we seek.
(In additional, in the case for the type $E_6$ and $E_7$ one can construct all positive roots so that it is obtained that $|\Phi| = \textrm{72, 126}$, respectively.)

Finally, we have all (irreducible) root systems.
Based on this result we will see that the corresponding finite-dimensional (semi)simple Lie algebras exist.

\newpage

\part{Universal enveloping algebras}

\newpage

\textbf{Universal enveloping algebra}

To go further, we need a more tool, which are more abstract.
What we first need is free objects.
There is a useful free object: Universal enveloping algebra.

Let $\lie{g}$ be a Lie algebra and $\mathcal{A}$ be an algebra (over the same field).
We can give a Lie algebra structure on $\mathcal{A}$, $\mathcal{A}_L$, which is generated by all $XY - YX$ for $X, Y \in \mathcal{A}$.
We are interested in the case for that there is a homomorphism $f : \lie{g} \to \mathcal{A}_L$.
The free object $U(\lie{g})$ of $\lie{g}$ (hence have a canonical embedding $\iota$ of $\lie{g}$ into $U(\lie{g})$) we shall meet is then such that there is a unique homomorphism $\phi : U(\lie{g}) \to \mathcal{A}$ such that $\phi \circ \iota = f$.
We call this object $U(\lie{g})$ the \textbf{universal enveloping algebra of $\lie{g}$}.

The uniqueness (up to (unique) isomorphism) is easy to show.
The problem is the existence.
It is needed to use the tensor algebra $T(\lie{g}) = \bigoplus_{i = 0}^\infty T^i(\lie{g})$.
We consider the two sided ideal $\mathcal{I}$ which is generated by all $X \otimes Y - Y \otimes X - [X, Y]$ for $X, Y \in \lie{g}$.
Let $U(\lie{g}) = T(\lie{g}) / \mathcal{I}$.
Note that $T(\lie{g})$ is the free algebra so that for any 'vector space' $\lie{g}$ with a mapping $f$ into an algebra $\mathcal{A}$ there is a homomorphism $\phi_0 : T(\lie{g}) \to \mathcal{A}$ such that $\phi_0 \circ \iota = f$.
We then consider the induced map $\phi : T(\lie{g})/\mathcal{I} \to \mathcal{A}$ from $\phi_0$.
Obviously, this is the homomorphism which we seek and it is unique.
Therefore, $U(\lie{g})$ is the universal enveloping algebra.

Not much about universal enveloping algebra is needed in this text.
But the following is important in any case: Poincar\'e-Birkhoff-Witt theorem (abbreviated as PBW theorem).
There is a general form of this, but what we need is the following.
\textit{Let $\lie{g}$ be a Lie algebra with a basis $U_i$.
Then the set of $U_{i_1} U_{i_2} \cdots U_{i_r}$ ($i_1 \le i_2 \le \cdots \le i_r$) is a basis of $U(\lie{g})$.}
(Such element can be written as $U_1^{r_1} U_2^{r_2} \cdots U_n^{r_n}$. This form is more useful in later.)
Here is a brief of the proof, but filling the detail is easy.
For a convenience, we let $S_r = \{U_{i_1} U_{i_2} \cdots U_{i_m} \SBar i_1 \le i_2 \le \cdots \le i_m, m \le r\}$.
Obviously, the set of $U_{i_1} \cdots U_{i_r}$, where $i_j$'s are not necessarily ordered, is a basis of $U(\lie{g})$.
We assume that for $i_1 \le \cdots \le i_r$ $U_{i_1} \cdots U_{i_r} U_j$ always can be written as a linear combination of $S_{r + 1}$.
Then by easy induction every $U_{i_1} \cdots U_{i_r}$, unordered $i_j$, can be written as a linear combination of $S_r$.
Since such elements form a basis of $U(\lie{g})$, this finishes our proof.
Hence, we need to show that, for $i_1 \le \cdots \le i_r$ and any $j$, $U_{i_1} \cdots U_{i_r} U_j$ can be written as a linear combination of $S_{r + 1}$.
But, if $i_r > j$, $U_{i_1} \cdots U_{i_r} U_j = U_{i_1} \cdots U_{i_{r - 1}} U_j U_{i_r} + U_{i_1} \cdots U_{i_{r - 1}} [U_{i_r}, U_j]$.
If we assume that, when $m < r$, for $i_1 \le \cdots \le i_m$ and any $j$, $U_{i_1} \cdots U_{i_m} U_m$ can be written as a linear combination of $S_{m + 1}$, then it is easy to see that the formula is a linear combination of $S_{r + 1}$.
Hence, by induction on $r$, we complete the proof.

\newpage

\textbf{Free Lie algebra}.

There is a powerful (but too abstract) ingredient: the free Lie algebra.
Let $\mathcal{X}$ be a set and assume that we have a mapping $f : \mathcal{X} \to \lie{g}$ into a Lie algebra.
The \textbf{free Lie algebra $\mathcal{F}_L(\mathcal{X})$ generated by $\mathcal{X}$}, with a canonical embedding $\iota : \mathcal{X} \to \mathcal{F}_L(\mathcal{X})$, is that for any such $f$ there is a unique Lie algebra homomorphism $\phi : \mathcal{F}_L(\mathcal{X}) \to \lie{g}$ such that $\phi \circ \iota = f$.
Such free object exists.
Let $V_\mathcal{X}$ be a vector space which is (formally) generated by $\mathcal{X}$.
We consider then the tensor algebra $T(V_\mathcal{X})$.
Let $\iota : \mathcal{X} \to T(V_\mathcal{X})$ be the canonical mapping.
We can identify the induced mapping $\mathcal{X} \mapsto f(\mathcal{X}) \in U(\lie{g})$ by $f$.
Since $T(V_\mathcal{X})$ is the free algebra, there is a (unique) homomorphism $\phi_1 : T(V_\mathcal{X}) \to U(\lie{g})$.
Now we let $\mathcal{F}_L(\mathcal{X})$ be the subalgebra of $(T(V_\mathcal{X}))_L$ generated by $\mathcal{X}$, and let $\phi = \phi_1|_{\mathcal{F}_L(\mathcal{X})}$.
Then the image of $\phi$ is in $\lie{g}$.
Of course, $\phi \circ \iota = f$.
Thus, we obtain a homomorphism $\phi : \mathcal{F}_L(\mathcal{X}) \to \lie{g} (\subseteq U(\lie{g}))$.
It is easy to see that this homomorphism is unique.
Therefore, $\mathcal{F}_L(\mathcal{X})$ is the free Lie algebra.
The uniqueness (up to isomorphism) is then obvious.
Moreover, it is immediate that $U(\mathcal{F}_L(\mathcal{X})) \cong T(V_\mathcal{X})$.

\newpage

\textbf{Existence theorem (1)}

Now we return to the theory of semisimple Lie algebras.
We had completed all possible class of (semi)simple Lie algebra and the existence of corresponding root systems.
But it is just for classification, not for existence.
That is, what we have done is showing that 'if it exists, then it must be one of them', but not 'it exists'.
Therefore, our next task is that, if we have a (definitely existing) root system, there exists a (semi)simple Lie algebra corresponding to the system, or a given Cartan matrix.

Let $\mathcal{X} = \{E_i, F_i, H_i \SBar i = 1, 2, \cdots, l\}$.
In our purpose, we shall find an ideal $\mathcal{I}$ of $\mathcal{F}_L(\mathcal{X})$ such that $\mathcal{F}_L(\mathcal{X})/\mathcal{I}$ is a semisimple Lie algebra.
We let $c_{ij} = \langle \alpha_i, \alpha_j \rangle$, the given Cartan matrix.
With this configuration what we can give to $\mathcal{I}$, or the 'relation', is, at least, 

(1) $[H_i, H_j]$ \;\; (2) $[E_i, F_j] - \delta_{ij} H_j$ \;\; (3) $[H_i, E_j] - c_{ji} E_j, \; [H_i, F_j] + c_{ji} F_j$

(4) $(\ad{E_i})^{-c_{ji} + 1}E_j, \;\; (\ad{F_i})^{-c_{ji} + 1}F_j$ ($i \ne j$)

It means that $\mathcal{I}$ is generated by, at least, the elemets in (1)$\sim$(4).
But, fortunately, we shall see that that is enough.
We will see that with this relation $\mathcal{F}_L(E_i, F_i, H_i \; | \; i = 1, 2, \cdots, l)/\mathcal{I}$ is semisimple.

We now restore the roots in $\lie{h}^*$, where $\lie{h} = \sum kH_i$.
We define $\alpha_i \in \lie{h}^*$ as $\alpha_i(H_j) = c_{ij}$.
Thus we can reconstruct the root system $\Phi$ on $\lie{h}^*$ and the scalar product, especially $\langle \cdot, \cdot \rangle$.
Hence for any $\alpha \in \lie{h}^*$ $\alpha(H_i) = \langle \alpha, \alpha_i \rangle$.

To go further, we need some tools.
First of all, a \textbf{multiple Lie product} $[AB \cdots YZ] \equiv (\ad{A})(\ad{B}) \cdots (\ad{Y})Z$.
We also denote $[A^i B] \equiv (\ad{A})^i B$.
Note that $(\ad{X})[AB \cdots Z] = [((\ad{X})A)B \cdots Z] + [A((\ad{X})B) \cdots Z] + \cdots + [AB \cdots ((\ad{X})Z)]$.
In particular, if $A, B, \cdots, Z$ are $E_i$'s and $F_i$'s, then we obtain that $(\ad{H_i})[AB \cdots Z]$ is a certain scalar multiplication of $(\ad{X})[AB \cdots Z]$.
Thus, any multiple Lie product of $H_i, E_i, F_i$'s is actually a scalar multiplication of a multiple Lie product of only $E_i, F_i$'s or a linear combination of $H_i$'s.
On the other hand, $(\ad{F_i}) [E_{i_1} E_{i_2} \cdots E_{i_r}]$ is the summation of $[E_{i_1} E_{i_2} \cdots E_{i_r}]$ in which one of $E_i$ is replaced by $[F_i, E_i] = -H_i$.
But as we mentioned any $H_i$ in the multiple Lie product becomes a just scalar multiplication, thus $(\ad{F_i}) [E_{i_1} E_{i_2} \cdots E_{i_r}]$ is a linear combination of $[E_{i_1} E_{i_2} \cdots E_{i_r}]$ in which one of $E_i$ is removed.
Similar result can be get for $(\ad{E_i}) [F_{i_1} F_{i_2} \cdots F_{i_r}]$.
In summary, we obtain that $\lie{g}$ is spanned by $H_i$'s and multiple Lie products of $[E_{i_1} \cdots E_{i_r}]$'s and multiple Lie products of $[F_{i_1} \cdots F_{i_r}]$'s.
This implies an important property.
Let $\lie{g}_\lambda = \{X \in \lie{g} \SBar [H, X] = \lambda(H)X \textrm{ for all $H \in \lie{h}$}\}$, as before.
Then it is obvious that $\lie{g} = \lie{h} \oplus \bigoplus \lie{g}_\alpha$, where the (direct) summation is taken over all nonnegative or nonpositive $\InZ$-linear combination of $\alpha_i$'s.

One more thing.
Let $\tau_i \equiv \exp{\ad{E_i}} \exp{(-\ad{F_i})} \exp{\ad{E_i}}$.
(This is a special case for $\tau^\rho_i \equiv \exp{\rho(E_i)} \exp{(-\rho(F_i))} \exp{\rho(E_i)}$ for representation $\rho$.)
It is easy to check that for $H \in \lie{h}$ $\tau_i^{-1} (\ad{H}) \tau_i = \ad{H} - \alpha_i(H) \ad{H_i}$.
Thus for $\alpha \in \lie{h}^*$ and any $X \in \lie{g}_\alpha$ ($\lie{g}_\alpha$ is defined as before) $(\ad{H})(\tau_i X) = \tau_i((\ad{H} - \alpha_i(H) \ad{H_i})X) = (\alpha - \langle \alpha, \alpha_i \rangle \alpha_i)(H) (\tau_i X)$.
From this we have that, since $\tau_i$ is bijective, $\tau_i \lie{g}_\alpha = \lie{g}_{\sigma_i(\alpha)}$, where $\sigma_i = \sigma_{\alpha_i}$ is the Weyl reflection.
It gives a useful work, but there is a problem: in general, $\tau_i$ is not well-defined since it contains exponentials.
However, in many cases one can show that the exponentials are well-defined, so that we can use this well.

\newpage

\textbf{Existence theorem (2), Isomorphism theorem}

Now we can prove the existence theorem.
To do this we shall investigate the condition for $\alpha \in \lie{h}^*$ to make $\lie{g}_\alpha \ne 0$ and $\dim{\lie{g}_\alpha}$.
This can be possible when $\tau_i$ is well-defined for all $i$.
Consider $X = [E_{i_1} E_{i_2} \cdots E_{i_m}]$ and fix $i \in \InZ$ with $1 \le i \le l$.
Let $q$ be the number of $i$'s in $i_j$'s.
One can remove $i$'s in the sequence $i_j$ and re-gather the remaining, making a new sequence $j_1, j_2, \cdots, j_n$.
Then the Leibnitz' rule implies that $X$ is summation of some (maybe not all) of $[((\ad{E_i})^{u_1} E_{j_1}) ((\ad{E_i})^{u_2} E_{j_2}) \cdots ((\ad{E_i})^{u_n} E_{j_n})]$ with $\sum u_t = q$.
For $X$ to not be 0 any of $u_t$ is smaller than or equal to $-c_{j_t i}$.
This implies that, if $s$ is bigger than $\sum (-c_{j_t i}) - q$, $(\ad{E_i})^s X = 0$.
Remind that the result of the calculation of $(\ad{E_i})[F_{i_1} F_{i_2} \cdots F_{i_m}]$ implies that $(\ad{E_i})^r [F_{i_1} F_{i_2} \cdots F_{i_m}] = 0$ for some $r$.
Also, $(\ad{E_i})^2 H_j = 0$ for any $j$.
Thus, $\ad{E_i}$ is locally nilpotent.
Similar, so is $\ad{F_i}$.
Therefore, $\tau_i$ is well-defined.
Remind that for $\alpha \in \Phi$ there is $\sigma \in W$ and $i$ such that $\sigma(\alpha) = \alpha_i$.
These give that for every $\alpha \in \Phi$ $\lie{g}_\alpha \ne 0$ and $\dim{\lie{g}_\alpha} = 1$.

We have seen that if $\alpha \in \Phi$ then $\lie{g}_\alpha \ne 0$.
We shall show that if $\lambda \notin \Phi$, then $\lie{g}_\lambda = 0$.
The above result says that for $\alpha \in \Phi$ and a scalar $c \ne \pm 1$ $\lie{g}_{c\alpha} = 0$ since, if not, $\lie{g}_{c\alpha_i} \ne 0$ for some $i$, which is absurd.
Now consider $\lambda$, a nonnegative or nonpositive $\InZ$-linear combination of $\alpha_i$'s, which is not 0 nor a multiple of any root.
Choose $\mu \in P_\lambda - \bigcup_{\alpha \in \Phi} P_\alpha$ ($P_\alpha$ is the plane), where the existence of such $\mu$ and that $\mu$ is regular is easy to show.
We know that there is $\sigma \in W$ such that $(\alpha_i, \sigma(\mu)) > 0$ for all $i$.
Let $\sigma(\lambda) = \sum a_i \alpha_i$.
Then $0 = (\sigma(\lambda), \sigma(\mu)) = \sum a_i (\alpha_i, \sigma(\mu))$, which implies that some but not all of $a_i$ is positive while the remainings are negative.
Thus $\lie{g}_{\sigma(\lambda)} = 0$ so that $\lie{g}_\lambda = 0$.
Therefore, we have that for $0 \ne \alpha \in \lie{h}^*$ $\lie{g}_\alpha \ne 0$ if and only if $\alpha \in \Phi$.
Since the number of roots is finite and $\dim{\lie{g}_\alpha} = 1$ for all root $\alpha$, $\lie{g}$ is finite-dimensional.
Finally, we restore the whole structure of finite-dimensional semisimple Lie algebra.

Finally, we can show that $\lie{g}$ is semisimple.
Let $\lie{r}$ be an Abelian ideal of $\lie{g}$.
Also let $A + \sum_{\alpha \in \Phi} a_\alpha E_\alpha \in \lie{r}$ where $A \in \lie{h}$.
Then, since $\lie{r}$ is an ideal, applying multiply $\ad{H}$ ($H \in \lie{h}$) to this element implies that $\lie{r} = (\lie{r} \cap \lie{h}) \oplus \bigoplus_{\alpha \in \Phi} (\lie{r} \cap \lie{g}_\alpha)$.
Suppose that $E_\alpha \in \lie{r}$ for some $\alpha \in \Phi$.
It involves that $H_\alpha = [E_\alpha, E_{-\alpha}] \in \lie{r}$ so that $\lie{s}_\alpha \subseteq \lie{r}$, a contradiction, so $\lie{r} \subseteq \lie{h}$.
But for any $X \in \lie{r} \subseteq \lie{h}$ we obtain that $[X, E_\alpha] = 0$ for any root $\alpha$, thus $\alpha(X) = 0$ for all root $\alpha$.
Then the fact that $\alpha_i$'s form a basis implies that $X = 0$.
Therefore, $\lie{r} = 0$ so that $\lie{g}$ is semisimple.

Our result proves also the isomorphism theorem.
First, we assume that $\Phi$ is irreducible so that $\lie{g}$ is simple.
Let $\lie{g}'$ be a finite-dimensional simple Lie algebra with a maximal toral subalgebra $\lie{h}$ with root system $\Phi' \subset \lie{h}^*$ with an isomorphism $m : \Phi \to \Phi'$ and $\Delta' = m(\Delta)$, which is obviously a base, and we choose $E'_\alpha \in \lie{g}'_\alpha$ for $\alpha \in \Delta'$ and let $H'_\alpha = [E'_\alpha, F'_\alpha] \in \lie{h}'$.
It is immediate that such $H'_\alpha, E'_\alpha, F'_\alpha$ ($\alpha \in \Delta'$) generate $\lie{g}'$ and satisfy the relations (1)$\sim$(4).
This implies that a linear mapping $\phi : \lie{g} \to \lie{g}'$ satisfying that $\phi|_\lie{h}$ coincides with the induced mapping $\lie{h} \to \lie{h}'$ from $m$ and $\phi(E_\alpha) = E'_{m(\alpha)}$ for all $\alpha \in \Delta$ is an isomorphism of $\lie{g}$ onto $\lie{g}'$.
Also, the universal property of free Lie algebra implies that any isomorphism of $\lie{g}$ onto $\lie{g}'$ satisfying the properties of $\phi$ coincides with $\phi$.
Therefore, for each type we see that there is unique (up to isomorphism) finite-dimensional simple Lie algebra of the type.

\newpage

\textbf{Lie elements}

There are some useful notes for free Lie algebras.
First, we let a (just) set $\mathcal{X} = \{X_1, \cdots, X_n\}$ and consider the free objects $T(V_\mathcal{X})$ and $\mathcal{F}_L(\mathcal{X}) (\subseteq (T(V_\mathcal{X}))_L)$.
Now we call $A \in T(V_\mathcal{X})$ a \textbf{Lie element} if $A \in \mathcal{F}_L(\mathcal{X})$.
We call $A \in T(V_\mathcal{X})$ \textbf{homogeneous of degree $r$} if $A$ is a linear combination of elements in the form of $X_{i_1} X_{i_2} \cdots X_{i_r}$.
It is immediate that every $A \in T(V_\mathcal{X})$ can be written (uniquely) as $A = \sum A_i$, where $A_i$ is homogeneous of degree $i$.

The following lemma (due to \textbf{Dynkin-Specht-Wever}) is relative to the homogeneity.
We employ again the multiple Lie product $[ABC \cdots Z]$.
\textit{We define $\sigma \in L([T(V_\mathcal{X}), T(V_\mathcal{X})], \mathcal{F}_L(\mathcal{X}))$ with $\sigma(X_i) = X_i$ and $\sigma(X_{i_1} \cdots X_{i_r}) = [X_{i_1} \cdots X_{i_r}]$.
Then a homogeneous $A \in [T(V_\mathcal{X}), T(V_\mathcal{X})]$ of degree $r$ is a Lie element if and only if $\sigma(A) = rA$.}
Since the form of $\sigma(A)$ is a form of a Lie element, if $\sigma(A) = rA$, then $A$ is a Lie element.
Now assume that $A$ is a Lie element.
To prove this bizarre lemma we need to see that $\sigma(A) \in \textrm{Der}(\mathcal{F}_L(\mathcal{X}))$.
First, we define an algebra-homomorphism $f : T(V_\mathcal{X}) \to L(\mathcal{F}_L(\mathcal{X}))$ generated by $f(X_i) = \ad{X_i}$.
Then $[X_{i_1} X_{i_2} \cdots X_{i_s} Y] = f(X_{i_1} X_{i_2} \cdots X_{i_s}) Y$.
Thus $\sigma(X_{i_1} X_{i_2} \cdots X_{i_s} Y) = f(X_{i_1} X_{i_2} \cdots X_{i_s}) \sigma(Y)$.
Especially, if $X \in \mathcal{F}_L(\mathcal{X})$, we have that $f(X)Y = [X, Y]$.
From this we obtain that, if $X, Y \in \mathcal{F}_L(\mathcal{X})$,  $\sigma([X, Y]) = \sigma(XY) - \sigma(YX) = f(X) \sigma(Y) - f(Y) \sigma(X) = [X, \sigma(Y)] + [\sigma(X), Y]$.
Thus $\sigma$ is a derivation.
This fact implies our lemma immediately.
Assume that $\sigma(A) = (r - 1)A$ if $A$ is of degree $r - 1$.
Then $\sigma([X_i, A]) = [\sigma(X_i), A] + [X_i, \sigma(A)] = [X_i, A] + [X_i, (r - 1)A] = r[X_i, A]$, which implies the result due to the induction on $r$.

Another lemma (due to \textbf{Friedrichs}) is introduced.
We define a product rule of $T(V_\mathcal{X}) \otimes T(V_\mathcal{X})$ as $(A \otimes B)(C \otimes D) = (AC) \otimes (BD)$.
It is immediate that $[A \otimes 1 + 1 \otimes A, B \otimes 1 + 1 \otimes B] = [A, B] \otimes 1 + 1 \otimes [A, B]$.
Based on this fact we argue that \textit{if $\delta : T(V_\mathcal{X}) \to T(V_\mathcal{X}) \otimes T(V_\mathcal{X})$ generated by $\delta(X_i) = X_i \otimes 1 + 1 \otimes X_i$ for all $i$, then $A \in T(V_\mathcal{X})$ is a Lie element if and only if $\delta(A) = A \otimes 1 + 1 \otimes A$.}
When $A$ is a Lie element, the fact that $[\delta(X), \delta(Y)] = \delta([X, Y])$ which we have seen implies that $\delta(A) = A \otimes 1 + 1 \otimes A$, immediately.
To treat the converse we let $(Y_i)$ be a basis of $\mathcal{F}_L(\mathcal{X})$.
By PBW theorem $\mathcal{B} = \{Y_1^{r_1} Y_2^{r_2} \cdots Y_m^{r_m} \SBar m \in \NaN, 0 \le r_i \in \InZ\}$ is a basis of $T(V_\mathcal{X}) \cong U(\mathcal{F}_L(\mathcal{X}))$ so that $\mathcal{C} = \{X \otimes Y \SBar X, Y \in \mathcal{B}\}$ is a basis of $T(V_\mathcal{X}) \otimes T(V_\mathcal{X})$.
On the other hand, $\delta(Y_1^{r_1} Y_2^{r_2} \cdots Y_m^{r_m}) = (Y_1 \otimes 1 + 1 \otimes Y_1)^{r_1} (Y_2 \otimes 1 + 1 \otimes Y_2)^{r_2} \cdots (Y_m \otimes 1 + 1 \otimes Y_m)^{r_m}$, and by expansion it can be written as $Y_1^{r_1} Y_2^{r_2} \cdots Y_m^{r_m} \otimes 1 + \sum r_i Y_1^{r_1} \cdots Y_{i - 1}^{r_{i - 1}} Y_i^{r_i - 1} Y_{i + 1}^{r_{i + 1}} \cdots Y_m^{r_m} \otimes Y_i + (\cdots)$, where the omitted terms are a linear combination of other elements in $\mathcal{C}$ with the second component not equal to 1 and any of $Y_i$.
Note that since this is written as a linear combination of the basis $\mathcal{C}$ it cannot be written as any other linear combination of $\mathcal{C}$.
Then to make this satisfy $\delta(A) = A \otimes 1 + 1 \otimes A$ it must be satisfied that all $r_j$ are 0 but only one $r_i$ is 1.
Therefore, $\delta(A) = A \otimes 1 + 1 \otimes A$ holds only if $A$ is a Lie element.

\newpage

\textbf{Formal power series}

We need a 'power series' which makes $\exp{A}$ and $\log{(1 + A)}$ sense for any (not neccesarily to be (locally) nilpotent $A \in T(V_{\mathcal{X}})$.
(We do not need a topology since everythig in this text is formal.)
Note that $\mathcal{A} = T(V_\mathcal{X})$ can be wrriten as $\mathcal{A} = \bigoplus_{i = 0}^\infty \mathcal{A}_i$, where $\mathcal{A}_i$ is the set of homogeneous elements of degree $i$, and we can write $A = \sum_{i = 0}^\infty A_i$, where $A_i \in \mathcal{A}_i$ and all $A_i$ are 0 but some finite is not.
We consider a 'summation' $A = \sum_{i = 0}^\infty A_i$ ($A_i \in \mathcal{A}_i$), but the number of non-zero $A_i$ may not be finite.
Of course, this is 'well-defined' if we just regard such element as a sequence $(A_0, A_1, A_2, \cdots)$.
One can give an addition and a multiplication to the set of such elements by pointwise.
Also, we can define a 'multiplication' as $(\sum_{i = 0}^\infty A_i) (\sum_{i = 0}^\infty B_i) = \sum_{i = 0}^\infty (\sum_{j = 0}^i A_j B_{i - j})$, naturally.
Now we let $\overline{\mathcal{A}}$ be the set of all such $\sum_{i = 0}^\infty A_i$, called a \textbf{formal power series}, and we can give an algebra structure as seen.
In the similar sense one can define $\overline{\mathcal{F}_L(\mathcal{X})}$.
Also we call $A \in \overline{\mathcal{A}}$ a Lie element if $A \in \overline{\mathcal{F}_L(\mathcal{X})}$.
It is immediate that $\sum_{i = 0}^\infty A_i$ ($A_i \in \mathcal{A}_i$) is a Lie element if and only if $A_i \in \mathcal{F}_L(\mathcal{X})$.

Our interest is in whether the Friedrichs' lemma can be used in $\overline{\mathcal{A}}$ and $\overline{\mathcal{F}_L(\mathcal{X})}$, especially the part for determination whether a given element in $\overline{\mathcal{A}}$ is in $\overline{\mathcal{F}_L(\mathcal{X})}$ or not.
To do this we need to define $\overline{\mathcal{A} \otimes \mathcal{A}}$ as above.
This is well-defined since we can give a homogeneous structure as $(\mathcal{A} \otimes \mathcal{A})_i = \{\sum_j A_j \otimes B_{i - j} \SBar A_r, B_r \in \mathcal{A}_r\}$.
Note that $\delta$ sends $(\mathcal{A})_i$ into $(\mathcal{A} \otimes \mathcal{A})_i$.
From this $\delta : \overline{\mathcal{A}} \to \overline{\mathcal{A} \otimes \mathcal{A}}$ is also well-defined and the Friedrichs' lemma holds for $\overline{\mathcal{A}}$ and $\overline{\mathcal{A} \otimes \mathcal{A}}$ and $\overline{\mathcal{F}_L(\mathcal{X})}$.

In the next page this configuration and the above lemmas give a new powerful formula.

\newpage

\textbf{Campbell-Baker-Hausdorff formula}

Now we get one of most important formulas in Lie theory.
As seen, on $T(V_\mathcal{X})$ we can define $\exp{A} = 1 + A + \frac{1}{2!} A^2 + \frac{1}{3!} A^3 + \cdots$ and $\log{(1 +A)} = A - \frac{1}{2} A^2 + \frac{1}{3} A^3 - \cdots$ for $A \in \bigoplus_{i = 1}^\infty (T(V_\mathcal{A}))_i \subseteq \overline{T(V_\mathcal{X})}$ (there is no issue about convergence; all we use is formal!)
Thanks to the 'standard' derivation, defined by $A \mapsto 1$, one can easily see that $\exp{\log{(1 + A)}} = 1 + A$ and $\log{\exp{A}} = A$.
Also, if $A, B \in \bigoplus_{i = 1}^\infty (T(V_\mathcal{A}))_i$ with $AB = BA$, it is immediate that $\exp{(A + B)} = \exp{A} \exp{B}$ and $\log{((1 + A)(1 + B))} = \log{(1 + A)} + \log{(1 + B)}$.

What about when $AB \ne BA$?
The following theorem gives an answer and the important formula: \textit{Let $X, Y \in \mathcal{X}$.
Then $Z = \log{(\exp{A} \exp{B})}$ is a Lie element, i.e., $Z \in \overline{\mathcal{F}_L(\mathcal{X})}$.
Moreover, }
\begin{displaymath}
  Z = \sum \frac{(\ad{X})^{p_1} (\ad{Y})^{q_1} (\ad{X})^{p_2} (\ad{Y})^{q_2} (\ad{X})^{p_3} (\ad{Y})^{q_3} \cdots}{p_1! q_1! p_2! q_2! \cdots p_m! q_m! (\sum p_i + \sum q_j)}.
\end{displaymath}

\textit{This formula is called the \textbf{Campbell-Baker-Hausdorff formula} (abbreviated CBH formula).}
We shall use the extended version of Friedrichs' lemma.
It is immediate that we can define $\exp$ and $\log$ in $\overline{T(V_\mathcal{X}) \otimes T(V_\mathcal{X})}$ and that $\exp{(1 \otimes A)} = (\exp{A}) \otimes 1$, $\exp{(A \otimes 1)} = 1 \otimes (\exp{A})$, and so is $\log$.
Also, since $(A \otimes 1)(1 \otimes B) = (1 \otimes B)(A \otimes 1)$, we have that $\exp{(A \otimes 1 + 1 \otimes B)} = ((\exp{A}) \otimes 1)(1 \otimes (\exp{B}))$ and $\log{((1 + A) \otimes 1)(1 \otimes (1 + B))} = (\log{(1 + A)}) \otimes 1 + 1 \otimes (\log{(1 + B)})$.
From these properties, it is immediate that
\begin{eqnarray*}
  & & \delta(\log{(\exp{X} \exp{Y})}) \\
  &=& \log{(\exp{(X \otimes 1 + 1 \otimes X)} \exp{(Y \otimes 1 + 1 \otimes Y)})} \\
  &=& \log{(((\exp{X}) \otimes 1) ((\exp{Y}) \otimes 1) (1 \otimes (\exp{X})) (1 \otimes (\exp{Y})))} \\
  &=& (\log{(\exp{X} \exp{Y})}) \otimes 1 + 1 \otimes (\log{(\exp{X} \exp{Y})}).
\end{eqnarray*}
Thus, due to the Friedrich' lemma, $Z$ is a Lie element.

The remaining is immediate.
Since $\exp{X} \exp{Y} = \sum \frac{X^m}{m!} \frac{Y^n}{n!}$, it is direct that $Z = \sum_{r = 0}^\infty Z_r$, where $Z_r = \sum_{\sum p_i + \sum q_i = r} \frac{X^{p_1}}{p_1!} \frac{Y^{q_1}}{q_1!} \frac{X^{p_2}}{p_2!} \frac{Y^{q_2}}{q_2!} \cdots \frac{X^{p_m}}{p_m!} \frac{Y^{q_m}}{q_m!}$.
Note that $Z$ is a Lie element if and only if all of $Z_r$ is a Lie element.
Therefore, applying the Dynkin-Specht-Wever lemma, we obtain the formula.

\newpage

\textbf{Products of Exponentials of matrices}

For further use we investigate the CBH formula in less abstract configuration.\footnote{This is an answer of Exercise 6 in Chapter V, Jacobson.}
Let $D_1 \in L_k(V)$, where $V$ is a $n$-dimensional vector space over the base field $k$.
We want to handle $\exp{D_1}$, but it is generally nonsense since we do not have any topology to make convergence.
To avoid this problem we extend the field.
Let $K_1$ be the field of quotients of $k[[t_1]]$, which is the algebra of formal power series with respect to an indeterminant $t_1$, and we denote the extensipn of $L_k(V)$ by $L_{K_1}(V)$.
Now, instead of $\exp{D_1}$, we consider $\exp{t_1 D_1}$.
If we fix a basis of $V$ and denote the $(i, j)$-component of the matrix of $D_1^n$ relative to the basis by $(D_1^n)_{ij}$, then $\sum_{n = 0}^\infty ((D_1^n)_{ij}/n!) t_1^n$ is a well-defined element in $K_1$, so $\exp{t_1 D_1}$ is well-defined.

We go further.
Let $D_2 \in L_k(V)$ and we consider $\exp{D_1} \exp{D_2}$.
Again, we need to a field extension to make $\exp{t_1 D_1} \exp{t_2 D_2}$ sense, so let $K_2$ be the field of quotients of $k[[t_1, t_2]]$.
Then $\exp{t_1 D_1}$ and $\exp{t_2 D_2}$ are well-defined.
What we shall do is to apply the CBH formula to $\exp{t_1 D_1} \exp{t_2 D_2}$.
Let $m_1(t)$ and $m_2(t)$ be the minimal polynomials of $t_1 D_1$ and $t_2 D_2$, respectively.
Also, let $X_1$ and $X_2$ be some element.
Then there is a (algebra) homomorphism $\phi : T(V_{\{X_1, X_2\}}) \to L_{K_2}(V)$ such that $\phi(X_i) = t_i D_i$.
Moreover, if $\mathcal{I}$ is the two-sided ideal generated by $m_1(X_1)$ and $m_2(X_2)$, then $T(V_{\{X_1, X_2\}})/\mathcal{I}$ is isomorphic to $\phi(T(V_{\{X_1, X_2\}}))$.
The CBH formula says that we have $Z = \log{(\exp{X_1} \exp{X_2})}$ and that $\exp{\phi(Z)} = \exp{t_1 D_1} \exp{t_2 D_2}$.
In this point the CBH formula gives more useful result.
Assume that $V$ has an algebra-structure and that $D_1$ and $D_2$ are derivatives of $V$.
Since the CBH formula consists of only $\ad{X_i}$'s, it is clear that $\phi(Z)$ consists of only $\ad{t_i D_i}$'s, so $\phi(Z)$ is a derivative.

This result can be more extended by induction.
Let $D_1, D_2, \cdots, D_r$ be derivatives of $V$, and $K_r$ be the field of quotients of $k[[t_1, t_2, \cdots, t_r]]$ with indeterminants $t_i$'s.
Then there is a derivation $D \in L_{K_r}(V)$ such that $\exp{D} = \exp{t_1 D_1} \exp{t_2 D_2} \cdots \exp{t_r D_r}$.

\newpage

\textbf{Preliminaries for Ado's theorem}

Although we already have a finite-dimensional faithful representation of semisimple Lie algebras, one might wish to have a finite-dimensional faithful representation of arbitrary Lie algebras.
Ado's theorem is the good news.
We shall study this theorem.

Let $\lie{g}$ be a finite-dimensional Lie algebra.
It is immediate that any aoosciative algebra with 1 has its faithful representation ($A \mapsto L_A$).
Thus if one can find a finite-dimensional associative $k$-algebra $\mathcal{A}$ with 1 and a monomorphism of $\lie{g}$ into $\mathcal{A}_L$, our proof is done.
Actually it can be archived if we can find finite-codimensional $\mathcal{K} \trianglelefteq U(\lie{g})$, so this is our aim.

To do this we need some stuffs.
Let $\mathcal{K} \trianglelefteq U(\lie{g})$.
We call $A \in U(\lie{g})$ \textbf{algebraic modulo $\mathcal{K}$} if $f(A) \in \mathcal{K}$ for some polynomial $f$, or $f(A + \mathcal{K}) = 0 + \mathcal{K}$.
Obviously, if $\mathcal{K}$ is finite-codimensional, every $A \in U(\lie{g})$ is algebraic modulo $\mathcal{K}$.
Actually, the inverse also holds, i.e., \textit{if $(U_i)$ is a basis of $\lie{g}$, then $\mathcal{K}$ is finite-codimensional if and only if all $U_i$ are algebraic modulo $\mathcal{K}$}.
The inverse is an almost direct consequence of PBW theorem.
Let $f_i$ be polynomials such that $f_i(U_i) \in \mathcal{X}$ and $m_i$ its degree.
Then every $\prod U_i^{r_i} + \mathcal{K}$ can be written as a linear combination of $\prod U_i^{s_i} + \mathcal{K}$ with $s_i < m_i$, which implies the finite-codimensionality.
From this it is immediate that \textit{if $\mathcal{K}, \mathcal{L} \trianglelefteq U(\lie{g})$ are finite-codimensional, then so is $\mathcal{K} \mathcal{L}$}; if we let $f_i, g_i$ be polynomials such that $f_i(U_i) \in \mathcal{K}$ and $g_i(U_i) \in \mathcal{L}$, then $f_i(U_i) g_i(U_i) \in \mathcal{K} \mathcal{L}$.
Another stuff is that \textit{for an algebra $\mathcal{A}$ with generator $\mathcal{X}$ and a derivative $D$ of $\mathcal{A}$ if for each $X \in \mathcal{X}$ there is $n_X \in \NaN$ such that $D^{n_X} X = 0$, then $D$ is locally nilpotent}, which is obvious by Leibnitz' rule.

What we are interested in is $\mathcal{K}$ such that all elements in the nil radical $\lie{n}$ of $\lie{g}$ are nilpotent modulo $\mathcal{K}$ (i.e., its some power is in $\mathcal{K}$).
Such ideal which is also finite-codimensional is useful, but we need a more strengthened ideal.
The following lemma is for this: \textit{if $\lie{g}$ is solvable, then for such above $\mathcal{K}$ there is a finite-codimensional ideal $\mathcal{L} \subseteq \mathcal{K}$ of $U(\lie{g})$ such that not only all elements in $\lie{n}$ is nilpotent modulo $\mathcal{L}$ but also for every derivation $D$ of $\lie{g}$ its extension on $U(\lie{g})$ leaves $\mathcal{L}$ invariant.}
What we first guess is that if we let $\mathcal{B}$ be the ideal generated by $\lie{n}$ and $\mathcal{K}$ then $\mathcal{B}$ is somewhat 'nilpotent modulo $\mathcal{K}$'.
It would be treated in the language of quotient spaces, so we consider $\mathcal{B}/\mathcal{K}$.
Obviously, it is an ideal of $U(\lie{g})/\mathcal{K}$ generated by $(\lie{n} + \mathcal{K})/\mathcal{K}$.
But by the assumptions (n.b., not only the nilpotency modulo $\mathcal{K}$ but also the fact that $\lie{n}$ is a nilpotent ideal are required) all elements in this ideal are nilpotent, thus the (finite-dimensional) subalgebra $(\lie{n} + \mathcal{K})/\mathcal{K}$ of $(\lie{g} + \mathcal{K})/\mathcal{K}$ is nilpotent so that it is contained in the radical of $(\lie{g} + \mathcal{K})/\mathcal{K}$.
Hence, as we know, $\mathcal{B}/\mathcal{K}$ is in the radical of $U(\lie{g})/\mathcal{K}$.
It implies that there is $r \in \NaN$ such that $\mathcal{B}^r \subseteq \mathcal{K}$.

Now we let $\mathcal{L} = \mathcal{B}^r$ and show that this is what we want.
It is obvious that $\mathcal{L} \trianglelefteq U(\lie{g})$.
Also, since $\mathcal{B}$ contains $\mathcal{K}$ so that it is finite-codimensional, $\mathcal{L} = \mathcal{B}^r$ is also finite-codimensional, as seen.
To see the property about derivative, remind that the radical of any finite-dimensional Lie algebra is sent into the nil radical by any derivative of the whole Lie algebra.
Using this, we can see that any derivative $D$ sends $\lie{g}$ into $\lie{n}$, which makes the last required property hold.

\newpage

\textbf{Ado's theorem}

In this page we show that \textit{every finite-dimensional Lie algebra has a finite-dimensional faithful representation}, called the \textbf{Ado's theorem}.
Let $\lie{g}$ be a finite-dimensional Lie algebra, and $\lie{c}$, $\lie{n}$, $\lie{r}$ its center, nil radical, radical, respectively.
We already know that the adjoint representation $\lie{c}$ becomes 0 so it is not faithful in general.
The solution is to find a finite-dimensional representation $\rho$ with $\rho(C) \ne 0$ for all $0 \ne C \in \lie{c}$, and to sum directly this with the adjoint representation.
Hence, such $\rho$ is our target.
The main plan is to use a chain $\lie{c} = \lie{r}_1 \triangleleftneq \lie{r}_2 \triangleleftneq \cdots \triangleleftneq \lie{r}_{r - 1} \triangleleftneq \lie{r}_r = \lie{n} \triangleleftneq \lie{r}_{r + 1} \triangleleftneq \lie{r}_{r + 2} \triangleleftneq \cdots \triangleleftneq \lie{r}_{s - 1} \triangleleftneq \lie{r}_s = \lie{r} \trianglelefteq \lie{g} = \lie{r}_{s + 1}$, which exists by Lie's theorem.
Note that for each $i$ there is a subalgebra $\lie{g}_i$ such that $\lie{r}_{i + 1} = \lie{g}_i \oplus \lie{r}_i$, where the case for $i = s$ the existence of such $\lie{g}_i$ is from Levi's theorem, and then if there is a representation $\rho_i$ of $\lie{r}_i$ for some $i$, we shall construct a representation $\rho_{i + 1}$ of $\lie{r}_{i + 1}$ with $\ker{\rho_{i + 1}} \subseteq \ker{\rho_i}$, then we can conclude that there will be a representation $\rho = \rho_{s + 1}$ of $\lie{g}$ which we seek, since we already have a sufficient faithful representation of $\lie{c}$.
To see this let $\rho_{i + 1}^0$ be a homomorphism $\lie{r}_{i + 1} \to \lie{gl}(U(\lie{r}_i))$ defined as $\rho_0(Y + Z)(A) = (\ad{Y})A + ZA$ for $Y \in \lie{g}_i$ and $Z \in \lie{r}_i$ and $A \in U(\lie{r}_i)$.
It is not finite-dimensional, but it is immediate that $\rho_0$ is well-defined representation of $\lie{r}_{i + 1}$.
To make this finite-dimensional, we let $\mathcal{K}$ be the kernel of homomorphism $U(\lie{r}_i) \to \lie{r}_i$, and let $\mathcal{L} \subseteq \mathcal{K}$ be the ideal constructed in the previous page.
Then it is also immediate that $\mathcal{L}$ is invariant under $\rho_{i + 1}^0(\lie{r}_{i + 1})$ since $\mathcal{K}$ is invariant under derivatives $\ad{Y}$, so we can have the induced representation, $\rho_{i + 1} : \lie{r}_{i + 1} \to U(\lie{r}_i)/\mathcal{L}$ which is finite-dimensional.
The remaining is to show that $\ker{\rho_{i + 1}} \subseteq \ker{\rho_i}$; it guarantees that $\ker{\rho_{i + 1}} \cap \lie{c} = 0$.
But it is obvious since the condition $\rho_{i + 1}(X) = 0$ is equivalent to that $X \in \mathcal{L}$ and $\mathcal{L} \subseteq \mathcal{K}$.
Hence, the proof of Ado's theorem is over.

\newpage

\part{Conjugation theorem and Automorphism theory}

\newpage

\textbf{Engel subalgebra}

Let $\lie{g}$ be an arbitrary (but finite-dimensional) Lie algebra and $A \in \lie{g}$.
We denote $\lie{g}_0(\ad{A}) = \{X \in \lie{g} \SBar [A, X] = 0\}$.
As the root space decomposition (consider a subalgebra $k(\ad{A})$) $\lie{g}_0(\ad{A})$ is a subalgebra.
We call this subalgebra $\lie{g}_0(\ad{A})$ an \textbf{Engel subalgebra}.

We investigate some lemma for Engel subalgbras.
First, \textit{let $\lie{h} \le \lie{g}$ and assume that we have $A \in \lie{h}$ such that for $X \in \lie{h}$ with $\lie{g}_0(\ad{X}) \subseteq \lie{g}_0(\ad{A})$ we have $\lie{g}_0(\ad{X}) = \lie{g}_0(\ad{A})$ (hence it is minimal) and $\lie{h} \subseteq \lie{g}_0(\ad{A})$.
Then $\lie{g}_0(\ad{A}) \subseteq \lie{g}_0(\ad{X})$ for every $X \in \lie{h}$ (hence it is least).}
To show this we denote $\lie{h}_0 = \lie{g}_0(\ad{A})$ and fix $B \in \lie{h}$, and consider $T_a = \ad{(A + aB)}$ ($a$ scalar).
Note that, since $\lie{h}_0$ is a subalgebra containing all $A + aB$, $T_a$ leaves $\lie{h}_0$ invariant.
This implies that if $f_a(t)$ is the characteristic polynomial of $T_a|_{\lie{h}_0}$ and $g_a(t)$ is that of the induced mapping $T'_a$ of $T_a$ on $\lie{g}/\lie{h}_0$, then the characteristic polynomial of $T_a$ is $f_a(t) g_a(t)$.
We denote $n = \dim{\lie{g}}$ and $r = \dim{\lie{h}}_0$.
We can denote $f_a(t) = \sum_{i = 0}^r f_{r - i}(a) t^i$ and $g_a(t) = \sum_{i = 0}^{n - r} g_{n - r - i}(a) t^i$, where $f_0(a) = g_0(a) = 1$.
Note that from the matrix language all $f_i(a)$ and $g_i(a)$ are polynomials of $a$ which are of degree at most $i$.

This is the main ingredients.
We shall mix them and then we show that $T_a|_{\lie{h}_0}$ is nilpotent for all $a$ so that $\lie{g}_0(T_a) \supseteq \lie{h}_0$.
To see this it is sufficient to show that all $f_i$ ($0 \le i \le r - 1$) are 0.
Since these are polynomials of degree at most $r$, what we do sufficiently is to find $a_1, a_2, \cdots, a_{r + 1}$ such that $f_i(a_j) = 0$ for all $i$ and $j$, which is equivalent to that $T_{a_j}|_{\lie{h}_0}$ is nilpotent.
Note that $T_a|_{\lie{h}_0}$ is nilpotent when $\lie{g}_0(T_a) = \lie{h}_0$.
On the other hand, due to the minimality of $\lie{h}_0$, for some $a$ to see this it is sufficient to show that $\lie{g}_0(T_a) \subseteq \lie{h}_0$.
It holds when $T'_a$, the induced mapping of $T_a$ on $\lie{g}/\lie{h}_0$ is invertible, or $g_{n - r}(a) \ne 0$.
But $g_{n - r}(0) \ne 0$, since there is no $X \in \lie{g}$ such that $T'_0(X + \lie{h}_0) = 0 + \lie{h}_0$ by the definition of $\lie{h}_0$, so that $g_{n - r} \ne 0$.
Thus we can choose $a_1, a_2, \cdots, a_{r + 1}$ such that $g_{n - r}(a_i) \ne 0$ for all $i$, so $T_{a_j}|_{\lie{h}_0}$ are nilpotent for all $j$, which is promised.

Finally, we have that $\lie{h}_0 \subseteq \lie{g}_0(\ad{(A + aB)})$ for any scalar $a$ and $B \in \lie{h}$ (not necessarily equal since there is the case for $g_0(a) = 0$).
If we choose $a = 1$ and $B = X - A$ for $X \in \lie{h}$, then $\lie{h}_0 \subseteq \lie{g}_0(\ad{X})$, which we seek.

There is another important lemma.
\textit{Let $\lie{h} \le \lie{g}$ contain an Engel subalgebra, i.e., $\lie{g}_0(\ad{A}) \subseteq \lie{h}$ for some $A \in \lie{h}$.
Then the normalizer $\lie{n}_\lie{g}(\lie{h})$ of $\lie{h}$ coincides with $\lie{h}$}, which is immediate from that $(\ad{A})(\lie{n}_\lie{g}(\lie{h})) \subseteq \lie{h}$, or $\ad{A} \in L(\lie{n}_\lie{g}(\lie{h}), \lie{h})$.

These lemmas will help to treat the next topics.
Before finishing this page, we need to mention about a term 'minimal Engel algebra'.
We will encounter a subalgebra $\lie{h} = \lie{g}_0(\ad{A})$ such that $\lie{g}_0(\ad{X}) \subseteq \lie{g}_0(\ad{A})$ yields $\lie{g}_0(\ad{X}) = \lie{g}_0(\ad{A})$ for only $X \in \lie{h}$.
Hence $\lie{g}_0(\ad{A})$ is minimal in all $\lie{g}_0(\ad{X})$ for $X \in \lie{h}$.
If one says that $\lie{g}_0(\ad{A})$ is minimal, then usually it means that it is minimal in all $\lie{g}_0(\ad{X})$ for $X \in \lie{g}$, not for only in $\lie{h}$.
However, in the case for such $\lie{h}$, actually, it is minimal in all $\lie{g}_0(\ad{X})$ for $X \in \lie{g}$; if for some $X \in \lie{g}$ $\lie{g}_0(\ad{X}) \subseteq \lie{g}_0(\ad{A})$, since $X \in \lie{g}_0(\ad{X}) \subseteq \lie{g}_0(\ad{A})$, $\lie{g}_0(\ad{X}) = \lie{g}_0(\ad{A})$.

\newpage

\textbf{Cartan subalgebra}

Given a Lie algebra $\lie{g}$ we call $\lie{h}$ a \textbf{Cartan subalgebra} (abbreviated CSA) if $\lie{h}$ is nilpotent and its normalizer coincides with $\lie{h}$, i.e., for some $X \in \lie{g}$ if $[X, \lie{h}] \subseteq \lie{h}$, then $X \in \lie{h}$.
One might guess the existence of Cartan subalgebra.
Fortunately, if the base field is of characteristic 0, we can always find a Cartan subalgebra.
The following is the generalized version of this statement: 
\textit{Let $\lie{h} \le \lie{g}$.
$\lie{h}$ is a CSA if and only if $\lie{h}$ is a minimal Engel subalgebra.}

The proof is here.
Assume that $\lie{h} = \lie{g}_0(\ad{A})$ is a minimal Engel subalgebra.
The second lemma in the previous page says that $\lie{n}_\lie{g}(\lie{h}) = \lie{h}$.
Thus we need to show that $\lie{h}$ is nilpotent.
The first lemma in the previous page says that $\lie{h} = \lie{g}_0(\ad{A}) \subseteq \lie{g}_0(\ad{X})$ for all $X \in \lie{h}$.
This implies that for any $X \in \lie{h}$ $\adu{\lie{h}}{X}$ are nilpotent.
Thus, by Engel's theorem, $\lie{h}$ is nilpotent, so $\lie{h}$ is a CSA.

Conversely, assume that $\lie{h}$ is a CSA.
The nilpotency says that $\lie{h} \subseteq \lie{g}_0(\ad{X})$ for all $X \in \lie{h}$.
If there is at least one $X \in \lie{h}$ such that the equality holds, then the proof is over.
Now we suppose that there is no such $X$.
Nevertheless, we can choose $A \in \lie{h}$ such that $\lie{g}_0(\ad{A})$ is minimal.
Then by the first lemma of the previous page $\lie{g}_0(\ad{A}) \subseteq \lie{g}_0(\ad{X})$ for all $X \in \lie{h}$.
From this we obtain that $\ad{\lie{h}}$ acts on $\lie{g}_0(\ad{A})/\lie{h}$ nilpotently.
Then the main part of Engel's theorem implies that there is $Y \in \lie{g}_0(\ad{A})$ such that $Y \notin \lie{h}$ and $\ad{\lie{h}}$ sends $Y + \lie{h}$ to $0 + \lie{h}$, that is, $[\lie{h}, Y] \subseteq \lie{h}$.
Thus we obtain $\lie{h} \ne \lie{n}_\lie{g}(\lie{h})$, a contradiction to the assumption.
Therefore, $\lie{h}$ is a minimal Engel subalgebra.

We return to the semisimplicity.
We can now prove that \textit{a subalgebra of a finite-dimensional semisimple Lie algebra $\lie{g}$ is a CSA if and only if it is a maximal toral subalgebra.}
It is easy that every maximal toral subalgebra of $\lie{g}$ is a CSA.
Now, let $\lie{h}$ be a CSA.
By the above theorem we can write $\lie{h} = \lie{g}_0(\ad{H})$ for some $H \in \lie{h}$, which is minimal.
Let $H_S$ be the semisimple part of $H$.
It is immediate that $\lie{g}_0(\ad{H_S}) \subseteq \lie{g}_0(\ad{H})$, thus by the minimality $\lie{h} = \lie{g}_0(\ad{H_S})$.
On the other hand, $\lie{g}_0(\ad{H_S}) = \lie{c}_\lie{g}(H_S)$, so $\lie{c}_\lie{g}(H_S) = \lie{h}$.
But $\lie{c}_\lie{g}(H_S)$ contains a maximal toral subalgebra $\lie{h}'$ (containing $H_S$), which is a CSA, or another Engel subalgebra.
Thus, again by the minimality, we obtain that $\lie{h} = \lie{h}'$.

As a corollary, for any maximal toral subalgebra (or CSA) $\lie{h} \le \lie{g}$ we can find $H \in \lie{h}$ such that $\lie{c}_\lie{g}(H) = \lie{h}$.
We call a semisimple $H \in \lie{g}$ \textbf{regular semisimple} if there is a maximal toral subalgebra (or CSA) $\lie{h}$ such that $\lie{c}_\lie{g}(H) = \lie{h}$.

There is some functional lemmas.
First, \textit{let $\phi : \lie{g} \to \lie{g}'$ be a epimorphism of Lie algebras. Then for any CSA $\lie{h} \le \lie{g}$ $\lie{h}' = \phi(\lie{h})$ is a CSA.}
Obviously, $\lie{h}'$ is nilpotent.
Let $\phi(X) \in \lie{n}_\lie{g}(\lie{h}')$.
Then $[X, H + A] \in \lie{h} + \ker{\phi}$ for any $H \in \lie{h}$ and $A \in \ker{\phi}$ so that $X \in \lie{n}_\lie{g}(\lie{h} + \ker{\phi})$.
But since $\lie{h} + \ker{\phi}$ contains an Engel subalgebra $\lie{h}$, $\lie{n}_\lie{g}(\lie{h} + \ker{\phi}) = \lie{h} + \ker{\phi}$.
Thus $\phi(X) \in \lie{h}'$, so $\lie{n}_\lie{g}(\lie{h}') = \lie{h}'$.
Another lemma is that \textit{if $\phi : \lie{g} \to \lie{g}'$ be a epimorphism of Lie algebras, again, but a CSA $\lie{h}' \le \lie{g}'$, then any CSA $\lie{h}$ of $\lie{k} = \phi^{-1} (\lie{h}')$ is a CSA of $\lie{g}$.}
Again, $\lie{h}$ is nilpotent.
Let $X \in \lie{n}_\lie{g}(\lie{h})$.
If we show that $X \in \lie{k}$, then we obtain that $X \in \lie{h}$.
To see this, we see that by the above lemma $\phi(\lie{h})$ is a CSA of $\lie{h}'$, so that $\lie{h}' = \phi(\lie{h})$ by the minimality of $\lie{h}'$.
Then $\phi(X) \in \lie{n}_\lie{g}(\phi(\lie{h})) = \lie{n}_\lie{g}(\lie{h}')$, so that $\phi(X) \in \phi(\lie{h})$ or $\phi|_\lie{k}(X) \in \phi|_\lie{k}(\lie{h})$, which implies that $X \in \lie{k}$.
Thus $X \in \lie{h}$, or $\lie{n}_\lie{g}(\lie{h}) = \lie{h}$.
Therefore, $\lie{h}$ is a CSA.

\newpage

\textbf{The group $E(\lie{g})$}

Again, let $\lie{g}$ be a finite-dimensional Lie algebra.
We call $X \in \lie{g}$ \textbf{strongly ad-nilpotent} if there are $Y \in \lie{g}$ and a non-zero scalar $a$ such that $X \in \lie{g}_a(\ad{Y})$ and denote the set of all strongly ad-nilpotent element of $\lie{g}$ by $\mathcal{N}(\lie{g})$.
The fact that $[\lie{g}_a(\ad{Y}), \lie{g}_b(\ad{Y})] \subseteq \lie{g}_{a + b}(\ad{Y})$ implies that $X$ is ad-nilpotent.
Now we let $E(\lie{g})$ be the group generated by all $\exp{\ad{X}}$ with $X \in \mathcal{N}(\lie{g})$.
Then $E(\lie{g})$ is obviously a subgroup of $\textrm{Int}(\lie{g})$.
Note that for any $\phi \in \textrm{Aut}(\lie{g})$ leaves $\mathcal{N}(\lie{g})$ invariant, so $E(\lie{g})$ is a normal group of $\textrm{Aut}(\lie{g})$.

This new type of automorphisms is more powerful to use than $\textrm{Int}(\lie{g})$.
Let $\lie{h} \le \lie{g}$.
See that since for any $X \in \lie{h}$ $\lie{h}_a(\ad{X}) \subseteq \lie{g}_a(\ad{X})$ so that for any $Y \in \lie{h}_a(\ad{X})$ and a scalar $b$ $[Y, \lie{g}_b(\ad{X})] \subseteq \lie{g}_{a + b}(\ad{X})$, $\mathcal{N}(\lie{h}) \subseteq \mathcal{N}(\lie{g})$.
This permits us to define a subgroup $E(\lie{h}; \lie{g})$ of $E(\lie{g})$, which is generated by all $\exp{\adu{\lie{g}}{X}}$ for all $X \in \lie{h}$.
Then $E(\lie{h})$ is just $\{A|_\lie{h} \SBar A \in E(\lie{h}; \lie{g})\}$.
Note that this property does not hold generally for $\textrm{Int}(\lie{h})$ and $\textrm{Int}(\lie{g})$.

Using this property we can show some functional properties.
\textit{Let $\phi : \lie{g} \to \lie{g}'$ be an epimorphism. Then $\phi(\mathcal{N}(\lie{g})) = \mathcal{N}(\lie{g}')$}; it is clear from the fact that for $X \in \lie{g}$ $\phi(\lie{g}_a(\ad{X})) = \lie{g}'_a(\ad{\phi(X)})$.
(To show that $\lie{g}'_a(\ad{\phi(Y)}) \subseteq \phi(\lie{g}_a(\ad{Y}))$, use that if $A \in \lie{g}$ but $A \notin \lie{g}_a(\ad{Y})$, then $A = \sum A_b$ with $A_b \in \lie{g}_b(\ad{Y})$.)
Now we show another.
Again, \textit{let $\phi : \lie{g} \to \lie{g}'$ be an epimorphism and $\sigma' \in E(\lie{g}')$.
Then there is $\sigma \in E(\lie{g})$ such that $\sigma' \circ \phi = \phi \circ \sigma$.}
To prove this it is sufficient to consider the case for $\sigma' = \exp{\ad{X'}}$ for some $X \in \mathcal{N}(\lie{g}')$.
The proceeding lemma says that there is $X \in \lie{g}$ such that $X' = \phi(X)$.
Note that for $A \in \lie{g}$ $((\ad{X'}) \circ \phi)(A) = [\phi(X), \phi(A)] = \phi([X, A]) = (\phi \circ (\ad{A}))(A)$.
It implies that $(\exp{\ad{X'}}) \circ \phi = \phi \circ (\exp{\ad{X}})$, which makes all done.

\newpage

\textbf{Conjugacy of CSA's (solvable case)}

Now we prove the conjugacy theorem.

The first step is to show this for the solvable case; \textit{let $\lie{g}$ be a finite-dimensional solvable Lie algebra and $\lie{h}_1$ and $\lie{h}_2$ CSA's of $\lie{g}$.
Then there is $\sigma \in E(\lie{g})$ such that $\lie{h}_2 = \sigma(\lie{h}_1)$.}
To prove this we assume that the statement holds when $\dim{\lie{g}} < n$ for some $n \in \NaN$ and $\lie{g}$ is not nilpotent (if nilpotent, the statement is trivial), and let $\dim{\lie{g}} = n$.
As usual, we choose a non-zero proper ideal $\lie{k}$ which is solvable, however for later we assume that $\lie{k}$ is abelian and as minimal as possible, which, as we know, is always possible .
Of course, $\lie{g}' = \lie{g}/\lie{k}$ is solvable and of dimension less than $n$.
Also, if we let $\phi : \lie{g} \to \lie{g}'$ be the canonical projection, as seen, all $\lie{h}'_i = \phi(\lie{h}_i)$ are some of CSA's of $\lie{g}'$.
Then by the assumption there is $\sigma' \in E(\lie{g})$ such that $\phi(\lie{h}_2) = \sigma'(\phi(\lie{h}_1))$.
Since $\phi$ is an epimorphism, as seen, there is $\sigma \in E(\lie{g})$ such that $\sigma' \circ \phi = \phi \circ \sigma$.
On the other hand, we saw also that if we let $\lie{g}_i = \phi^{-1}(\lie{h}_i)$, then for each $i$ $\lie{h}_i$ is a CSA of $\lie{g}_i$.
It is immediate that $\lie{g}_2 = \sigma(\lie{g}_1)$, hence $\sigma(\lie{h}_1)$ is in $\lie{g}_2$.
If $\dim{\lie{g}_2} < n$, by the hyphothesis, we can find $\sigma' \in E(\lie{g}_2; \lie{g})$ such that $\sigma'(\sigma(\lie{h}_1)) = \lie{h}_2$.
Therefore, in this case our proof is over.

Our argument is so natural.
However, we did not check the case for $\dim{\lie{g}_2} = n$, or $\lie{g}_2 = \lie{g}_1 = \lie{g}$, and treating this case is the hardest part of this proof.
In other words, we must construct an automorphism in $E(\lie{g})$ explicitly.
Note that in this case $\lie{g} = \lie{h}_1 + \lie{k} = \lie{h}_2 + \lie{k}$.
We let $\lie{h}_1 = \lie{g}_0(\ad{X})$ for $X \in \lie{g}$.
If one finds $Z \in \lie{g}$ such that $Y = (\exp{\ad{Z}})(X) \in \lie{h}_2$, then $\lie{g}_0(\ad{Y}) = (\exp{\ad{Z}})(\lie{h_1})$ is a CSA containing $\lie{h}_2$, so by the minimality $\lie{h}_2 = (\exp{\ad{Z}})(\lie{h}_1)$.
From this what we expect is that we can find $Z \in \lie{k}$ such that $\exp{\ad{Z}} \in E(\lie{g})$ and $(\exp{\ad{Z}})(X) \in \lie{h}_2$.
We wish that $Z$ is a linear combination of elements in $\mathcal{N}(\lie{g})$ which commute all, and we shall show that we can even go further.
Since $\lie{k}$ is an ideal, $\ad{X}$ leaves $\lie{k}$ invariant, so we can write $\lie{k} = \lie{k}_0(\ad{X}) \oplus \lie{k}_*(\ad{X})$, where $\lie{k}_*(\ad{X}) = \bigoplus_{0 \ne a \in k} \lie{k}_a(\ad{X})$.
Suppose that $\lie{k}_0(\ad{X}) \ne 0$.
It is immediate that $\lie{k}_0(\ad{X})$ is an Abelian ideal so that $\lie{k} = \lie{k}_0(\ad{X})$ by the minimality of $\lie{k}$, but then $\lie{g}$ becomes nilpotent, and this is not our case.
Hence we have that $\lie{k} = \lie{k}_*(\ad{X}) = \lie{g}_*(\ad{X})$.
This yields a quite striking result.
Note that for any $A \in \lie{k}$ $A$ is a linear combination of $A_a$'s with $A_a \in \lie{g}_a(\ad{X}) \subseteq \lie{k}$ and $A_a$'s commute, so we obtain that $\exp{\ad{A}} \in E(\lie{g})$ for any $A \in \lie{k}$.

The remaining is to find $Z \in \lie{k}$ such that $Y = (\exp{\ad{Z}})(X) \in \lie{h}_2$.
Note that we can write $X = Y + Z'$, where $Y \in \lie{h}_2$ and $Z' \in \lie{k}$.
Also note that since $\lie{k} = \lie{g}_*(\ad{X})$, $(\ad{X})|_\lie{k}$ is invertible, so we can find $Z \in \lie{k}$ such that $Z' = (\ad{X})(Z) = [X, Z]$.
Since $\lie{k}$ is also an ideal, $(\ad{Z})^2 = 0$ so that $(\exp{\ad{Z}})X = X + [Z, X] = X - Z' = Y \in \lie{h}_2$, which finishes our proof.

\newpage

\textbf{Borel subalgebra}

($\lie{g}$ is a finite-dimensional Lie algebra.)
Before treating the general case we need a new tool: \textbf{Borel subalgebra}, a maximal solvable subalgebra.
To use this we need some lemmas.
The first is that \textit{if $\lie{b} \le \lie{g}$ is a Borel algebra, then $\lie{n}_\lie{g}(\lie{b}) = \lie{b}$}; it is easy because for any $X \in \lie{n}_\lie{g}(\lie{b})$, since $[\lie{b} + kX, \lie{b} + kX] \subseteq \lie{b}$ so that $\lie{b} + kX$ is solvable, hence equal to $\lie{b}$ due to the maximality.
Another lemma is that \textit{if the radical $\lie{r}$ of $\lie{g}$ is not equal to $\lie{g}$, then there is one-to-one correspondence of Borel subalgebras to Borel subalgebras of the subalgebra $\lie{g}/\lie{r}$.}
Obviously, for $\lie{b} \le \lie{g}$ if $\lie{b}/\lie{r}$ is a maximal solvable subalgebra, then $\lie{b}$ is a maximal solvable subalgebra.
On the other hand, let $\lie{b}$ be a Borel subalgebra.
Then since $\lie{b} + \lie{r}$ is a solvable subalgebra, due to the maximality, $\lie{r} \subseteq \lie{b}$, and the lemma is immediate by this since in this case ($\lie{r} \le \lie{b}$) the (disjoint) union of $\lie{b}/\lie{r}$ is exactly $\lie{b}$.

The last lemma (and the second functional lemmas of Cartan subalgebras and $E(\lie{g})$) connects the conjugacy of Cartan subalgebras in arbitrary Lie algebras with that in semisimple Lie algebras.
We, however, postpone discussing this and focus on Borel subalgebras of $\lie{g}$ when $\lie{g}$ is \textit{semisimple}.
Let $\lie{h} \le \lie{g}$ be a CSA and $\Delta$ a base of a root system $\Phi \subset \lie{h}^*$, and let $\lie{n}(\Delta) = \bigoplus_{\alpha \succ 0} \lie{g}_\alpha$.
It is obvious that $\lie{n}(\Delta)$ is nilpotent.
Now we let $\lie{b}(\Delta) = \lie{h} \oplus \lie{n}(\Delta)$.
We have that $[\lie{b}(\Delta), \lie{b}(\Delta)] \subseteq \lie{n}(\Delta)$.
Thus $[\lie{b}(\Delta), \lie{b}(\Delta)]$ is nilpotent, so we have that $\lie{b}(\Delta)$ is solvable.
Furthermore, it is immediate that any subalgebra containing $\lie{b}(\Delta)$ but not equal to $\lie{b}(\Delta)$ contains $\lie{g}_{-\alpha}$ for some root $\alpha \succ 0$, so it contains $\lie{s}_\alpha$, which implies the failure of solvability.
Hence we obtain that \textit{$\lie{b}(\Delta)$ is a Borel subalgebra} (we call such Borel subalgebra \textbf{standard} relative to $\lie{h}$).
We can go further.
Let $\lie{b}(\Delta')$ ($\Delta'$ another base) be another standard Borel subalgebra relative to $\lie{h}$.
Since there is $\sigma$ in the Weyl group such that $\Delta' = \sigma(\Delta)$ and we have $\tau_i = (\exp{\ad{E_i}}) (\exp{(-\ad{F_i})}) (\exp{\ad{E_i}})$ and $E_i$ and $F_i$ are evidently in $\mathcal{N}(\lie{g})$, we have that $\lie{b}(\Delta')$ is conjugate under $E(\lie{g})$.
Thus, \textit{every standard Borel subalgebras relative to $\lie{h}$ are conjugate under $E(\lie{g})$.}

One more thing:
Again, \textit{let $\lie{b}$ be a Borel subalgebra of $\lie{g}$ semisimple.
Then the semisimple part $A_S$ and the nilpotent part $A_N$ of any $A \in \lie{b}$ are in $\lie{b}$}; it is immediate that $(\ad{A_S})\lie{b} \subseteq \lie{b}$, since $\ad{A_S}$ is a polynomial of $\ad{A}$, so that $[\lie{b} + kA_S, \lie{b} + kA_S] \subseteq \lie{b}$ and that $\lie{b}$ is maximal.

\newpage

\textbf{Conjugacy theorem (1)}

Now we prove the conjugacy theorem.
We will see that if we prove that \textit{for any finite-dimensional Lie algebra $\lie{g}$ every Borel subalgebras of $\lie{g}$ are conjugate under $E(\lie{g})$}, then we have the general conjugacy theorem.
To do this we assume that the statement holds when $\dim{\lie{g}} < n$ for some $n \in \NaN$, and let $\dim{\lie{g}} = n$.
If $\lie{g}$ is not semisimple, that is, the radical $\lie{r}$ of $\lie{g}$ is not zero, the correspondence of Borel subalgebras of $\lie{g}$ to that of $\lie{g}/\lie{r}$ and the functional properties and the induction hypothesis implies the conjugacy.
Thus we assume that $\lie{g}$ is semisimple.
Let $\lie{b}_1$ and $\lie{b}_2$ be Borel subalgebras of $\lie{g}$.
We restrict $\lie{b}_1$ be $\lie{b}(\Delta)$ for a CSA $\lie{h}$ and a base $\Delta$ of the root system $\Phi \subset \lie{h}^*$; after finishing discussion with this, the general case holds automatically.
We first show that $\lie{b}_1 \cap \lie{b}_2 = 0$ cannot be hold.
We know that $\dim{\lie{b}_1} > (1/2)\dim{\lie{g}}$.
Thus we have that $\dim{\lie{b}_2} \le (1/2)\dim{\lie{g}}$.
Now we let $\lie{t} \subseteq \lie{b}_2$ be a maximal toral subalgebra.
If $\lie{t} = 0$, then, since we know that the semisimple part of any element in $\lie{b}_2$ is in $\lie{b}_2$, $\lie{b}_2$ consists of nilpotent elements, so it is nilpotent, but then it is a CSA or maximal toral subalgebra of $\lie{g}$, a contradiction, hence $\lie{t} \ne 0$.
Let $\lie{h} \le \lie{g}$ be a maximal toral subalgebra containing $\lie{t}$ and $\lie{b}'$ be a standard Borel subalgebra relative to $\lie{h}$.
Then $\lie{b}_2 \cap \lie{b}' \ne 0$.
We will see that in this case ($\lie{b}_1 \cap \lie{b}_2 \ne 0$) these Borel subalgebras are conjugate (under $E(\lie{g})$), thus $\dim{\lie{b}_2} = \dim{\lie{b}_1} > (1/2)\dim{\lie{g}}$, a contradiction.
Therefore, if we prove the conjugacy when $\lie{b}_1 \cap \lie{b}_2 \ne 0$, the proof is over.

Assume that $\lie{a} = \lie{b}_1 \cap \lie{b}_2 \ne 0$.
In this proof we use a double induction; let $n' = \dim{\lie{b}_1}$ and we assume that the statement (with the induction hypothesis on $n$) holds when $n' - \dim{\lie{a}} < m$ for some $m \in \NaN$, and let $n' - \dim{\lie{a}} = m$.
(In the case for $m = 0$ is obviously; then $\lie{b}_1 = \lie{b}_2$.)
This configuration then indicates our direction that we find $\sigma \in E(\lie{g})$ such that $\dim{(\lie{b}_1 \cap \sigma(\lie{b}_2))} > n' - m$ .
We will treat several cases and use same strategy as following.
If we can find another Borel subalgebra $\lie{b}'$ which is conjugate with $\lie{b}_2$ under $E(\lie{g})$ and satisfies that $\dim{(\lie{b}_1 \cap \lie{b}')} > \dim{\lie{a}}$ or is a standard BSA (must be standard!) conjugate with $\lie{b}_1$ under $E(\lie{g})$ and $\dim{(\lie{b}_2 \cap \lie{b}')} > \dim{\lie{a}}$, by the second induction, we have the conjugacy of $\lie{b}_1$ and $\lie{b}_2$ under $E(\lie{g})$.
Hence finding such $\lie{b}'$ is our aim.

We divide the proof in two cases: whether $\lie{a}$ contains a non-zero nilpotent element or not.
We assume the first case.
Let $\lie{n}$ be the set of nilpotent elements of $\lie{a}$.
The standard-ness of $\lie{b}$ says that $\lie{n}$ is a subspace and for any $N \in \lie{n}$ and $X \in \lie{a}$ $[N, X]$ is nilpotent.
Thus $\lie{n} \trianglelefteq \lie{a}$, but $\lie{n} \not\trianglelefteq \lie{g}$, obviously, so $\lie{k} = \lie{n}_\lie{g}(\lie{n})$ is a proper subalgebra of $\lie{g}$.
Obviously, $\dim{\lie{k}} < n$.
To use this configuration , for each $i$ let $\lie{b}'_1 = \lie{b}_1 \cap \lie{k}$, which is obviously solvable, and $\lie{t}_i$ be a Borel subalgebra of $\lie{k}$ containing $\lie{b}'_i$.
By the first induction there is $\tau \in E(\lie{g}; \lie{k})$ such that $\tau(\lie{t}_1) = \lie{t}_2$.
Now let $\lie{b}'$ be a BSA of $\lie{g}$ containing $\lie{t}_2$.
If $\lie{a} \subsetneq \lie{b}'_1$ and $\lie{b}'_2$, then $\dim{(\tau(\lie{b}_1) \cap \lie{b}')} = \dim{\lie{t}_1} > \dim{\lie{a}}$, so by the second induction $\lie{b}_1$ and $\lie{b}'$ are conjugate under $E(\lie{g})$, $\lie{b}'$ being standard, and $\dim{(\lie{b}' \cap \lie{b}_2)} > \dim{\lie{a}}$.

Now we show that $\lie{a} \subsetneq \lie{t}_i$.
To do this we see that $\adu{\lie{b}_i/\lie{a}}{\lie{n}} \le \lie{gl}(\lie{b}_i/\lie{a})$ is nilpotent, so by the main part of Engel's theorem there is $Y \in \lie{b}_i$ with $Y \notin \lie{a}$ such that for all $N \in \lie{n}$ $[N, Y] \in \lie{a}$.
On the other hand, since $[N, Y] \in [\lie{b}_i, \lie{b}_i]$, due to Lie's theorem, $[N, Y]$ is nilpotent, so $[N, Y] \in \lie{n}$ for all $N \in \lie{n}$.
Thus $Y \in \lie{k}$ so that $Y \in \lie{t}_i$ but $Y \notin \lie{a}$.
Hence, $\lie{t}_i$ contains $\lie{a}$ properly.
Finally, we can find all $\lie{b}'_i$ so that $\lie{b}_i$ are conjugate under $E(\lie{g})$.

\newpage

\textbf{Conjugacy theorem (2)}

The remaining is the case for $\lie{a} = \lie{b}_1 \cap \lie{b}_2 \ne 0$ and it does not contain any non-zero nilpotent element.
Since for each $i$, as we know, the nilpotent part of any element in $\lie{b}_i$ is in $\lie{b}_i$, $\lie{a}$ is a toral subalgebra.
We expect that $\lie{a} \subseteq \lie{h}$.
But in general it does not hold.
Nevertheless, we first assume that $\lie{a} \subseteq \lie{h}$ and show the conjugacy, and we will treat later the general case.
Assume also that $\lie{a} = \lie{h}$.
Then since $\lie{h} \subsetneq \lie{b}_2$ there is $\alpha \in \Phi^+(\Delta)$ such that $E_{-\alpha} \in \lie{b}_2$.
But we know that a prouct $\tau$ of some $\tau_i$'s, which is in $E(\lie{g})$, sends $E_{-\alpha} (\notin \lie{b}_1)$ to $E_\alpha \in \lie{b}_1$, so by the second induction the conjugacy of $\lie{b}_1$ with $\tau(\lie{b}_2)$, hence with $\lie{b}_2$ is obtained.

We assume that $\lie{a} \subsetneq \lie{h}$.
We hire another assumption $\lie{b}_2 \subseteq \lie{p} = \lie{c}_\lie{g}(\lie{a})$.
Since $\lie{h} \subseteq \lie{p}$ and $\lie{h}$ is nilpotent, there is a Borel subalgebra $\lie{b}'$ of $\lie{p}$.
Also, $\lie{b}_2$ is a Borel subalgebra of $\lie{p}$.
Since $\dim{\lie{p}} < n$ obviously ($\lie{p}$ has non-zero center), by the first induction, we find $\sigma \in E(\lie{g}; \lie{p}) \subseteq E(\lie{g})$ with $\lie{b}' = \sigma(\lie{b}_2)$.
Thus $\lie{b}'$ is a Borel subalgebra of $\lie{g}$.
Now since $\lie{b}_1 \cap \lie{b}' \supseteq \lie{h} \supsetneq \lie{a}$ we can apply the second induction, and this case is done.

The case $\lie{b}_2 \not\subseteq \lie{p}$ seems hopeless, but actually the proof for this is quite pretty, since in this case we can find $X \in \lie{b}_2$ such that $(\ad{\lie{a}})X$ does not vanish so that there is $T \in \lie{a}$ such that $[T, X] = aX$ for some $0 \le a \in \RaQ$.
Then $\Psi = \{\alpha \in \Phi \SBar 0 \le \alpha(T) \in \RaQ\}$ is not empty.
Let $\lie{b}'$ be a Borel subalgebra of $\lie{g}$ containing $\lie{h} \oplus \bigoplus_{\alpha \in \Psi} \lie{g}_\alpha$ which is obviously a solvable subalgebra.
Note that $(\lie{a} \not\ni) T \in \lie{b}_1 \cap \lie{b}'$ and $(\lie{a} \subsetneq) \lie{h} \subseteq \lie{b}' \cap \lie{b}_2$ so that $\dim{(\lie{b}_1 \cap \lie{b}')} > \dim{(\lie{b}_1 \cap \lie{b}_2)}$ and $\dim{(\lie{b'} \cap \lie{b}_2)} > \dim{(\lie{b}_1 \cap \lie{b}_2)}$.
Thus by the second induction we have that $\lie{b}_1$ and $\lie{b}_2$ are conjugate under $E(\lie{g})$.

Finally, we meet the case for $\lie{a} \not\subseteq \lie{h}$.
We shall see that actually this case is irrelevant.
It is time to use the conjugacy of CSA in solvable case.
What we need is a CSA of solvable $\lie{b}_1$ containing $\lie{a}$ and we will use the conjugacy of $\lie{h}$ and the CSA we find.
To see a motivation to find a CSA we let $\lie{h}'$ be a nilpotent subalgebra of $\lie{b}_1$ containing $\lie{a}$ and $X \in \lie{n}_{\lie{b}_1}(\lie{h}')$.
We have that for any $A \in \lie{a}$ $[A, X] \in \lie{h}'$.
Due to the nilpotency of $\lie{h}'$ there is $r \in \NaN$ such that $(\ad{A})^r X = 0$.
But $\ad{A}$ is semisimple so that $[A, X] = 0$.
Thus $\lie{n}_{\lie{b}_1}(\lie{h}') \subseteq \lie{c}_{\lie{b}_1}(\lie{a})$.
Now we have our way: the CSA $\lie{h}'$ containing $\lie{a}$, which we seek, must be a CSA of $\lie{c}_{\lie{b}_1}(\lie{a})$.
Strikingly, every CSA of $\lie{c}_{\lie{b}_1}(\lie{a})$ contains $\lie{a}$ since $[\lie{a}, \lie{h}'] = 0 \subseteq \lie{h}'$.
Thus we found another CSA $\lie{h}'$ of $\lie{b}_1$ containing $\lie{a}$.
By the conjugacy of Cartan subalgebras in solvable Lie algebra there is $\sigma \in E(\lie{g}; \lie{b}_1) \subseteq E(\lie{g})$ such that $\lie{h}' = \sigma(\lie{h})$.
Then replacing $\lie{b}_1$ and $\lie{h}$ by their image of $\sigma$ we can say that $\lie{a} \subseteq \lie{h}$.

We have done the proof for conjugacy of Borel subalgebras of $\lie{g}$ under $E(\lie{g})$.
Finally, we can proof that \textit{every Cartan subalgebras of any finite-dimensional Lie algebra $\lie{g}$ are conjugate under $E(\lie{g})$.}
Let $\lie{h}_1$ and $\lie{h}_2$ be Cartan subalgebras of $\lie{g}$.
Since $\lie{h}_1$ are solvable, for each $i$ there is a Borel subalgebra $\lie{b}_i$ containing $\lie{h}_i$.
We have seen that there is $\sigma \in E(\lie{g})$ such that $\lie{b}_2 = \sigma(\lie{b}_1)$.
Then the isomprphic image $\sigma(\lie{h}_1)$ and $\lie{h}_2$ are CSAs of $\lie{b}_2$, due to the conjugacy of CSA's in solvable Lie algebra our proof for conjugacy of Cartan subalgebras is finished.

This result yields a new algebraic invariance of Lie algebras: The dimension of Cartan subalgebra.
We call this invariance \textbf{rank} of a Lie algebra.

\newpage

\textbf{Note on completion of classification}

We have found all finite-dimensional (semi)simple Lie algebras.
But a sensitive reader may ask: Is there any duplicate of another in our list?
Also, actually, there is a somewhat serious gap; when we pick up two maximal toral subalgebra of the semisimple Lie algebra and construct root systems, we do not have any guarantee that these root systems are isomorphic.
These questions can be read as whether any two finite-dimensional simple Lie algebra of different types are isomorphic or not.
(N. B., $B_1$, $C_1$, $C_2$, $D_1$, $D_2$, $D_3$ are not in our list.)
To answer this, first, we compare the dimensions of them.
We obtained the cardinalities of all root systems.
Since $\dim{\lie{g}_\alpha} = 1$ for any root $\alpha$, the dimension is the cardinality of the root system plus the rank $l$.
This is the list of the dimensions of finite-dimensional simple Lie algebras in each types:

\begin{eqnarray*}
  &\;& A_l \;:\; l(l + 2), \;\;\; B_l \;:\; l(2l + 1), \;\;\; C_l \;:\; l(2l + 1), \;\;\; E_l \;:\; l(2l - 1), \\
  &\;& E_6 \;:\; 78, \;\;\; E_7 \;:\; 133, \;\;\; E_8 \;:\; 248, \;\;\; F_4 \;:\; 52, \;\;\; G_2 \;:\; 14.
\end{eqnarray*}

It is not helpful since we have a pair such that $A_{10}$ and $D_8$, of which the simple Lie algebras have the dimension 120.
In this moment we use the conjugacy of CSA.
Note that by the conjugation theorem if any two types are isomophic, then they have same rank.
Thus, most of pairs of finite-dimensional simple Lie algebras of different types, especially $A_{10}$ and $D_8$, are not isomorphism.
But we still have pairs of types with same dimension and same rank: $B_l$ and $C_l$, $B_6$ and $E_6$, $C_6$ and $E_6$.
Actually, from the results in automorphism theory and representation theory, these pairs are not isomorphic.
By automorphism theory, especially about graph automorphism, one can see that any of simple Lie algebra of type $E_6$ cannot be isomorphic to that of neither type $B_l$ nor type $C_l$.
If Lie algebras in these pair are isomorphic, the dimensions of smallest (non-trivial) irreducible modules must be same.
We will see that (Weyl's formula) these dimensions for $B_l, C_l$ are $2l + 1$, $2l$, respectively.
Therefore, we have proved that \textit{any two finite-dimensional simple Lie algebra of different types are not isomorphic}, if we assume the Weyl's formula.
The Weyl's formula will be proved when we treat the representation theory.

\newpage

\textbf{Generic element, A lemma for fixed elements by an automorphism}

To complete our analysis of $\textrm{Aut}(\lie{g})$ we need a peculiar lemma.
Let $\sigma \textrm{Aut}(\lie{g})$ and define $\lie{i}_\sigma = \{X \in \lie{g} \SBar \sigma(X) = X\}$.
It is obviously a subalgebra of $\lie{g}$.
What we shall show is \textit{for any $\sigma \in \textrm{Int}(\lie{g})$ $\dim{\lie{i}_\sigma}$ is bigger than the rank of $\lie{g}$.}
First, we prove this for the simplest case; $\sigma = \exp{\ad{Z}}$ for a nilpotent $Z \in \lie{g}$.
If one can show that $\dim{\ker{(\ad{Z})}} \ge l$, $l$ the rank, then $\dim{\lie{i}_\sigma} \ge l$, obviously.
To show this we introduce the \textbf{generic element}\footnote{The following is from Jacobson, Lie Algebras, Dover.} $X(t) = \sum t_i U_i$, where $(U_i)$ is a basis of $\lie{g}$ and $t_i$'s are indeterminants, and the \textbf{characteristic polynomial of Lie algebra $\lie{g}$}, which is the characteristic polynomial $m_{X(t)}(s)$ of $\ad{X}$, which must be calculated in the field extension $\lie{g}_{k(t)}$ of $\lie{g}$ into $k(t)$.
We can write $m_{X(t)}(s) = s^n + f_1(t) s^{n - 1} + f_2(t) s^{n - 2} + \cdots + f_m(t) s^{n - m}$, where $f_i$ are polynomials of $t_i$'s, which is homogeneous of degree $i$.
Now evaluating $t_i$ to $a_i$ arbitrarily, for any $A \in \lie{g}$ we obtain all characteristic polynomial of $\ad{A}$.
This implies that the minimal $i$ of which $f_i$ is non-zero must be $n - l$, where $l$ is the dimension of a minimal Engel subalgebra, or the rank of $\lie{g}$.
Since the rank of a Lie algebra is determined only by the algebra-structure, i.e., what $[U_i, U_j]$ is, $\lie{g}$ and $\lie{g}_{k(t)}$ has same rank.
Thus, $(\lie{g}_{k(t)})_0(\ad{X(t)})$ is a minimal Engel subalgebra, or a CSA.
In the same reason for rank, $\lie{g}$ is (semi)simple if and only if $\lie{g}_{k(t)}$ is (semi)simple.
It implies that $(\lie{g}_{k(t)})_0(\ad{X(t)})$ is a maximal toral subalgebra, so $(\lie{g}_{k(t)})_0(\ad{X(t)}) = \ker{\ad{X(t)}}$.
Thus $\dim{(\ad{X(t)}) \lie{g}_{k(t)}} = n - l$.
This property now induces an important result.
For any $A = \sum a_i U_i \in \lie{g}$, $(\ad{A}) \lie{g}$ is the image of evaluation homomorphism $t_i \mapsto a_i$.
Hence $\dim{(\ad{A}) \lie{g}} \le \dim{(\ad{X(t)}) \lie{g}_{k(t)}} = n - l$.
Therefore, \textit{$\dim{\ker{\ad{A}}} \ge l$ for every $A \in \lie{g}$.}

The result says that $\dim{\lie{i}_\sigma} \ge l$ when $\sigma = \exp{\ad{Z}}$.
Now we handle the general case: $\sigma = \exp{\ad{X_1}} \exp{\ad{X_2}} \cdots \exp{\ad{X_r}}$ with all $\ad{X_i}$ nilpotent.
We shall use $\sigma(t) = \exp{\ad{Z(t)}} = \exp{\ad{t_1 X_1}} \exp{\ad{t_2 X_2}} \cdots \exp{\ad{t_r X_r}}$, where $Z(t) \in \lie{g}_K$, $K$ the field of quotients of $k[[t_1, t_2, \cdots, t_r]]$.
Since $\lie{g}_K$ is semisimple with rank $l$, $\dim{\ker{\ad{Z(t)}}} \ge l$, so $\dim{\lie{i}_{\sigma(t)}} \ge l$, or $\dim{(\lie{i}_{\sigma(t)} - 1) \lie{g}_K} \le n - l$.
From this and the evaluation homomorphism $t_i \mapsto 1$ we would say that $\dim{(\lie{i}_{\sigma} - 1) \lie{g}} \le \dim{(\lie{i}_{\sigma(t)} - 1) \lie{g}_K} \le n - l$.
One might guess that the evaluation may not work since $\exp{\ad{Z(t)}}$ is not well-defined for all evaluation.
However, fortunately, the evaluation $t_i \mapsto 1$ does work since its result is just $\exp{\ad{X_1}} \exp{\ad{X_2}} \cdots \exp{\ad{X_r}}$.
Finally, we have that $\dim{\lie{i}_\sigma} \ge l$.
It will contribute centrally to the study of structure of automorphisms.

\newpage

\textbf{Structure of Automorphisms of Semisimple Lie Algebras}

Now we focus on the automorphism group of finite-dimensional semisimple Lie algebra.
Let $\sigma \in \textrm{Aut}(\lie{g})$ and $\lie{h}$ be a CSA.
Then $\sigma(\lie{h})$ is also a CSA.
By the conjugacy, there is $\sigma_1^{-1} \in E(\lie{g})$ such that $\lie{h} = \sigma_1^{-1} (\sigma(\lie{h}))$.
For this automorphism it preserves the root system, of course, but the base would be changed.
However, due to $\tau_i$'s and the conjugacy of bases under the Weyl group there is $\sigma_2^{-1} \in E(\lie{g})$ such that $\sigma_2^{-1} \circ \sigma_1^{-1} \circ \sigma$ sends $\lie{h}$ onto $\lie{h}$ and preserves the base.

It is not over since there is a peculiar type of automorphisms.
For a base $\Delta = \{\alpha_1, \cdots, \alpha_l\}$, if there is a (non-trivial) permutation $\tau : \NaN_l \to \NaN_l$ such that $\langle \alpha_{\tau(i)}, \alpha_{\tau(j)} \rangle = \langle \alpha_i, \alpha_j \rangle$, then the linear mapping $\varphi_\tau$ of $\lie{h}^*$ by $\alpha_i \mapsto \alpha_{\tau(i)}$ preserves the base.
This mapping implies an automorphism of $\lie{g}$ sending $E_{\alpha_i}$ to $E_{\alpha_{\tau(i)}}$, which is then preserves $\lie{h}$ and the base.
We call an automorphism of this type a \textbf{graph automorphism}, and denote the set (subgroup) of graph automorphisms by $\Gamma(\lie{g})$.
(The naming is in this sense: it preserves actually the shape of the corresponding Dynkin diagram.)
Thus we can say that there is $\sigma_4^{-1} \in \Gamma(\lie{g})$ such that $\sigma'_3 = \sigma_4^{-1} \circ \sigma_2^{-1} \circ \sigma_1^{-1} \circ \sigma$ sends $\lie{h}$ onto $\lie{h}$ identically.
By the way, $\sigma_3 = \sigma_4 \circ \sigma'_3 \circ \sigma_4^{-1}$ also sends $\lie{h}$ onto $\lie{h}$ identically.
Thus we write $\sigma = \sigma_1 \sigma_2 \sigma_3 \sigma_4$.

Finally we shall show that \textit{any automorphism $\sigma$ such that $\sigma(H) = H$ for all $H \in \lie{h}$, such as $\sigma_3$, is in $E(\lie{g})$.}
It is sufficient to consider $\sigma$ such that for a fixed $r$ $\sigma(E_r) = cE_r$ ($\sigma(F_r) = (1/c)F_r$) and $\sigma(E_j) = E_j$ ($\sigma(F_j) = F_j$) for all $j \ne r$.
It is immediate that such automorphism must send $\lie{g}_\alpha$ into $\lie{g}_\alpha$.
To show the lemma we define $\tau_i(x) = (\exp{\ad{xF_i}}) (\exp{\ad{(-x^{-1} E_i)}}) (\exp{\ad{xF_i}})$.
It is immediate that $\tau_i(x)E_i = (2/x^2)(-2F_i)$ and $\tau_i(x)(-H_i) = H_i$ and $\tau_i(x)(-2F_i) = (x^2/2) E_i$.
If we define $\omega_i(x) = \tau_i(x) \tau_i(1)$, then $\omega_i(x)E_i = x^2 E_i$ and $\omega_i(x)F_i = (1/x)^2 F_i$ and $\omega_i(x)H = H$ for all $H \in \lie{h}$.
Also if for $j$ with $c_{ij} = 0$, where $(c_{ij})$ is the Cartan matrix, it is obvious that $\omega_i(x)E_j = E_j = x^0 E_j$ and $\omega_i(x)F_j = (1/x)^0 F_i$.
From this one might expect that $\omega_i(x)E_j = x^{c_{ij}} E_j$ and $\omega_i(x)F_j = (1/x)^{c_{ij}} F_j$.
This can be shown by simple calculation with the matrix form of $\tau_i(x)$ relative to $(E_j, (\ad{E_i})E_j, \cdots, (\ad{E_i})^{-c_{ij}} E_j)$ and that relative to $(F_j, (\ad{F_i})F_j, \cdots, (\ad{F_i})^{-c_{ij}} F_j)$.
Although $\omega_i(x)$ is not what we want, we can construct $\sigma$ by them.
Let $\sigma' = (\omega_1(x))^{d_1} (\omega_2(x))^{d_2} \cdots (\omega_l(x))^{d_l}$ with $d_i \in \InZ$.
Then $\sigma'(E_j) = x^{\sum c_{ij} d_i} E_j$ and $\sigma'(F_j) = (1/x)^{\sum c_{ij} d_i} F_j$.
Since $(c_{ij})$ is invertible and all of $c_{ij}$ is an integer, there is a set $(d_i)$ such that $\sum c_{ij} d_i = d \delta_{jr}$ where $d$ is the determinant of $(c_{ij})$, which is an integer.
With this we have that $\sigma'(E_r) = x^d E_r$ and $\sigma'(F_r) = (1/x^d) F_r$ and $\sigma'(E_j) = E_j$, $\sigma'(F_j) = F_j$ for $j \ne r$.
Now we set $x = c^{1/d}$.\footnote{There is a strong dependency of algebraic closedness.
For example, if the base field is $\ReR$ and $\lie{g}$ is $\lie{sl}(2)$, where in this case $d = l + 1 = 2$, an automorphism defined as $H \mapsto H$, $E \mapsto -E$, $F \mapsto -F$ is not in $E(\lie{g})$.}
Then finally we have that $\sigma' = \sigma$, so $\sigma \in E(\lie{g})$.

Thus we obtain that $\textrm{Aut}(\lie{g}) = E(\lie{g}) \cdot \Gamma(\lie{g})$.
But we still have something more to be investigated.
We will treat it.

\newpage

\textbf{Graph automorphisms}

We introduced graph automorphisms and $\Gamma(\lie{g})$, which corresponds to permutations in a base preserving the Cartan matrix, or equivalently, preserving the Dynkin diagram.
Thus $\Gamma(\lie{g})$ is not trivial if and only if the Dynkin diagram has a symmetry.
It implies that, in the case for simple $\lie{g}$, almost all $\Gamma(\lie{g})$ is trivial except for $A_l$, $D_l$, $E_6$ and for these exception $\Gamma(\lie{g}) \cong \InZ/2\InZ$, except for $D_4$; in the case for $D_4$ $\Gamma(\lie{g}) \cong \mathcal{S}_3$.
(See their Dynkin diagram.)

Now to complete the investigation of the structure of $\textrm{Aut}(\lie{g})$, we have to show that $\textrm{Int}(\lie{g}) \cap \Gamma(\lie{g}) = \{1\}$.
To show this we shall show that \textit{if $\sigma \in \textrm{Int}(\lie{g})$ preserves $\lie{h}$ and satisfies that $(\sigma|_\lie{h})^* \alpha = \alpha$ for any $\alpha$ in a given base $\Delta$, then for any $H \in \lie{h}$ $\sigma(H) = H$} (thus $\sigma \in E(\lie{g})$).
First, we need a simple lemma: for an automorphism $\tau$ such that $\tau|_\lie{h} = 1$ if we denote $\tau(E_i) = a_i E_i$ and $\alpha = \sum \alpha_{i_j}$ is a positive root, then $\tau(E_{\pm \alpha}) = \left( \prod a_{i_j}^{\pm 1} \right) E_{\pm \alpha}$, which is immediate.
Now let $\mu = (\sigma|_\lie{h})^*$.
We know that $\mu$ is a permutation of $\Phi^+(\Delta)$.
To describe this permutation we denote all elements in $\Phi^+(\Delta)$ by $\beta(r, s)$, such that $\mu(\beta(r, s)) = \beta(r, s + 1)$ if $s < l_r$ and $\mu(\beta(r, l_r)) = \beta(r, 1)$ ($l = \sum l_r$).
It implies that $\sigma(E_{\beta(r, s)}) = a_{r, s} E_{\beta(r, s)}$ ($s < l_r$) and $\sigma(E_{\beta(r, l_r)}) = a_{r, l_r} E_{\beta(r, 1)}$ for some non-zero scalar $a_{r, s}$.
Then, for any $r$, $\lie{g}_r = \sum_s \lie{g}_{\beta(r, s)}$ ($\lie{g}_{-r} = \sum_s \lie{g}_{-\beta(r, s)}$) is leaved invariant by $\sigma$.
The key is using that $\dim{\lie{i}_\sigma} \ge l$.
Due to this if we see that $\lie{i}_\sigma \subseteq \lie{h}$, then we obtain our theorem.
But in general such $\sigma$ does not satisfy this; see when $\sigma = 1$.
However, if we obtain a sufficient $\tau \in \textrm{Int}(\lie{g})$ such that $\tau(H) = H$ for all $H \in \lie{h}$ but $\lie{i}_{\tau \sigma} \subseteq \lie{h}$, then our proof is finished.

Let $\sigma' = \tau \sigma$.
Of course, there is a (non-zero) scalar $c_i$ such that $\tau(E_i) = c_i E_i$ for all $i$ and we can control them arbitrarily.
To get our aim it is sufficient to show that $\lie{i}_{\sigma'} \cap \lie{g}_{\pm r} = 0$ for any $r$ since $\lie{g} = \lie{h} \oplus \bigoplus_r (\lie{g}_r \oplus \lie{g}_{-r})$.
We shall investigate the characteristic polynomial of $\sigma'|_{\lie{g}_r}$.
It is immediate, due to the above lemma, that $\sigma'^{l_r}(E_{\beta(r, s)}) = a_{r, 1} a_{r, 2} \cdots a_{r, l_r} c_1^{b_1} c_2^{b_2} \cdots c_l^{b_l} E_{\beta(r, s)}$, where we write $\sum_s \beta(r, s) = \sum_i b_i \alpha_i$ for any $r$ and $s$.
Thus the characteristic polynomial is $t^r - a_{r, 1} \cdots a_{r, l_r} c_1^{b_1} \cdots c_l^{b_l}$.
If we choose $c_i$'s to make the constant term not equal to 1 for all $r$, which is always possible, then $t - 1$ does not divide the polynomial so that $ \lie{i}_{\sigma'} \cap \lie{g}_{\pm r} = 0$, which we want.

We have that all graph automorphism (except the identity) is not inner.
We have seen that every automorphism of $\lie{g}$ is product of an automorphism in $E(\lie{g})$ and a graph automorphism.
Thus $\textrm{Aut}(\lie{g}) = E(\lie{g}) \cdot \Gamma(\lie{g})$.
The above result implies finally that $\textrm{Aut}(\lie{g})$ is the semidirect product of $E(\lie{g})$ and $\Gamma(\lie{g})$.
It is immediate that $\textrm{Int}(\lie{g}) = E(\lie{g})$.

This result gives a striking result, but which is mentioned.
It is immediate that if there are two simple Lie algebra which are isomorphic and all one of these algebras has only inner automorphisms, then so is the other algebra.
Remind that we do not know whether a simple Lie algebra of type $E_6$ is isomorphic to that of type $B_6$ or $C_6$.
Now we know that $B_6$ and $C_6$ has no non-inner automorphism, while $E_6$ has nontrivial graph automorphism which cannot be an inner automorphism as we have seen.
This situation concludes that \textit{any of simple Lie algebras of type $E_6$ cannot be isomorphic to that of neither type $B_6$ nor type $C_6$.}

\newpage

\part{Representation theory}

\newpage

\textbf{Weight and Dominant weight}

(Again, in this part $E$ is a $l$-dimensional vector space over an ordered field $k'$ with a positive definite and symmetric bilinear form $(\cdot, \cdot)$, such as the $l$-dimensional Euclidean space, and $\Phi$ is a root system.)

We shall introduce some features which play a central role for the representation theory.
We let $\Lambda = \{\lambda \in E \SBar \langle \lambda, \alpha \rangle \in \InZ \textrm{ for all $\alpha \in \Phi$}\}$ and an element in $\Lambda$ is called a (abstract) \textbf{weight}.
This is the generalization of the case for $\lie{s}$, the three-dimensional simple Lie algebra.
We fix a base $\Delta = (\alpha_i)$.
Then $\langle \lambda, \alpha_i \rangle = (\lambda, \alpha_i^\vee)$.
If $(\lambda, \alpha_i^\vee) \in \InZ$ for all $i$, since $\Delta^\vee$ is a base of $\Phi^\vee$, then $(\lambda, \alpha^\vee) \in \InZ$, or $\langle \lambda, \alpha \rangle \in \InZ$ for all $\alpha \in \Phi$.
This implies that $\lambda \in \Lambda$ if and only if $\langle \lambda, \alpha_i \rangle \in \InZ$ for all $i$.
For a convenience we give a (partial) ordering of $E$: $\lambda \succ \mu$ if $\lambda - \mu$ is a nonnegative ($k'$-)linear combination of $\Delta$.

We also define $\Lambda^+ = \{\lambda \in \Lambda \SBar \langle \lambda, \alpha \rangle \ge 0 \textrm{ for all $\alpha \in \Phi^+(\Delta)$}\}$ (hence $\lambda \in \Lambda^+$ if and only if $\langle \lambda, \alpha_i \rangle \ge 0$ for all $i$) and we call an element in $\Lambda^+$ \textbf{dominant}.
Especially, if $\langle \lambda, \alpha_i \rangle > 0$ for all $i$, then we call $\lambda$ \textbf{strongly dominant}.
We define special dominant weights.
We let $\lambda_i \in \Lambda^+$ be such that $\langle \lambda_i, \alpha_j \rangle = \delta_{ij}$.
Since $\alpha_i$'s form a basis, all $\lambda_i$ are well-defined.
We call these weights the \textbf{fundamental dominant weights}.
It is immediate that \textit{for every $\mu \in E$ we have that $\mu = \sum_i \langle \mu, \alpha_i \rangle \lambda_i$} and also that $\sigma_i(\lambda_j) = \lambda_j - \delta_{ij} \alpha_i$.
This fact implies that $W$ leaves $\Lambda$ invariant and that for every $\lambda \in \Lambda$ and $\sigma \in W$ $\sigma(\lambda) \succ \lambda$ or $\sigma(\lambda) \prec \lambda$.
In addition, since it is immediate that $\langle \delta, \alpha_i \rangle = 1$ for all $i$, where $\delta = \frac{1}{2} \sum_{\alpha \in \Phi^+(\Delta)} \alpha$ (remember that $\sigma_i(\delta) = \delta - \alpha_i$), so that $\delta = \sum_i \lambda_i$.

In this page we also show some important lemmas.
We saw that for any $\lambda \in E$ if $\sigma(\lambda)$ is dominant then $\sigma(\lambda) \succ \lambda$ and there exists $\sigma \in W$ such that $\sigma(\lambda)$ is dominant (so all the closures of Weyl chambers cover $E$).
It is also seen that for $\lambda$ and $\mu$ in a common closure of Weyl chamber and $\sigma \in W$ if $\sigma(\lambda) = \mu$ then $\lambda = \mu$ and that the action of $W$ on the set of Weyl chambers is simple.
From these lemmas we obtain that \textit{for every $\lambda \in \Lambda$ the $W$-orbit contains only one dominant weight and if $\lambda \in \Lambda^+$ then $\lambda \succ \sigma(\lambda)$ for every $\sigma \in W$ and for a strongly dominant weight $\lambda$ $\sigma(\lambda) = \lambda$ only if $\sigma = 1$}.
Another important lemma is that \textit{for any dominant weight $\lambda$ the number of dominant weight $\mu$ such that $\lambda \succ \mu$ is finite}.
Its proof is easy.
Since $\lambda + \mu \in \Lambda^+$ and $\lambda - \mu$ is a summation of some positive roots, $0 \le (\lambda + \mu, \lambda - \mu) = (\lambda, \lambda) - (\mu, \mu)$, which implies the lemma.
There is another useful lemma: \textit{if $\lambda \in \Lambda^+$ and $\sigma \in W$ where $\lambda \ne \sigma(\lambda)$, then $(\lambda + \delta, \lambda + \delta) > (\sigma(\lambda) + \delta, \sigma(\lambda) + \delta)$.}
It can be seen easily.
See that $(\mu + \delta, \mu + \delta) = (\lambda + \sigma^{-1}(\delta), \lambda + \sigma^{-1}(\delta)) = (\lambda + \delta, \lambda + \delta) - 2(\lambda, \delta - \sigma^{-1}(\delta))$.
(Note that $(\delta, \delta) = (\sigma^{-1}(\delta), \sigma^{-1}(\delta))$.)
Since $\delta \succ \sigma^{-1}(\delta)$, $(\lambda, \delta - \sigma^{-1}(\delta)) \ge 0$ so that $(\lambda + \delta, \lambda + \delta) \ge (\sigma(\lambda) + \delta, \sigma(\lambda) + \delta)$.
If the equality holds, then $(\lambda, \delta - \sigma^{-1}(\delta)) = (\lambda, \delta) - (\lambda, \sigma^{-1}(\delta)) = (\lambda, \delta) - (\sigma(\lambda), \delta) = (\lambda - \sigma(\lambda), \delta) = 0$, which holds only if $\lambda -\sigma(\lambda) = 0$ since $\lambda \succ \sigma(\lambda)$.

\newpage

\textbf{Saturated set}

We now define \textbf{saturated set}, a subset $\Pi$ of $\Lambda^+$ such that for any $\lambda \in \Pi$ and $\alpha \in \Phi^+(\Delta)$ $\lambda - i\alpha \in \Pi$ for all $i \in \InZ$ with $0 \le i \le \langle \lambda, \alpha \rangle$.
This definition immediately implies that $\Pi$ is invariant under $W$.
Also, we day that $\Pi$ has the \textbf{highest weight $\lambda$} if $\lambda \succ \mu$ for all $\mu \in \Pi$.
If $\Pi$ has highest weight $\lambda \in \Lambda^+$, then $\Pi$ is finite since the number of dominant weight smaller than $\lambda$ is finite.

We can go further.
\textit{If $\mu \in \Lambda^+$ satisfies that $\lambda \succ \mu$, then $\mu \in \Pi$}.
To show this, we investigate elements in the form of $\mu + \sum a_i \alpha_i$ ($0 \le a_i \in \InZ$) and which $a_i$ makes this in $\Pi$.
At first, $\lambda$ is in the form and is in $\Pi$.
Now we shall show that if for some $a_i$ $\mu' = \mu + \sum a_i \alpha_i \in \Pi$, then there is $j$ such that $\mu + \sum_{i \ne j} a_i \alpha_i + (a_j - 1) \alpha_j \in \Pi$ where $a_j > 0$, which implies finally that $\mu \in \Pi$.
To show this it is necessary to show that $\langle \mu', \alpha_j \rangle > 0$ for some $j$ with $a_j > 0$.
Suppose that for any $j$ with $a_j > 0$ $(\mu', \alpha_j) \le 0$.
Then $(\mu', \sum a_j \alpha_j) \le 0$, or $\sum a_j (\mu, \alpha_j) + (\sum a_i \alpha_i, \sum a_i \alpha_i) \le 0$, which requires that if $a_j > 0$ then $(\mu, \alpha_j) < 0$, a contradiction.
Thus we obtain that $\langle \mu', \alpha_j \rangle > 0$ for some $j$ with $a_j > 0$.
(Note that $\mu' - \alpha_i$ is not necessarily dominant but it does not matter to our proof.)
Finally, we obtain that $\mu \in \Pi$.

This gives central structure of saturated set having highest weight $\lambda$; the union of $W$-orbits of all dominant weights which smaller than $\lambda$ coincides with the saturated set.
Furthermore, the converse holds: \textit{if for a $\lambda \in \Lambda^+$ we let $\Pi(\lambda)$ be the union of all $W$-orbits each of which is from $\mu \in \Lambda^+$ with $\mu \prec \lambda$, then $\Pi(\lambda)$ is a saturated set}.
To show this\footnote{the following proof is from http://math.stackexchange.com/questions/830709/proving-that-there-exists-a-saturated-set-with-given-highest-weight}, note that $\Pi(\lambda)$ can be written as $\{\mu \in \Lambda \SBar \sigma(\mu) \prec \lambda \textrm{ for all $\sigma \in W$}\}$ since one (and only one) of $\sigma \in W$ makes $\sigma(\mu)$ dominant.
Let $\mu \in \Pi$, and $\alpha \in \Delta$ and $\sigma \in W$.
It is sufficient to consider only the case for $m = \langle \mu, \alpha \rangle \ge 0$; the other case is immediate if we replace $\mu$ by $\sigma_\alpha(\mu)$.
Let $r \in \NaN$ with $1 \le r \le m$.
The aim is to show that $\mu - r\alpha \in \Pi$, or, as noted, $\lambda \succ \sigma(\mu - r\alpha)$.
This is immediate since $\sigma(\alpha) \in \Phi$ so that $\sigma(\mu) \succ \sigma(\mu) - r\sigma(\alpha) \succ \sigma(\mu) - m\sigma(\alpha)$, if $\sigma(\alpha) \succ 0$, or $\sigma(\mu) - m\sigma(\alpha) \succ \sigma(\mu) - r\sigma(\alpha) \succ \sigma(\mu)$, if $\sigma(\alpha) \prec 0$, and $\lambda \succ \sigma(\mu)$ and $\lambda \succ (\sigma \sigma_\alpha)(\mu) = \sigma(\mu) - m\sigma(\alpha)$.
Therefore, $\Pi$ is saturated.
These lemmas give us an interesting fact.
Every saturated set having highest weight is completely determined by its highest weight.
That is, every saturated set $\Pi$ having highest weight $\lambda$ actually coincides with $\Pi(\lambda)$.
We will see a central connection between finite-dimensional irreducible representations and saturated sets.

We finish this page with a lemma which is useful when we treat about the Freudenthal formula.
We are interested in comparison of $(\lambda + \delta, \lambda + \delta)$ and $(\mu + \delta, \mu + \delta)$ for $\mu \in \Pi(\lambda)$.
The last lemma in the previous page says that it is sufficient to consider only the case for $\mu \in \Lambda^+$ and $\mu \ne \lambda$.
Let us see that $(\mu + \delta, \mu + \delta) = (\lambda + \delta - (\lambda - \mu), \lambda + \delta - (\lambda - \mu)) = (\lambda + \delta, \lambda + \delta) - 2(\lambda + \delta, \lambda - \mu) + (\lambda - \mu, \lambda - \mu) = (\lambda + \delta, \lambda + \delta) - (\lambda + \mu + 2\delta, \lambda - \mu)$.
Since $\lambda + \mu + 2\delta$ is strongly dominant inevidently and $\lambda \succ \mu$, the second term is bigger than 0.
Therefore, \textit{for every $\lambda \in \Lambda^+$ and $\lambda \ne \mu \in \Pi(\lambda)$ we have that $(\lambda + \delta, \lambda + \delta) > (\mu + \delta, \mu + \delta)$.}

\newpage

\textbf{Standard cyclic modules}

Before we study about finite-dimensional irreducible modules of semisimple Lie algebras, we need some ingredients.
Note that for any finite-dimensional representation $\rho : \lie{g} \to \lie{gl}(V)$, since every element in $\lie{h}$ is always semisimple in any (faithful) representation and $\lie{h}$ is Abelian, there is a basis $v_i$ of $V$ with $\lambda_i \in \lie{h}^*$ for each $v_i$ such that $Hv_i = \lambda_i(H) v_i$ for all $H \in \lie{h}$.
We call such $\lambda_i$ a \textbf{weight} of $V$ (it is different from the above definition of 'weight' a little bit), where $V_\lambda = \{v \in V \SBar Hv = \lambda(H) v \textrm{ for all $H \in \lie{h}$}\}$ is called the \textbf{weight space} of $\lambda \in \lie{h}^*$.
Also it is immediate that for a root $\alpha$ and $v \in V_\lambda$ $\rho(E_\alpha) v \in V_{\lambda + \alpha}$.
Since we are interested in the case for $\dim{V} < \infty$, there is a weight $\lambda$ such that for any $v \in V_\lambda$ $E_\alpha v = 0$ for all positive root $\alpha$.
We call such weight the \textbf{highest weight}, and $v$ a \textbf{maximal vector} (of weight $\lambda$).

The first ingredient we introduce is a realization of a highest weight.
Let $\lambda \in \lie{h}^*$.
We assume that there is $v$ such that $Hv = \lambda(H) v$ for all $H \in \lie{h}$ and $E_i v = 0$ for all $i$ (equivalently, $E_\alpha v = 0$ for all positive root $\alpha$).
Also we assume that that $V = U(\lie{g}) \cdot v$, where $U(\lie{g}) \cdot v$ is definitely a $\lie{g}$-module and $v$ is a maximal vector with highest weight $\lambda$.
We call a such module satisfying all these assumption \textbf{standard cyclic} of (highest) weight $\lambda$.

We can see that at least one such standard cyclic module exists for any $\lambda \in \lie{h}^*$.
It can be found from $U(\lie{g})$.
Let $\mathcal{I}$ be the left ideal generated by all $E_\alpha$ for any $\alpha$ and all $H - \lambda(H) 1$ for any $H \in \lie{h}$.
We denote then $V = U(\lie{g})/\mathcal{I}$ and let $v = 1 + \mathcal{I}$.
Then $Hv = \lambda(H) v$ for all $H \in \lie{h}$ and $E_\alpha v = 0$ for all root $\alpha$.
Also, $U(\lie{g}) v = V$.
Therefore, we have a standard cyclic module $V$.
Especially, we denote this special standard cyclic module by $U(\lambda)$.

There is a useful note.
We have a basis of $\lie{g}$, $F_\alpha$ ($\alpha$ a root), $E_\alpha$ ($\alpha$ a root), $H_1, H_2, \cdots, H_l$.
From this arrangement, by PBW theorem, $U(\lie{g})$ has a basis $F_{\beta_1}^{r_1} \cdots F_{\beta_n}^{r_n} E_{\beta_1}^{s_1} \cdots E_{\beta_n}^{s_n} H_1^{m_1} \cdots H_l^{m_l}$, where $\beta_1, \cdots, \beta_n$ are all roots.
But we have that $H_i v$ is a scalar multiplication of $v$ and $E_\alpha v = 0$.
Thus every element in $V$ is a linear combination of some $F_{\beta_1}^{r_1} \cdots F_{\beta_n}^{r_n} v$.

We are actually interested in a standard cyclic module which is irreducible.
This will be the central feature of the representation theory.

\newpage

\textbf{Irreducible modules}

Now we construct an irreducible module from a standard cyclic module of $\lambda$.
Let $Z(\lambda)$ be a cyclic module, and for our purpose it may not harm the generality if we assume that $U(\lambda)$ is not irreducible.
Then $Z(\lambda)$ has a maximal proper submodule $Y(\lambda)$.
We then obtain that $V(\lambda) = Z(\lambda) / Y(\lambda)$ is an irreducible module of which a highest weight is $\lambda$.
Thus our aim is to find such $Y(\lambda)$.

To see this we first see that $Z(\lambda)$ is a direct sum of all $Z(\lambda)_\mu$'s, where $\mu = \lambda - \sum_{i = 1}^l k_i \alpha_i$ with $0 \le k_i \in \InZ$ and that $\dim{Z(\lambda)_\mu} < \infty$ and $Z(\lambda)_\lambda = kv$, $v$ the maximal vector.
This is immediate from the fact that the set of all $F_{\beta_1}^{r_1} \cdots F_{\beta_n}^{r_n} v$ is a basis of $Z(\lambda)$.
Moreover, if $W$ is a submodule of $Z(\lambda)$, it can be shown that $W$ is a direct sum of some $Z(\lambda)_\mu$'s.
Suppose that $W$ is not.
Then there is $w \in W$ such that $w = v_1 + v_2 + \cdots + v_r$ for $v_i \in Z(\lambda)_{\mu_i}$, weights $\mu_i$, but any of $v_i$ is not in $W$.
We then let $r$ be minimum, which can be possible.
But if for any $H \in \lie{h}$ we have that $(H - \mu_1(H) 1)w = (\mu_2 - \mu_1)(H) v_2 + (\mu_3 - \mu_1)(H) v_3 + \cdots + (\mu_r - \mu_1)(H) v_r$, which is in $W$, a contradiction to the minimality of $r$.
Therefore, any submodule must be a summation of some $Z(\lambda)_\mu$'s.
Note that if $W$ contains $Z(\lambda)_\lambda$, since $Z(\lambda) = U(\lie{g}) \cdot v$, $W = Z(\lambda)$.

Now let $Y(\lambda)$ be the direct sum of all proper submodule of $Z(\lambda)$.
It must be a submodule of $Z(\lambda)$ and maximal.
But it is not $Z(\lambda)$ since any of proper submodule cannot contain $Z(\lambda)_\lambda$.
Hence we find the maximal proper submodule $Y(\lambda)$, which we seek.

We have constructed an irreducible module $V(\lambda)$, which has $\lambda$ as a highest weight.
Now we let $V$ be any irreducible module having $\lambda$ as a highest weight.
Let $v, v'$ be the corresponding maximal vectors of $V(\lambda)$ and $V$, respectively.
Then we can construct a nonzero $\lie{g}$-homomorphism of $V(\lambda)$ into $V$, just defining that $v \mapsto v'$.
By Schur's lemma this nonzero homomorphism must be an isomorphism.
Therefore, we obtain that the irreducible module with highest weight $\lambda$ is unique (up to isomorphism).
This consequence says that every finite-dimensional irreducible module is completely determined by its highest weight.

\newpage

\textbf{Finite-dimensional irreducible modules}

We have found irreducible modules which has a highest weight, and that the highest weight is the essential property of these modules.
But note that they are not finite-dimensional, necessarily.

Note that $\lie{g}$-module $V(\lambda)$ is actually a $\lie{s}_i$-module, where $\lie{s}_i = kH_i + kE_i + kF_i$.
Although $V(\lambda)$ may be not an irreducible $\lie{s}_i$-module, but it is definite that, if $V(\lambda)$ is finite-dimensional, $\lambda(H_i)$ is the highest weight of this $\lie{s}_i$-module.
It implies that $0 \le \lambda(H_i) \in \InZ$.
Therefore, if $V(\lambda)$ is finite-dimensional, $\lambda \in \Lambda^+$.

This is a necessary condition.
Is it sufficient?
We shall see that the answer is yes.
Let $\lambda \in \Lambda^+$.
We now assume that if for some $\mu \in E$ $V_\mu \ne 0$, then $V_{\sigma(\mu)} \ne 0$ for all $\sigma \in W$.
Then, since every weight $\mu$ of $V(\lambda)$ is in the form of $\mu = \lambda - \sum_{i = 1}^l a_i \alpha_i$ with $0 \le a_i \in \InZ$, we have that the set of every weights of $V(\lambda)$ is actually the saturated set $\Pi(\lambda)$.
We know that every weight $\mu$ of $V(\lambda)$ is in the form of $\mu = \lambda - \sum_{i = 1}^l a_i \alpha_i$ with $0 \le a_i \in \InZ$.
We already saw that $\Pi(\lambda)$ is finite.
According to this and the finiteness of $\dim{V(\lambda)_\mu}$ we finally have that $\dim{V(\lambda)}$ is finite.
Therefore, our aim becomes to show that for any weight $\mu$ and $i= 1, \cdots, l$ $\sigma_i(\mu)$ is also a weight.

We already have a good tool; $\tau_i = \exp{E_i} \exp{(-F_i)} \exp{E_i}$.
Since $\tau_i(V(\lambda)_\mu) = V(\lambda)_{\sigma_i(\mu)}$, using this we can finish our proof.
But, as mentioned, to use this we must show that these exponentials are well-defined, or equivalently $E_i$ and $F_i$ are (locally) nilpotent, and this makes our proof not simple.
Fortunately, we can show that $V(\lambda)$ is a direct sum of finite-dimensional irreducible modules of $\lie{s}_i$.
This immediately implies the local nilpotency of $E_i$ and $F_i$.
To see this let $V_i$ be the direct sum of finite-dimensional irreducible $\lie{s}_i$-submodules.
If $W$ is a finite-dimensional $\lie{s}_i$-submodule, then it is immediate that the space spanned by all $E_\alpha W$ for $\alpha \in \Phi$ is a finite-dimensional $\lie{s}_i$-submodule.
Therefore, $V_i$ is a $\lie{g}$-submodule of $V(\lambda)$, which must be equal to $V(\lambda)$ by the irreducibility.
It seems clear, but actually there is one obstacle: we do not know that $V_i \ne 0$.
Thus we have to find a nonzero finite-dimensional $\lie{s}_i$-submodule.
To see this, we need some formulae: $[H_j, F_i^{r + 1}] = -(r + 1) \alpha_i(H_j) F_i^{r + 1}$ and $[E_j, F_i^{r + 1}] = -\delta_{ij} (r + 1) F_i^r (r1 - H_i)$.
The first formula is immediate from the Leibnitz' rule and the second can be shown easily by the induction on $r$.
Let $v$ be the maximal vector of $V(\lambda)$ and $m_i \equiv \lambda(H_i)$.
From the above formula, if we let $w = F_i^{m_i + 1} v$, then $E_j w = 0$ for all $j$, which forces that $w = 0$.
Hence the subspace spanned by $v, F_i v, F_i^2 v, \cdots, F_i^{m_i} v$ is an $(m_i + 1)$-dimensional irreducible $\lie{s}_i$-module.
Now we have a nonzero finite-dimensional irreducible $\lie{s}_i$-submodule, so that $V_i \ne 0$.
Finally, we obtain what we want, thus $\tau_i$ is well-defined so that $\sigma(\mu)$ is a weight for any $\sigma \in W$ and any weight $\mu$.
From this, as mentioned above, we have finished to show that $V(\lambda)$ is finite-dimensional.

\newpage

\textbf{Universal Casimir element}

We introduce a lemma to start this page.
This lemma is for simple $\lie{g}$.
Let $f_1(X, Y)$ and $f_2(X, Y)$ be non-degenerate, associative and symmetric bilinear forms of $\lie{g}$.
We define for each $i$ and $X \in \lie{g}$ $s^i_X \in \lie{g}^*$ as $s^i_X(Y) = f_i(X, Y)$.
Then for each $i$ $s^i : X \mapsto s^i_X$ is a bijective linear mapping.
Also, we define $A \cdot s^i_X \equiv s^i_{[A, X]}$ for $A, X \in \lie{g}$.
Then, by the associativity of $f_i$, $s^i : X \mapsto s^i_X$ is a $\lie{g}$-homomorphism, where we regard $\lie{g}$ and $\lie{g}^*$ as $\lie{g}$-modules.
Thus $(s^2)^{-1} s^1 : \lie{g} \to \lie{g}$ is a nonzero $\lie{g}$-homomorphism.
But since $\lie{g}$ is simple so that $\lie{g}$ is an irreducible module of $\lie{g}$, by Schur's lemma, $(s^2)^{-1} s^1$ is a scalar multiplication.
Therefore, for any $X, Y \in \lie{g}$, $f_2(X, Y) = s^2_X(Y) = c s^1_X(Y) = cf_1(X, Y)$, where $c$ is some (nonzero) scalar, so $f_2$ is a scalar multiplication of $f_1$.

We met a special element in a (faithful) representation: the Casimir operator.
We can find a such element in $U(\lie{g})$.
Also, we will see that, actually, this element is more general than the Casimir operator.

The construct is same as the original Casimir operator.
Let $(U_i)$ be a basis of $\lie{g}$, and $(U'_i)$ be the dual basis of $(U_i)$ relative to the Killing form.
We can view these elements as in $U(\lie{g})$.
Then we can let $C_\lie{g} = \sum U_i U'_i$.
(As for the Casimir element, $C_\lie{g}$ is independent of the choosing basis.)
We have a Lie algebra homomorphism $\ad : \lie{g} \to (L(\lie{g}))_L$.
The universality of $U(\lie{g})$ then says that $\ad$ can be extended to a algebra homomorphism $\ad : U(\lie{g}) \to L(\lie{g})$.
Hence $\ad{C_\lie{g}} = C_\textrm{ad}$.
In this sense, we call $C_\lie{g}$ the \textbf{universal Casimir operator}.

Now we discuss about $\rho(C_\lie{g})$, $\rho$ a faithful representation and $\lie{g}$ simple.
We know that $\beta_\rho(X, Y) = \tr{\rho(X)\rho(Y)}$ is an non-degenerate, associative and symmetric bilinear form of $\lie{g}$.
Then by the above lemma there is a (non-zero) scalar $c_\rho$ such that for any $X, Y \in \lie{g}$ $\beta_\rho(X, Y) = c_\rho \kappa(X, Y)$.
From this we obtain that if $\beta_\rho(U_i, U'_j) = c_\rho \kappa(U_i, U'_j) = c_\rho \delta_{ij}$.
Also, if we let $(U''_i)$ be the dual basis of $(U_i)$ relative to $\beta_\rho$, it is immediate that $U'_i = c_\rho U''_i$.
Therefore, we obtain that $\rho(C_\lie{g}) = c_\rho C_\rho$, $C_\rho$ the Casimir operator of $\rho$.
This gives a help for analysis of structure of $\lie{g}$-modules.
We will see how to get $c_\rho$.
By the way, this argument does not imply that $C_\lie{g}$ commutes with all element in $U(\lie{g})$.
We will discuss about this later.

(The case for semisimples is different, but easy.
If $\lie{g} = \lie{g}_1 \oplus \lie{g}_2 \oplus \cdots \oplus \lie{g}_n$, $\lie{g}_i$ simple, then $C_\lie{g} = C_{\lie{g}_1} + C_{\lie{g}_2} + \cdots + C_{\lie{g}_n}$.)

For further usage, we fix the basis $U_i$'s as the set of $H_i$'s and $E_\alpha$'s ($\alpha$ a root).
Then $U'_i$, for which $U_i$ is one of $H_i$, satisfies that $U'_i \in \lie{h}$ and that $\kappa(H_i, U'_i) = \delta_{ij}$, so that $U'_i = T_{\lambda_i} \equiv T_i$, where $\lambda_i \in \lie{h}^*$ is defined as $\lambda_i(H_j) = \delta_{ij}$, and also, since $\kappa(E_\alpha, F_\beta) = \frac{2}{(\alpha, \alpha)} \delta_{\alpha \beta}$, $U'_i$, for which $U_i$ is one of $E_\alpha$, must be $\frac{1}{2} (\alpha, \alpha) F_\alpha$, denoted by $Y_\alpha$.

\newpage

\textbf{Algorithm to decompose an irreducible module}

Now we return to the investigation about the structure of irreducible modules.
Let $\alpha$ be a root and $\mu$ be a weight of $V = V(\lambda)$ for a dominant integral $\lambda \in\lie{h}^*$.
We are interseted in how to calculate $\dim{V_\mu}$; if we know them all, then we can expect how $E_\alpha$'s works well.
To do this, we need to calculate $\tru{V_\mu}{E_\alpha Y_\alpha}$ for a fixed root $\alpha$.
Let $B_\mu$ be the set of the weight vectors in $V_\mu$, where it is of course a basis of $V_\mu$.
It is immediate that each in $B_\mu$ is an eigenvector of $E_\alpha Y_\alpha$.
Then, $\tru{V_\mu}{E_\alpha Y_\alpha}$ is just the summation of the eigenvalues of $E_\alpha Y_\alpha$ relative to each in $B_\mu$.
On the other hand, for each $w \in B_\mu$ there is an irreducible $\lie{s}_\alpha$-submodule $U_w$ where $w$ is a weight vector of $U_w$.
Note that we can choose the maximal vector $w^+$ of $U_w$ such that $\frac{1}{i_w!} F_\alpha^{i_w} w^+ = w$ ($i_w \in \NaN_0$).
We denote $m_w = \dim{U_w} - 1$.
Since $w^+$ is also a weight vector, there is a weight $\mu_0$ ($=\mu + i_w \alpha$) such that $w^+ \in V_{\mu_0}$, and we have that $H_\alpha w^+ = m_w w^+$, or $(\mu_0, \alpha) w^+ = T_\alpha w^+ = \frac{(\alpha, \alpha)}{2} m_w w^+$.
It is well-known that $E_\alpha F_\alpha w = (i_w + 1)(m_w - i_w) w$, so $E_\alpha Y_\alpha w = \frac{(\alpha, \alpha)}{2} (i_w + 1)(m_w - i_w) w$.
Remind that this formula is got by $E_\alpha F_\alpha w = \frac{1}{i_w!}[E_\alpha, F_\alpha^{i_w + 1}]w^+ = \frac{1}{i_w!} \sum_{j = 0}^{i_w} F_\alpha^{i_w - j} H_\alpha F_\alpha^j w^+ = (\sum_{j = 0}^{i_w} (m_w - 2j)) w$.
From this, $E_\alpha Y_\alpha w = \frac{(\alpha, \alpha)}{2} E_\alpha F_\alpha w = (\sum_{j = 0}^{i_w} (\mu_0 - j\alpha, \alpha)) w = \sum_{j = 0}^{i_w} [(\mu + j\alpha, \alpha) w]$.
We denote $S_w = \sum_{j = 0}^{i_w} (\mu + j\alpha, \alpha)$.

The aim is getting $\sum_{w \in B_\mu} S_w$.
Now we assume that $m_w - i_w \ge 0$ for some $w \in B_\mu$.
Since $Hw = \mu(H)w$ for all $w \in B_\mu$, it means that $m_w - i_w \ge 0$ for all $w \in B_\mu$, or equivalently, $\alpha \succ 0$.
We denote $E_\alpha^i B_\mu = \{E_\alpha^i w \SBar w \in B_\mu\}$.
Of course, rescaling all elements in $B_{\mu + i\alpha}$ sufficiently (and temporarily), for each $i$ $E_\alpha^i B_\mu$ can be regarded as a subset of $B_{\mu + i\alpha}$.
Moreover, we can say that $B_{\mu + i\alpha}$ is contained in $E_\alpha^i B_\mu$ since each in $B_{\mu + i\alpha}$ is in $U_w$ for some $w \in B_\mu$; this is why we assume that $m_w - i_w \ge 0$ to have a guarantee for this argument (if $m_w - i_w < 0$, we cannot argue that).
These situations and $E_\alpha F_\alpha w = \frac{1}{i_w!} \sum_{j = 0}^{i_w} F_\alpha^{i_w - j} H_\alpha F_\alpha^j w^+$ say that each $(\mu + j\alpha, \alpha)$ in $S_w = \sum_{j = 0}^{i_w} (\mu + j\alpha, \alpha)$ is from $E_\alpha^j w$, or from one in $B_{\mu + j\alpha}$ and that the number of such terms in all $S_w$ is exactly $|B_{\mu + j\alpha}|$, or $\dim{V_{\mu + j\alpha}}$.
Thus, in this case we can say that $\sum S_w = \sum_{j = 0}^\infty m(\mu + j\alpha) (\mu + j\alpha, \alpha)$, where $m(\mu) = \dim{V_\mu}$.

How about the case for $m_w - i_w < 0$ for some $w \in B_\mu$?
Of course, in the same reason, $m_w - i_w < 0$ for all $w \in B_\mu$, or equivalently, $\alpha \prec 0$.
But in this case, as mentioned, we cannot say that $E_\alpha^i B_\mu = B_{\mu + i\alpha}$.
Instead, we return to $E_\alpha Y_\alpha w$.
It is actually $\frac{(\alpha, \alpha)}{2} E_\alpha F_\alpha w = \frac{(-\alpha, -\alpha)}{2} F_{-\alpha} E_{-\alpha} w = \frac{(-\alpha, -\alpha)}{2} E_{-\alpha} F_{-\alpha} w - \frac{(-\alpha, -\alpha)}{2} H_{-\alpha} w = E_{-\alpha} Y_{-\alpha} w - T_{-\alpha} w$.
If $\alpha$ is replaced by $-\alpha$, then for this new root $m_w - i_w \ge 0$, so we can use the above result.
Thus, $\tru{V_\mu}{E_\alpha Y_\alpha} = \tru{V_\mu}{(E_{-\alpha} Y_{-\alpha} + T_\alpha)} = \sum S_w + \tru{V_\mu}{T_\alpha}$.

To be clear, it is better to write $\tru{V_\mu}{E_\alpha Y_\alpha} + \tru{V_\mu}{E_{-\alpha} Y_{-\alpha}}$ for each positive root $\alpha$, not only $\tru{V_\mu}{E_\alpha Y_\alpha}$.
Then, it would be written as $2 \sum_{i = 0}^\infty m(\mu + i\alpha) (\mu + i\alpha, \alpha) + \tru{V_\mu}{T_{-\alpha}}$.
Since $T_{-\alpha} w = -(\mu, \alpha)w$ for any $w \in B_\mu$, we finally have that $\tru{V_\mu}{E_\alpha Y_\alpha} + \tru{V_\mu}{E_{-\alpha} Y_{-\alpha}} = 2 \sum_{i = 0}^\infty m(\mu + i\alpha) (\mu + i\alpha, \alpha) - m(\mu) (\mu, \alpha)$.

\newpage

\textbf{The Freudenthal's formula}

Now we use the previous result to calculate $m(\mu)$ for a weight $\mu$.
Note that by Schur's lemma $\rho(C_\lie{g})$ is a scalar multiplication of $V(\lambda)$.
Thus $\tru{V_\mu}{\rho(C_\lie{g})} = cm(\mu)$ for some scalar $c$.
We shall calculate it.

Remind that $C_\lie{g} = \sum H_i T_i + \sum_{\alpha \in \Phi} E_\alpha Y_\alpha$.
For $w \in V_\mu$ we have that $H_i T_i w = \mu(H_i) \mu(T_i) w = \frac{2(\mu, \alpha_i)(\mu, \lambda_i)}{(\alpha_i, \alpha_i)} w$.
Note that $(\alpha_i, \lambda_j) = \frac{(\alpha_i, \alpha_i)}{2} \delta_{ij}$.
Thus if we write $\mu = \sum a_i \alpha_i$, then $\frac{2(\mu, \alpha_i)(\mu, \lambda_i)}{(\alpha_i, \alpha_i)} = (\mu, a_i \alpha_i)$, so that $(\sum H_i T_i) w = (\sum (\mu, a_i \alpha_i)) w = (\mu, \mu) w$.
Thus $\tru{V_\mu}{\sum H_i T_i} = m(\mu) (\mu, \mu)$.

We already calculated $\tru{V_\mu}{E_\alpha Y_\alpha}$.
Thus we have that $\tru{V_\mu}{\rho(C_\lie{g})}$ is equal to 
\begin{eqnarray*}
  cm(\mu) &=& \sum H_i T_i + \sum_{\alpha \in \Phi^+} (E_\alpha Y_\alpha + E_{-\alpha} Y_{-\alpha}) \\
  &=& m(\mu) (\mu, \mu) + \sum_{\alpha \in \Phi^+} \left[ 2 \left( \sum_{i = 0}^\infty m(\mu + i\alpha) (\mu + i\alpha, \alpha) \right) - m(\mu)(\mu, \alpha) \right] \\
  &=& m(\mu) (\mu, \mu) + 2 \sum_{\alpha \in \Phi^+} \sum_{i = 1}^\infty m(\mu + i\alpha) (\mu + i\alpha, \alpha) + 2m(\mu)(\mu, \delta) \\
  &=& m(\mu) ( (\mu + \delta, \mu + \delta) - (\delta, \delta) ) + 2 \sum_{\alpha \in \Phi^+} \sum_{i = 1}^\infty m(\mu + i\alpha) (\mu + i\alpha, \alpha).
\end{eqnarray*}

Now, we have to get $c$.
It can be easily, because we know a special answer: $m(\lambda) = 1$ and $m(\lambda + \alpha) = 0$ for all $\alpha \in \Phi^+$.
From this, $c = (\lambda + \delta, \lambda + \delta) - (\delta, \delta)$.
Finally, we obtain that 
\begin{displaymath}
  ((\lambda + \delta, \lambda + \delta) - (\mu + \delta, \mu + \delta))m(\mu) = 2 \sum_{\alpha \in \Phi^+} \sum_{i = 1}^\infty m(\mu + i\alpha) (\mu + i\alpha, \alpha).
\end{displaymath}

This formula is called the \textbf{Freudenthal's (recursion) formula}.
We can then calculate all $\dim{V_\mu} = m(\mu)$ by this formula recursively.

\newpage

\textbf{Formal Character}

For each $\mu \in \Lambda$ we think about a 'formal' element $e(\mu)$.
But we set that $e : \mu \mapsto e(\mu)$ is not a linear mapping.
Instead of this, we define $e(\mu)e(\nu) = e(\mu + \nu)$.
(As noticed, this is an 'abstract version' of the exponential mapping.)
Now we let $\InZ(\Lambda)$ be the $\InZ$-group formally generated by all $e(\mu)$ for $\mu \in \Lambda$.
Equivalently, $\InZ(\Lambda)$ is the $\InZ$-module with basis elements $e(\mu)$.
Furthermore, the multiplication rule $e(\mu)e(\nu) = e(\mu + \nu)$ gives the natural commutative ring structure on $\InZ(\Lambda)$.
We give an action of the Weyl group $W$ on $\InZ(\Lambda)$ as $\sigma(e(\mu)) \equiv e(\sigma(\mu))$ for $\sigma \in W$.

Now we let $\textrm{ch}_\lambda = \sum_{\mu \in \Lambda} m(\mu) e(\mu)$, called the \textbf{(formal) character}.
(It is also denoted by $\textrm{ch}_{V(\lambda)}$ or $\textrm{ch}_V$, where $V = V(\lambda)$.)
Since $m(\mu) = 0$ if $\mu \notin \Pi(\lambda)$, the summation is finite so that it is well-defined.
One can define, as this, for a module $V = V(\lambda_1) \oplus V(\lambda_2) \oplus \cdots \oplus V(\lambda_m)$ $ch_V = \sum ch_{\lambda_i}$.
Note that the action of $W$ fixes $ch_V$, whenever $V$ is irreducible or not.

The following lemma shows that this definition presents well the characteristics of $V$.
\textit{Let $f = \sum_{\lambda \in \Lambda} c_\lambda e(\lambda)$ ($c_\lambda \in \InZ$) be fixed by the action of $W$.
Then it can be written one and only one way as a $\InZ$-linear combination of $ch_\lambda$ ($\lambda \in \Lambda^+$).}
To do this, note that $f$ can be written as $\sum_{\lambda \in \Lambda^+} c_\lambda \sum_{\sigma \in W} e(\sigma \lambda)$.
If $\lambda' \in \Lambda^+$ is a maximum of $\lambda$'s with $c_\lambda \ne 0$ and $f' = f - c_{\lambda'} ch_{\lambda'}$ is written as $\sum_{\lambda \in \Lambda} c_\lambda e(\lambda)$, then the number of nonzero $c_\lambda$ in $f'$ is less than that in $f$ since any pther $ch_\lambda$ does not contain a term for $e(\lambda')$.
Thus we can use an induction on the number of something about $\lambda$'s.
To make the start of induction easy, we define $M_f$ as $\bigcup_{\lambda \in \Lambda^+, c_\lambda \ne 0} \{\mu \in \Lambda^+ \SBar \mu \prec \lambda\}$.
Obviously, $|M_{f'}| < |M_f|$.
Also, if $|M_f| = 1$, then $f$ must be $ch_{\lambda_i}$ since in this case $c_\mu$ is nonzero if and only if $\mu \in W\lambda$ and, when nonzero, $c_\mu = 1$.
Therefore, by induction on $|M_f|$, the existence has been proved.
The uniqueness of this expression is immediate: Assume $0 = \sum_{\lambda \in \Lambda^+} a_\lambda ch_\lambda$ and let $\lambda_M$ be a maximum of the set of $\lambda \in \Lambda^+$ with $a_\lambda \ne 0$ and guess what $c(\lambda_M)$ is.

One may be interested in the character of tensor product of two modules.
Let $V$ and $W$ be two modules of $\lie{g}$.
The module structure of $V \otimes W$  says that for weights $\lambda$ and $\lambda'$ of $V$ and $W$, respectively, and weights $\mu$ and $\mu'$ of $V$ and $W$, respectively, with $\lambda + \lambda' = \mu + \mu'$, $V_\mu \otimes W_{\mu'}$ is in $(V \otimes W)_{\lambda + \lambda'}$, and all subspaces with such $\mu$ and $\mu'$ are in $(V \otimes W)_{\lambda + \lambda'}$.
From this we obtain that $\dim{(V \otimes W)_{\lambda + \lambda'}} = \sum_{\lambda + \lambda' = \mu + \mu'} m_V(\mu) m_W(\mu')$, so it is obvious that \textit{$ch_{V \otimes W} = ch_V \cdot ch_W$}.
This will help us when we treat such module.


\newpage

\textbf{Invariant polynomial function}

We met 'formal character'.
To meet characters more deeply, we need some additional stuffs.
We know that we can construct a 'symmetric algebra' from $T(V)$ (with presentation by $v \otimes w - w \otimes v$), where $V$ is a vector space.
Now we call the symmetric algebra from $V^*$ the algebra of \textbf{polynomial function}, where the algebra is denoted by $\mathfrak{B}(V)$.

There is a useful lemma about polynomial function; \textit{every polynomial of $n$ variables can be written as a linear combination of powers of linear polynomials, i.e., the form of $\sum a_i (\sum a_{ij} t_j)^{r_i}$.}
By induction on $n$, one can generalize immediately the following and obtain the lemma: $x^{r - q} y^q$ ($x, y$ indeterminants, $r, q \in \NaN_0$) can be written as $\sum_i a_i(x + c_i y)^r$.
Expanding this it is obtained that it is equal to $\sum_{i, j} a_j c_j^{i - 1} x^{r - i} y^i$.
Thus we should find $c_i$ and $a_i$ such that $\sum_j c_j^{i - 1} a_j = \delta_{iq}$.
But by the Vandermonte's determinant, it is immediate that choosing $c_i$ distinct guarantees the existence of such $a_i$, which is always possible, so the proof is done,

We are ready to equip this to Lie algebra theory.
Let $\lie{g}$ be a finite-dimensional semisimple Lie algebra and $\lie{h}$ a Cartan subalgebra of $\lie{g}$, with the Weyl group $W$.
We focus on special elements in $\mathfrak{B}(\lie{h})$ and $\mathfrak{B}(\lie{g})$.
On $\mathfrak{B}(\lie{h})$, note that, since it is generated by $\lie{h}^*$, $W$ acts on $\mathfrak{B}(\lie{h})$ naturally.
We call a $W$-fixed element in $\mathfrak{B}(\lie{h})$ a \textbf{$W$-invariant polynomial function}, where the set of these elements is denoted by $\mathfrak{B}(\lie{h})^W$.
Also for $f \in \mathfrak{B}(\lie{h})$ we denote the summation of all in $W$-orbit of $f$, which is of course in $\mathfrak{B}(\lie{h})^W$, by $\textrm{Sym}\; f$.
Obviously, with the above lemma, all $\textrm{Sym}\; \lambda^r$ for $\lambda \in \Lambda$ span $\mathfrak{B}(\lie{h})^W$.
Meanwhile, let $G = \textrm{Int}\; \lie{g}$.
As of $W$, $G$ can act on $\mathfrak{B}(\lie{g})$ by $f \mapsto f \circ \sigma$ for $f \in \mathfrak{B}(\lie{g})$ and $\sigma \in G$.
We call a $G$-fixed element in $\mathfrak{B}(\lie{g})$ a \textbf{$G$-invariant polynomial function}, where the set of these elements is denoted by $\mathfrak{B}(\lie{g})^G$.

\newpage

\textbf{Trace polynomial}

There is an important ingredient.
Let $\phi$ be a finite-dimensional irreducible representation of $\lie{g}$.
We call $\lie{g} \ni X \mapsto \tr{(\phi(X)^r)}$ ($r \in \NaN$) a \textbf{trace polynomial}, which is obviously in $\mathfrak{B}(\lie{g})$ since this is a polynomial of entries of $\phi(X)$ (relative to a basis).
We wonder how $G$ acts on this function.
For $\sigma\in G$ we define a new representation $\phi^\sigma$ as $\phi^\sigma(X) = \phi(\sigma(X))$.
To investigate this let $\Delta$ be a base and choose $Z \in \lie{n}(\Delta)$.
Then, as we know ($\textrm{Int}\; \lie{g} = E(\lie{g})$), $\sigma = \exp{\ad{Z}}$ is in $G$ and such elements generate $G$.
Let $\lambda$ be the highest weight of $V$ relative to $\Delta$ and $v^+$ the maximal vector of $V$.
Then for $H \in \lie{h}$ it is immediate that $\phi^\sigma(H) v^+ = \phi(H + [Z, H]) v^+ = \phi(H)v^+$ since $Z \in \lie{n}(\Delta)$ so that $\phi(Z) v^+ = 0$.
Furthermore, since $\sigma(\lie{n}(\Delta)) \subseteq \lie{n}(\Delta)$, for all $N \in \lie{n}(\Delta)$ $\phi^\sigma(N) v^+ = 0$, so $v^+$ is also the maximal vector of the representation $\phi^\sigma$.
Thus, if we define a linear mapping $\psi_\sigma : V \to V$ as $\phi(X) (\psi_\sigma v) = \psi_\sigma(\phi^\sigma(X) v)$, $\psi_\sigma$ is an module isomorphism.
(N.B., this is not necessarily a scalar multiplication since $\phi$ and $\phi^\sigma$ are different representations.)
From this we obtain that for any $X \in \lie{g}$ $\phi(X)$ and $\phi^\sigma(X)$ are similar.
Hence, $\tr{(\phi^\sigma(X)^r)} = \tr{(\phi(X)^r)}$.
But $\sigma$ sends the polynomial function $X \mapsto \tr{(\phi(X)^r)}$ to $X \mapsto \tr{(\phi(\sigma(X))^r)} = \tr{(\phi^\sigma(X)^r)}$.
Therefore, \textit{the trace polynomial is in $\mathfrak{B}(\lie{g})^G$} (remind that such $\sigma$ generates $G$).

What we shall see next is the relation of $\mathfrak{B}(\lie{h})^W$ and $\mathfrak{B}(\lie{g})^G$.
We wonder that there is a homomorphism of $\mathfrak{B}(\lie{g})^G$ into $\mathfrak{B}(\lie{h})^W$.
It is natural to consider the restriction: $\theta : \mathfrak{B}(\lie{g})^G \ni f \mapsto f|_{\lie{h}}$.
Of course, $\theta$ sends $f$ into $\mathfrak{B}(\lie{h})$.
One might guess whether $f|_\lie{h}$ is in $\mathfrak{B}(\lie{h})^W$.
Fortunately, we have $\tau^\phi_i$ which corresponds to $\sigma_i$ in $W$.
This tool says immediately that for $f \in \mathfrak{B}(\lie{g})^G$ $f|_\lie{g}$ is fixed by all $\sigma_i$ so that it is in $\mathfrak{B}(\lie{h})^W$.
Of course $\theta$ preserves the algebraic structure.
Thus, we have \textit{a homomorphism $\theta : \mathfrak{B}(\lie{g})^G \to \mathfrak{B}(\lie{h})^W$ defined as $f \mapsto f|_\lie{h}$}.

To go further, we need to show something more: \textit{The homomorphism $\theta : \mathfrak{B}(\lie{g})^G \to \mathfrak{B}(\lie{h})^W$ is surjective}, which is called \textbf{Chevalley's theorem}.
To show this (Steinberg), it is sufficient to show that all $\textrm{Sym}\; \lambda^r$ for all $\lambda \in \Lambda^+$ lies in $\mathfrak{B}(\lie{h})^W$.
Let $\phi_\lambda$ be the irreducible representation of $\lie{g}$ with highest weight $\lambda$.
Note that the set of all weight vectors (up to scalar multiplication) forms a basis, thus the diagonal components of $\phi_\lambda(H)$ for $H \in \lie{h}$ are $\mu(H)$ ($\mu \in \Pi(\lambda)$).
Hence, $\tr{(\phi_\lambda(H)^r)} = \sum_{\mu \in \Pi(\lambda)} m_\lambda(\mu) (\mu(H))^r$.
This form says that the restriction of the trace polynomial is actually $\textrm{Sym}\; \lambda^r + \sum_{\lambda \prec \mu \in \Pi(\lambda) \cap \Lambda^+} c_\lambda(\mu) \textrm{Sym}\; \mu^r$ ($c_\lambda(\mu)$ some scalar).
Thus, if for all $\lambda \prec \lambda' \in \Lambda^+$ $\textrm{Sym}\; \lambda'^r$ is in $\mathfrak{B}(\lie{h})^W$, the proof is over.
From this form it is good to use the induction on the number of $\lambda' \in \Lambda^+$ with $\lambda' \succ \lambda$, while the case for this number 0 is obvious, so the proof is done.

Also note that, by the coincidence of the abstract decomposition with the Jordan decomposition in a representation, the value of $X \mapsto \tr{(\phi(X)^r)}$ is determined only by the semisimple part of $X$, that is, if $X, Y \in \lie{g}$ has same semisimple part, then $\tr{(\phi(X)^r)} = \tr{(\phi(Y)^r)}$.
We assume that for a trace polynomial $f : X \mapsto \tr{(\phi(X)^r)}$ $f|_\lie{h} = 0$.
Then obviously $\lambda(H) = 0$ for all $H \in \lie{h}$, where $\lambda$ is the highest weight, which is possible only when $\phi$ is trivial.
Hence, we have that $f = 0$.
Let $\mathfrak{T}$ be a subalgebra generated by all trace polynomial.
All what we have seen then shows that $\theta|_\mathfrak{T}$ is bijective.
We will use this fact to investigate about character.

\newpage

\textbf{Character}

Let $\mathfrak{C}$ be the center of $U(\lie{g})$.
Of course, every element of $\mathfrak{C}$ is fixed under $G = \textrm{Int}\; \lie{g}$.
But $\mathfrak{C}$ has more interesting property; \textit{an element in $U(\lie{g})$ is in $\mathfrak{C}$ if and only if it is fixed by $G$.}
Let $\lie{h}$ be a Cartan subalgebra of $\lie{g}$ and $\Phi$ the root system, and $X \in U(\lie{g})$ fixed by $G$.
It is immediate that if one shows that $[Z, X] = 0$ for every $\alpha \in \Phi$ and $Z \in \lie{g}_\alpha$, then $X$ is in $\mathfrak{C}$, since all such $Z$ generates $\lie{g}$.
To show this we wish that $\ad{Z}$ can be written as a linear combination of some $\exp{\ad{Y}}$.
But it is easy; if $(\ad{Z})^{r - 1} \ne 0$ but $(\ad{Z})^r = 0$, $\exp{(a_i \ad{Z})} = \sum_{j = 0}^{r - 1} \frac{a_i^j}{i!} (\ad{Z})^j$.
Using the Vandermonte's determinant, if we choose $a_i$ distinctly, we can always find $b_i$ such that $\ad{Z} = \sum b_i \exp{(a_i \ad{Z})}$.
Then, $[Z, X] = (\sum b_i \exp{(a_i \ad{Z})}) X = (\sum b_i)X$.
Since $\ad{Z}$ is nilpotent, it must be 0 so that our proof is done.
The first usage is that \textit{the Casimir element $C_\lie{g}$ is in $\mathfrak{C}$}, which we promised to show.
It is immediate from the fact that $\kappa(gX, gY) = \kappa(X, Y)$, where $\kappa$ is the Killing form, and that the construction of $C_\lie{g}$ is independent of the choosing basis.

We investigate more about the center.
Let $Z \in \mathfrak{C}$, and let $U(\lambda)$ be the standard cyclic module which we constructed with $U(\lie{g})$ ($\lambda$ not necessarily dominant) and $v^+$ the maximal vector of $U(\lie{g})$.
Then for every $X \in \lie{g}$ $XZv^+ = ZXv^+$.
So, for every $H \in \lie{h}$, $H(Zv^+) = Z(Hv^+) = \lambda(H) Zv^+$ and $E_i(Zv^+) = Z(E_i v^+) = 0$.
It implies that $Zv^+ \in U(\lambda)_\lambda$.
Since $\dim{U(\lambda)_\lambda} = 1$, $Zv^+$ is a scalar multiplication of $v^+$, or $Zv^+ = \chi_\lambda(Z) v^+$ for some scalar $ch_\lambda(Z)$.
Also it is immediate that $\chi_\lambda : Z \to k$ is a $k$-algebra homomorphism.
We call $\chi_\lambda : Z \to k$ the \textbf{character} determined by $\lambda$.

Now we shall see an important features about character.
We say that two $\lambda, \mu \in \Lambda$ is \textbf{linked} if and only if $\lambda + \delta$ ($\delta$ the half of the summation of all positive roots) is conjugate with $\mu + \delta$ under the Weyl group $W$, demoted by $\lambda \sim \mu$.
What we shall see is that \textit{if $\lambda \sim \mu$, then $\chi_\lambda = \chi_\mu$.}
We need a property: \textit{For $\lambda \in \Lambda$ let $\alpha$ be a simple root and $m = \langle \lambda, \alpha \rangle$.
Then the coset of $Y_\alpha^m$ in $U(\lambda)$ is a submodule which is standard cyclic having highest weight $\lambda - (m + 1)\alpha$}; it is immediate from the fact that for a maximal vector $v^+$ $E_i Y_\alpha^{m + 1} v^+ =0$ for all $i$, which is shown in the theorem for finite-dimensionalness of $V(\lambda)$.
Note that since $Z Y_\alpha^{m + 1} v^+ = Y_\alpha^{m + 1} Zv^+$, these two standard cyclic modules have same character.
To show the theorem we consider $\mu = \sigma_\alpha(\lambda + \delta) - \delta$.
Note that $\lambda \sim \mu$ and that if one can show that $\chi_\mu = \chi_\lambda$ for such $\mu$, then our proof is done immediately.
On the other hand, $\mu$ is actually equal to $\lambda - (m + 1)\alpha$.
This and the previous proposition suggest that we can identify $U(\mu)$ with a submodule of $U(\lambda)$, which is got in the proposition and has same character as $U(\lambda)$.
Therefore, $\chi_\lambda \sim \chi_\mu$.

This result is most of a half part of a theorem we shall prove.
In fact, the result can be extended; \textit{for $\lambda, \mu \in \lie{h}^*$ if $\lambda \sim \mu$, then $\chi_\lambda = \chi_\mu$.}
We will study about the extensions in the next page.

\newpage

\textbf{Extension to $\lie{h}^*$}

We have proved a theorem: for $\lambda, \mu \in \Lambda$ if $\lambda \sim \mu$ then $\chi_\lambda = \chi_\mu$, and mentioned that this result can be extended to the case for $\lambda, \mu \in \lie{h}^*$, that is, we can have that \textit{for $\lambda, \mu \in \lie{h}^*$ if $\lambda \sim \mu$ then $\chi_\lambda = \chi_\mu$}.
To extend this we need more algebraic tools to express characters in another way which will be helpful.

First, choose and fix a base and consider a basis $E_\beta, F_\beta, H_i$ ($\beta \succ 0$) of $\lie{g}$.
We consider a PBW basis $F_{\beta_1}^{p_1} \cdots F_{\beta_m}^{p_m} H_1^{q_1} \cdots H_l^{q_l} E_{\beta_1}^{r_1} \cdots E_{\beta_m}^{r_m}$, where we denote all positive roots by $\beta_i$.
Let $\lambda \in \lie{h}^*$ and $v^+$ a maximal vector of $V(\lambda)$.
Then it is immediate that only elements of the basis in the form of $\prod H_i^{q_i}$ sends $v^+$ to its scalar multiplication $\lambda(H_1)^{q_1} \cdots \lambda(H_l)^{q_l} v^+$.
This result can be expressed in more useful way.
We can consider an algebra homomorphism $\xi : U(\lie{g}) \to U(\lie{h})$ generated by the configuration $E_\alpha \mapsto 0$ and $F_\alpha \mapsto 0$ and $H_i \mapsto H_i$.
If we confuse $\lambda$ with the $k$-algebra homomorphism of $U(\lie{g})$ into $k$ generated by $\lambda$, we have that $\lambda \circ \xi : U(\lie{g}) \to k$.
Especially, it is immediate that $\chi_\lambda(Z) = \lambda(\xi(Z))$ for $Z \in \mathfrak{C}$ and this mapping of $\mathfrak{C}$ into $k$ is an algebra homomorphism.

This configuration is, however, not enough to us since it does not say anything about $W$-invariance.
Instead of this, we introduce a bizarre mapping which will cooperate with the relation between linkage and character we saw.
Let $\eta : U(\lie{h}) \to U(\lie{h})$ be generated by $H_i \mapsto H_i - 1$, which exists since the linear mapping spanned by $H_i \mapsto H_i - 1$ is a Lie homomorphism.
Now $\psi = \eta \circ \xi|_\mathfrak{C}$.
Actually, this mapping is designed to be compatible to $\lambda + \delta$; since for all $i$ $\delta(H_i) = 1$, $(\lambda + \delta)(\psi(H_i)) = (\lambda + \delta)(H_i - 1) = \lambda(H_i) - \delta(H_i) + 1 = \lambda(H_i)$.
Thus, we have that $\chi_\lambda(Z) = (\lambda + \delta)(\psi(Z))$.
Now in this stage we can use the theorem in the previous page to explain the $W$-invariance.
Assume that $\lambda \in \Lambda$.
Then the theorem says that for every $\sigma \in W$ and $Z \in \mathfrak{C}$ $(\sigma(\lambda + \delta))(\psi(Z)) = (\lambda + \delta)(\psi(Z))$.
It implies that every $\lambda \in \Lambda$, or hence $\mu = \lambda + \delta \in \Lambda$ takes same value for all $W$-conjugates of $\psi(Z)$, where the action of $W$ on $U(\lie{h})$, denoted by $\sigma \cdot H$ for $\sigma \in W$ and $H \in \lie{h}$, is that, if we define $\lambda_H : \lie{h} \ni A \mapsto \kappa(H, A)$ ($\kappa$ the Killing form), then $\lambda_{\sigma \cdot H} = \sigma(\lambda_H)$ (this implies an identification of $\lie{h}$ with $\lie{h}^*$ as $W$-modules).
This then forces $\psi(Z)$ is fixed by $W$.
Note that in this case, since $(\lie{h}^*)^* = \lie{h}$ and $\lie{h}$ is Abelian so that $U(\lie{h})$ can be identified with the symmetric algebra generated by $\lie{h}$, we can say that $\psi(Z) \in \mathfrak{B}(\lie{h}^*)^W$.
Thus, we can say that $\psi$ is a mapping of $\mathfrak{C}$ into $\mathfrak{B}(\lie{h}^*)^W$.

From this, we can extend our previous theorem.
For any $Z \in \mathfrak{C}$ then $(\sigma(\lambda + \delta))(\psi(Z)) = (\lambda + \delta)(\psi(Z))$ for every $\lambda \in \lie{h}^*$ and $\sigma \in W$, where the two sides are equal to $\chi_{\sigma(\lambda + \delta) - \delta}(Z)$ and $\chi_\lambda(Z)$, respectively.
Therefore, we have proved that \textit{for $\lambda, \mu \in \lie{h}^*$ if $\lambda \sim \mu$, then $\chi_\lambda = \chi_\mu$}.

The used terminologies can be used to prove a lemma which is needed in the proof of an important theorem we will meet: \textit{For $\lambda_1, \lambda_2 \in \lie{h}^*$ if $\lambda_1(A) = \lambda_2(A)$ for all $A \in \mathfrak{B}(\lie{h}^*)^W$, then $\lambda_1$ anf $\lambda_2$ are conjugate under $W$}.
To see this, suppose that $\lambda_1$ and $\lambda_2$ lie in distinct $W$-orbits.
Let $M = (W\lambda_1 \setminus \{\lambda_1\}) \cup W\lambda_2$ and for each $\mu \in M$ choose $H_\mu \in \lie{h}$ such that $\lambda_1(H_\mu) \ne \mu(H_\mu)$, and let $X = \prod_{\mu \in M} (H_\mu - \mu(H_\mu) 1)$.
Then $\lambda_1(X) \ne 0$ but for any $\mu \in M$ $\mu(X) = 0$.
So, for $A = \sum_{\sigma \in W} \sigma \cdot X$ $\lambda_1(A) \ne 0$ while $\lambda_2(A) = 0$, a contradiction.
Therefore, $W\lambda_1 = W\lambda_2$.

\newpage

\textbf{Harish-Chandra's theorem}

Now it is time to the final extension of the theorem we proved; \textit{for $\lambda, \mu \in \lie{h}^*$ $\chi_\lambda = \chi_\mu$ if and only if $\lambda \sim \mu$}, called the \textbf{Harish-Chandra's theorem}.
We proved an half of this theorem in the previous page.
Also, the last lemma implies that if one can show that $\psi : \mathfrak{C} \to \mathfrak{B}(\lie{h}^*)^W$ is surjective, then the other half is shown.
This page is spared to prove it.

The main idea of the proof is to use the Chevalley's theorem; the homomorphism $\theta : \mathfrak{B}(\lie{g})^G \to \mathfrak{B}(\lie{h})^W$ is surjective.
Note that the identification of $\lie{h}$ and $\lie{h}^*$, which is mentioned, yields an $W$-module isomorphism $\tau_\lie{h} : \mathfrak{B}(\lie{h}^*)^W \to \mathfrak{B}(\lie{h})^W$, and we denote $X' \equiv \tau_\lie{g}(X)$ for $X \in \mathfrak{B}(\lie{h})^W$.
Similarly, we can get an $G$-module isomorphism $\tau_\lie{g} : \mathfrak{B}(\lie{g}^*)^G \to \mathfrak{B}(\lie{g})^G$, and denote $Y' \equiv \tau_\lie{h}(Y)$ for $Y \in \mathfrak{B}(\lie{g})^G$.
In this moment one might wish a bijection of $\mathfrak{B}(\lie{g}^*)^G$ onto $\mathfrak{C}$.
There is a good mapping for us.
It is immediate that $\mathfrak{B}(\lie{g})$, or the symmetric polynomial algebra of $\lie{g}$ is isomorphic to the symmetric tensor $\mathcal{S}$ in $U(\lie{g})$, which is spanned by $(1/r!) \sum_{\sigma \in S_r} X_{\sigma(1)} \otimes X_{\sigma(2)} \otimes \cdots \otimes X_{\sigma(r)}$ ($X_i \in \lie{g}$, $S_r$ the symmetric group), and that by PBW theorem $\mathcal{S}$ is complementary to the kernel $\mathcal{I}$ (or the presentation) of the canonical mapping $T(\lie{g}) \to U(\lie{g})$.
It implies that the canonical mapping yields a linear isomorphism $\pi' : \mathfrak{B}(\lie{g}^*) \to U(\lie{g})$.
Moreover, it is immediate that $\pi$ is a $G$-module homomorphism (but not an algebra homomorphism).
Thus, we have a $G$-module isomorphism $\pi = \pi'|_{\mathfrak{B}(\lie{g}^*)^G} : \mathfrak{B}(\lie{g}^*)^G \to U(\lie{g})^G = \mathfrak{C}$.

Now, one might guess that $\tau_\lie{g}$ and $\tau_\lie{h}$ lift $\theta$ to $\psi \circ \pi$.
This means that for some $X \in \mathfrak{B}(\lie{g^*})^G$ $((\psi \circ \pi)(X))'$ does not coincide with $\theta(X')$.
It can be checked if, for example, one lets $\lie{g}$ be the three-dimensional simple Lie algebra and calculates, for $C = (1/64)H^2 + (1/32)EF + (1/32)FE( \in \mathfrak{C})$, $\psi(C)$ and $\tau_\lie{h}^{-1} \circ \theta \circ \tau_\lie{g} \circ \pi^{-1}(C)$, where the result is $\lambda_1^2 - 1/64$ and $\lambda_1^2$, respectively.
But notice that these results have same highest homogeneous part.
This is remarkable; one might guess that for $X \in \mathfrak{B}(\lie{g^*})^G$ $\theta(X')$ and $\psi(\pi(X))'$ have same highest homogeneous part.
To check this let $X \in \mathfrak{B}(\lie{g}^*)^G$ be homogeneous.
Note that for any root $\alpha$ $(E_\alpha)'(H) = \kappa_\lie{g}(E_\alpha, H) = 0$ for all $H \in \lie{h}$ ($\kappa_\lie{g}$ the Killing form of $\lie{g}$), so when we write $X$ as a linear combination of terms in the form of products of $H_i$, $E_\alpha$, $F_\alpha$ (not ordered, which we should remember), the only surviving term after applying $\theta$ to $X'$ is only in the form of product of $H_i'$.
Now we turn to $\xi(\pi(X))$.
The situation that only terms in the form of product of $H_i$ survive is similar, but to do this remind that we need to rewrite $\pi(X)$ by the PBW basis introduced above.
But note that although this task is applied, the term of product of $H_i$ are untouched, and so does $\eta$ (sending $H_i$ to $H_i - 1$).
Thus, the highest homogeneous term of $\psi(\pi(X))'$ is exactly $\theta(X')$, which we seek.

What we saw is actually that for every homogeneous element $Y \in \mathfrak{B}(\lie{h}^*)^W$ there is $C \in \mathfrak{C}$ such that the highest homogeneous part of $\psi(C)$ coincides with $Y$.
Then the discrepancy from lower degree terms can be backed up, i.e., we can find another $C_1 \in \mathfrak{C}$ such that $\psi(C)$ is the highest homogeneous part of $Y - \psi(C)$, which can be expressed by induction on the degree.
Therefore, $\psi$ is surjective, so we proved the Harish-Chandra's theorem.

\newpage

\textbf{From formal character to functions on $\lie{h}^*$}

Now we look up to the formal characters.
We define formal character on finite-dimensional modules.
But the definition cannot be applied to $Z(\lambda)$ for arbitrary $\lambda \in \Lambda$ since it can involve an awkward infinite summation.
In this situation, we are trained to adopt 'formal summation' using function; we denote the set of mappings $\Lambda \to \InZ$ by $\InZ[\Lambda]$ and we define $ch_{Z(\lambda)} \in \InZ[\Lambda]$ with $ch_{Z(\lambda)}(\mu) = \dim{Z(\lambda)_\mu}$.
Of course, this definition allows a natural summation rule.
Also, the product rule in the finite case can be extended to infinite case via \textbf{convolution}; for $f, g \in \InZ[\Lambda]$ we define $(f * g)(\lambda) = \sum_{\mu + \mu' = \lambda} f(\mu) g(\mu')$.
By these rules $\InZ[\Lambda]$ has a ring structure.
We also define $\mathfrak{X}$ by the set of $f : \lie{h}^* \to k$ of which support (the set of $\lambda \in \lie{h}^*$ with $f(\lambda) \ne 0$) is in a finite union of some sets in the form of $\{\lambda - \sum_{\alpha \succ 0} a_\alpha \alpha \SBar a_\alpha \in \NaN_0\}$.
Finally, it is obvious that $\mathfrak{X}$ has an associative $k$-algebra structure via convolution, containing 'formal characters' of any $Z(\lambda)$ for $\lambda \in \lie{h}^*$.

To go further, we introduce some special functions on $\lie{h}^*$.
In this part the \textbf{characteristic function} $\epsilon_{\lambda}$, which is defined as $\epsilon_{\lambda}(\lambda) = 1$ while $\epsilon_{\lambda}(\mu) = 0$ for any $\lambda \ne \mu \in \lie{h}^*$ plays an important role, which corresponds to $e(\lambda)$.
It is obvious that $\epsilon_\lambda * \epsilon_\mu = \epsilon_{\lambda + \mu}$ and $\epsilon_0$ works as an identity of $\mathfrak{X}$.
Note that the Weyl group $W$ acts on the functions as $(\sigma^{-1} f)(\lambda) = f(\sigma(\lambda))$, especially $\sigma \epsilon_\lambda = \epsilon_{\sigma(\lambda)}$.
Another important function is $p$, where $p(\lambda)$ is the number of $l$-tuples $(a_i)$ with $\sum a_i \alpha_i = -\lambda$, called the \textbf{Kostant function}.
It is immediate that $p = ch_{Z(0)}$.
The last function is $q = \prod_{\alpha \succ 0} (\epsilon_{\alpha / 2} - \epsilon_{-\alpha / 2})$, called the \textbf{Weyl function}.
We introduce one more function which is not so important, but useful.
For each root $\alpha \succ 0$ let $f_\alpha(-r\alpha) = 1$ ($r \in \NaN_0$) and $f_\alpha(\lambda) = 0$ otherwise.
(Formally, $f_\alpha = \epsilon_0 + \epsilon_{-\alpha} + \epsilon_{-2\alpha} + \cdots$.)
It is clear that these functions $p, q, f_\alpha$ are in $\mathfrak{X}$.

It is time to check some useful formulae.
We have (FC1) $p = \prod_{\alpha \succ 0} f_\alpha$, (FC2) $(\epsilon_0 - \epsilon_{-\alpha}) * f_\alpha = \epsilon_0$, (FC3) $q = \epsilon_\delta * \prod_{\alpha \succ 0} (\epsilon_0 - \epsilon_{-\alpha})$, where their proofs are easy (the proof of (FC2) may use that $\sum_{i = 0}^{n - 1} r^i = (1 - r^n) / (1 - r)$, but it is needed to be rigorous, which is immediate).
Another formula is that for each $i$ $\sigma_i q = -q$, so (FC4) $\sigma q = (-1)^{l(\sigma)} q$ ($\sigma \in W$), which directly comes from the fact that $\sigma_i$ permutes all positive roots excepts only for $\alpha_i$.
Also, (FC5) $q * p * \epsilon_{-\delta} = \epsilon_0$, which can be directly shown by expressing $p$ by (FC1) and using (FC2).
We will need (FC6) $ch_{Z(\lambda)}(\mu) = p(\lambda - \mu) = (p * \epsilon_\lambda)(\mu)$, of which the proof is easy too.
The final is (FC7) $q * ch_{Z(\lambda)} = \epsilon_{\lambda + \delta}$, which comes from (FC6) and (FC5).

We finish this page by introducing a useful notation.
For $\sigma \in W$ we denote $sn(\sigma) = (-1)^{l(\sigma)}$.
It is actually an analogue of sign of permutations.
In fact, if the given Lie algebra is of type $A_l$, $W$ is just a permutation and so $sn(\sigma)$ is exactly the sign of permutations.

\newpage

\textbf{Kostant's formula}

We need one more formula, which comes from Harish-Chandra's theorem.
We shall use this.
For $\lambda \in \lie{h}^*$ let $\mathfrak{M}_\lambda$ be the set of $\lie{g}$-modules $V$ which satisfy that (1) $V$ is the direct summation of some weight spaces (2) the action of $C \in \mathfrak{C}$ on $V$ is just by $\chi_\lambda(C)$ (3) the formal character of $V$ belongs to $\mathfrak{X}$.
It is obvious that $\mathfrak{M}_\lambda$ contains all standard cyclic module of $\lambda$ (and its submodules).
Note that by Harish-Chandra's theorem $\mathfrak{M}_\lambda = \mathfrak{M}_\mu$ if $\lambda \sim \mu$.

Now we introduce a lemma: \textit{any $0 \ne V \in \mathfrak{M}_\lambda$ has at least one maximal vector.}
To show this, note that iy is obvious that every weight $\mu$ of $V$ and each $\alpha \succ 0$ there is $m \in \NaN$ such that $\mu + r\alpha$ is not a weight of $V$ for all $m \le r \in \NaN$.
It implies the existence of weight $\lambda'$ of $V$ such that $\lambda' + \alpha$ is not a weight of $V$ for all $\alpha \succ 0$, which finishes the proof.
We let $\theta(\lambda) = \{\mu \in \lie{h}^* \SBar \mu \prec \lambda, \mu \sim \lambda\}$.
Now we shall prove that \textit{$Z(\lambda)$ has a composition series, that is, modules $W_i$ such that $Z(\lambda) = W_0 \gneq W_1 \gneq W_2 \gneq \cdots \gneq W_n = 0$ such that $W_{i + 1}$ is maximal in $W_i$ for each $i$.}
It is okay if we assume that $Z(\lambda)$ is not irreducible.
We shall find $W \le Z(\lambda)$ such that $W$ and $Z(\lambda)/W$ such that they can lie in $\mathfrak{M}_\lambda$ and satisfy some properties which make them able to be applied induction, which can finish the proof directly.
Remind what $Y(\lambda)$ is, which lies in $\mathfrak{M}_\lambda$, so it has a maximal vector and highest weight $\mu$, which implies that $V$ has a homomorphic image $W$ of $Z(\mu)$.
We know that $W$ has same character with that of $Z(\lambda)$, so $\lambda \sim \mu$ by Harish-Chandra's theorem, and $\mu \in \theta(\lambda)$ but $mu \ne \lambda$.
Now note that we can apply the above method to $W$ (actually $Z(\mu)$) and $Z(\lambda)/W$, which are standard cyclic.
On the other hand, $|\theta(\mu)| < |\theta(\lambda)|$ and (the maximum of) weights of $Z(\lambda)/W$ are less than that of $Z(\lambda)$ (note that they are finite), which give us a chance to use induction.
Therefore, we can construct a composition series.
This theorem and the proof immediately implies that \textit{each composition factor is isomorphic to one of (irreducible) $V(\mu)$ ($\mu \in \theta(\lambda)$)}, ($W_i / W_{i + 1}$ composition factor) and that \textit{$V(\lambda)$ occurs at only once in the composition factors}.

These results gives a key stone; $ch_{Z(\lambda)} = ch_{V(\lambda)} + \sum_{\lambda \ne \mu \in \theta(\lambda)} d(\mu) ch_{V(\mu)}$ ($d(\mu) \in \NaN$).
Now we fix $\lambda \in \lie{h}^*$ and give an ordering of $\theta(\lambda) = \{\mu_1, \mu_2, \cdots, \mu_r\}$ where $\mu_i \succ \mu_j$ implies $i \ge j$ (so $\mu_r = \lambda$).
What we have seen implies that for each $1 \le s \le r$ if one has that $ch_{V(\mu_j)}$ with $j < r$ can be written as a $\InZ$-linear combination of $ch_{Z(\mu_i)}$ ($i \le j$), then so is $ch_{V(\mu_s)}$, where the case for $j = 1$ holds initially.
Therefore, we have that \textit{for any $\lambda \in \lie{h}^*$ $ch_{V(\lambda)}$ can be written as $\sum_{\mu \in \theta(\lambda)} c(\mu) ch_{Z(\mu)}$ ($c(\mu) \in \InZ$), in particular, $c(\lambda) = 1$}.
Finally, we can apply this result to the case for $\lambda$ dominant integral.
In this case note that $\sigma(ch_\lambda) = ch_{V(\lambda)}$ for every $\sigma \in W$.
To use the linkage, we shall use, instead $ch_{V(\lambda)}$, $q * ch_{V(\lambda)} = \sum_{\mu \in \theta(\mu)} c(\mu) q * ch_{Z(\mu)} = \sum_{\mu \in \theta(\mu)} c(\mu) \epsilon_{\mu + \delta}$.
Then, $\sigma(q * ch_{V(\lambda)}) = sn(\sigma) q * ch_{V(\lambda)}$, while $\sigma(\sum_{\mu \in \theta(\mu)} c(\mu) \epsilon_{\mu + \delta}) = \sum_{\mu \in \theta(\mu)} c(\mu) \epsilon_{\sigma(\mu + \delta)}$.
These are same, especially, since $c(\lambda) = 1$, $c(\sigma^{-1}(\mu) - \delta) = sn(\sigma)$.
Since these cover all terms (run over $\mu \in \theta(\lambda)$), we have that (*) $q * ch_\lambda = \sum_{\sigma \in W} sn(\sigma) \epsilon_{\sigma(\lambda + \delta)}$.

Now, it is time to show one important result.
We can apply (FC5) and then (FC6) to (*), which gives that $m_\lambda(\mu) = \sum_{\sigma \in W} sn(\sigma) p(\mu + \delta - \sigma(\lambda + \delta))$, the \textbf{Kostant formula}.

\newpage

\textbf{Weyl's formula}

The formula (*) immediately implies that $q = \sum_{\sigma \in W} sn(\sigma) \epsilon_{\sigma(\delta)}$ (just putting $\lambda = 0$), and applying this to (*) we obtain that $(\sum_{\sigma \in W} sn(\sigma) \epsilon_{\sigma(\delta)}) * ch_\lambda = \sum_{\sigma \in W} sn(\sigma) \epsilon_{\sigma(\lambda + \delta)}$, called the \textbf{Weyl's formula}.
For a convenience we abbreviate by $\omega(\lambda + \delta)$ $\sum_{\sigma \in W} sn(\sigma) \epsilon_{\sigma(\lambda + \delta)}$, so the formula will be $\omega(\delta) * ch_\lambda = \omega(\lambda + \delta)$.

Now we are interested in $deg(\lambda) = \sum_{\mu \in \Pi(\lambda)} m_\lambda(\mu)$ for $\lambda \in \Lambda^+$, which is actually $\dim{V(\lambda)}$.
It is also obtained by putting all $\epsilon(\mu)$ in $ch_\lambda = \sum_{\mu \in \Pi(\lambda)} m_\lambda(\mu) \epsilon(\mu)$ to 1.
Now we let $\mathfrak{X}_0 \subseteq \mathfrak{X}$ be the subalgebra generated by all $\epsilon_\lambda$ with $\lambda \in \Lambda$, and let $v : \mathfrak{X}_0 \to k$ be the $k$-algebra homomorphism generated by the above evaluation (thus $v$ is actually the 'evaluation').
Hence, $deg(\lambda) = v(ch_\lambda)$.
From the Weyl's formula one expect that we can get this by calculating $v(\omega(\delta) * ch_\lambda) = v(\omega(\delta)) deg(\lambda) = v(\omega(\lambda + \delta))$.
But it is failed since $v(\omega(\delta)) = 0$.
We need to find an alternative way.

To solve this we shall see that for a root $\alpha$ $\epsilon_\lambda \mapsto (\lambda, \alpha) \epsilon_\lambda$ extends to an endomorphism $\partial_\alpha$ of $\mathfrak{X}_0$ which is actually a derivative, and let $\partial = \prod_{\alpha \succ 0} \partial_\alpha$.
Our strategy is that, instead of $v(\omega(\delta) * ch_\lambda)$, we shall calculate $v(\partial(\omega(\delta) * ch_\lambda) = v(\partial(\omega(\lambda + \delta))$.
Remind that $\omega(\delta) = q = \epsilon_{-\delta} \prod_{\alpha \succ 0} (\epsilon_\alpha - 1)$, so any term of which all $(\epsilon_\alpha - 1)$ are not killed by $\partial$ will be evaluated by $v$ to 0.
Thus we have that $v(\partial(\omega(\delta) * ch_\lambda)) = v(\partial(\omega(\delta))) deg(\lambda) = v(\partial(\omega(\lambda + \delta)))$ and $v(\partial(\omega(\delta)))$ would be nonzero, so we can get $deg(\lambda)$.

Remind that $\omega(\lambda + \delta) = \sum_{\sigma \in W} sn(\sigma) \epsilon_{\sigma(\lambda + \delta)}$.
On the other hand, we have that $\partial(\epsilon_{\sigma(\lambda + \delta)}) = (\prod_{\alpha \succ 0} (\alpha, \sigma(\lambda + \delta))) \epsilon_{\sigma(\lambda + \delta)} = (-1)^{n(\sigma)} (\prod_{\alpha \succ 0} (\alpha, \lambda + \delta)) \epsilon_{\sigma(\lambda + \delta)}$, where $n(\sigma)$ is the number of positive roots sent to a negative root by $\sigma$, which is equal to $l(\sigma)$ as we saw.
Putting all together, we have that $v(\partial(\omega(\lambda + \delta))) = |W| \prod_{\alpha \succ 0} (\lambda + \delta, \alpha)$.
Finally, we have that
\begin{displaymath}
  \dim{V(\lambda)} = deg(\lambda) = \frac{\prod_{\alpha \succ 0} (\lambda + \delta, \alpha)}{\prod_{\alpha \succ 0} (\delta, \alpha)}.
\end{displaymath}

This can be modified more easily to handle.
It is obvious that we can write the formula with $\langle \cdot, \cdot \rangle$ instead of $(\cdot, \cdot)$.
Remind that the set of $\alpha_i^\vee$ form a dual base and that $\langle \mu, \alpha \rangle = (\mu, \alpha^\vee)$ and that $\langle \mu, \alpha \rangle = (\mu, \alpha^\vee)$.
Using these one can easily calculate the formula.
There is another expression (from Jacobson).
Let $w_i = (\alpha_i, \alpha_i) / (\alpha_r, \alpha_r)$, ($r$ is fixed with $(\alpha_r, \alpha_r) = 1$).
Then, for $\alpha = \sum a_i^\alpha \alpha_i$, $(\sum m_i \lambda_i, \alpha) = \sum a_i^\alpha m_j (\lambda_j, \alpha_i) = \sum a_i^\alpha m_j ((\lambda_j, \alpha_i) / (\alpha_i, \alpha_i)) (\alpha_i, \alpha_i)$, so $\sum a_i^\alpha m_j (w_j/2) (\alpha_r, \alpha_r) \langle \lambda_j, \alpha_i \rangle = \frac{(\alpha_r, \alpha_r)}{2} \sum a_i^\alpha w_i m_i$.
From this, $deg(\lambda) = \prod_{\alpha \succ 0} \frac{\sum a_i^\alpha w_i (m_i + 1)}{\sum a_i^\alpha w_i}$.

This is what we need to finish the classification theorem, as mentioned.
It can be calculated that in the case for $B_l$ the roots are $\alpha_i + \cdots + \alpha_j$ and $\alpha_i + \cdots + \alpha_j + 2\alpha_{j + 1} + \cdots + 2\alpha_l$ with $w_1 = \cdots = w_{l - 1} = 2$ and $w_l = 1$; the case for $C_l$ $\alpha_i + \cdots + \alpha_j$ and $2\alpha_i + \cdots + 2\alpha_j + \alpha_{j + 1} + \cdots + \alpha_l$ with $w_1 = \cdots = w_{l - 1} = 1$ and $w_l = 2$.
From this one can directly (although it is complicating a bit; we omit it) calculate that in the case for $B_l$ $deg(\lambda_r) = \binom{2l + 1}{r}$ ($r < l$) and $deg(\lambda_l) = 2^l$, while in $C_l$ $deg(\lambda_r) = \binom{2l}{r} - \binom{2l}{r - 2}$.
In each case, the minimums are $2l + 1$ and $2l$.
If these simple Lie algebras are isomorphic, they must be same, but it is not.
Therefore, \textit{for any $l$ ($l \ge 3$) the type of $B_l$ and $C_l$ are not isomorphic.}

\newpage

\textbf{Steinberg's formula}

This is a usage of Kostant's formula and Weyl's formula.
In many case one might wonder the multiplicity of a certain irreducible submodule in the tensor product of two irreducible modules.
We will see that the two formulae will answer this question.

In the language of character the given problem can be read as to find $n(\lambda)$ in $ch_{\lambda'} * ch_{\lambda''} = \sum n(\lambda) ch_\lambda$.
Now multiplying $\omega(\delta)$ and using the Weyl's formula one obtains that $ch_{\lambda'} * \omega(\lambda'' + \delta) = \sum n(\lambda) \omega(\lambda + \delta)$.
In this point we use Kostant's formula in $ch_{\lambda'} = \sum m_{\lambda'}(\mu) \epsilon_\mu$, so we obtain that $(\sum_\mu \sum_{\sigma \in W} sn(\sigma) p(\mu + \delta - \sigma(\lambda' + \delta)) \epsilon_\mu) * \omega(\lambda'' + \delta) = \sum_\lambda n(\lambda) \omega(\lambda + \delta)$.
And then we put the explicit form of $\omega(\lambda + \delta) = \sum_{\tau \in W} sn(\tau) \epsilon_{\tau(\lambda + \delta)}$.
Now we have the following:
\begin{displaymath}
  \sum_\mu \sum_{\sigma \in W} \sum_{\tau \in W} sn(\sigma \tau) p(\mu + \delta - \sigma(\lambda' + \delta)) \epsilon_{\mu + \tau(\lambda'' + \delta)} = \sum_\lambda \sum_{\sigma \in W} n(\lambda) sn(\sigma) \epsilon_{\sigma(\lambda + \delta)}.
\end{displaymath}

To obtain $n(\lambda)$ we replace $\mu$ on the LHS and $\lambda$ on the RHS by $\nu$, where on the right $\nu = \sigma(\lambda + \delta) - \delta$ while on the left $\nu = \mu + \tau(\lambda'' + \delta) - \delta$.
Then the RHS becomes $\sum_{\sigma \in W} n(\sigma^{-1}(\nu + \delta) - \delta) \epsilon_{\nu + \delta}$, while the LHS $\sum_\nu \sum_{\sigma \in W} \sum_{\tau \in W} sn(\sigma \tau) p(\nu - (\sigma(\lambda' + \delta) - \delta) - (\tau(\lambda' + \delta) - \delta)) \epsilon_{\nu + \delta}$.
But note that if $\lambda$ is not dominant, $n(\lambda)$ is 0.
In this view we focus on the case for $\nu$ in the RHS dominant.
Then $\sigma^{-1}(\nu + \delta) - \delta$ is dominant if and only if $\sigma = 1$, so in the RHS all terms are terminated except for $\sigma = 1$.
Therefore, we conclude that
\begin{displaymath}
  n(\lambda) = \sum_{\sigma \in W} \sum_{\tau \in W} sn(\sigma \tau) p(\lambda - (\sigma(\lambda' + \delta) - \delta) - (\tau(\lambda' + \delta) - \delta)).
\
\end{displaymath}
We call this formula \textbf{Steinberg's formula}.

\newpage

\part{Cohomology groups of Lie algebras}

\newpage

\textbf{Cohomology groups of Lie algebras}

 Let $\lie{g}$ be a Lie algebra and $\rho : \lie{g} \to \lie{gl}{V}$ be a representation.
For $r \in \NaN$, we call a skew-symmetric mapping of $\lie{g} \times \lie{g} \times \cdots \cdots \lie{g}$ ($r$-times) into $V$ an \textbf{$r$-dimensional $V$-cochain of $\lie{g}$} (or just \textbf{$r$-cochain}) for $\lie{g}$ and for $r = 0$ we call a 'constant' mapping $\lie{g} \ni X \mapsto v$, $v \in V$ fixed, \textbf{$0$-dimensional $V$-cochain of $\lie{g}$} (or just \textbf{$0$-cochain}) for $\lie{g}$.
Also for each $r$, we denote the set of $r$-cochains for $\lie{g}$ by $C^r(\lie{g}; V)$, and we give a natural vector space structure to $C^r(\lie{g}; V)$.

Now for each $r$ we define a mapping $\delta_r : C^r(\lie{g}; V) \to C^{r + 1}(\lie{g}; V)$ as
\begin{eqnarray*}
  & & (\delta_i f)(X_0, X_1, \cdots, X_r) = \sum_i (-1)^i \rho(X_i) f(X_0, X_1, \cdots, \hat{X_i}, \cdots, X_r) + \\
  & & \;\;\;\; \sum_{i < j} (-1)^{i + j} f([X_i, X_j], X_0, X_1, \cdots, \hat{X_i}, \cdots, \hat{X_j}, \cdots, X_r).
\end{eqnarray*}
($\hat{\;}$ means that this is omitted.)
This mappimg is obviously a linear mapping.
If there is no ambiguity, we denote frequently $\delta_i$ by just $\delta$; for example, we denote $\delta_{i + 1} \circ \delta_i$ by $\delta \circ \delta$, or $\delta^2$.
There is an important result: \textit{for any case $\delta^2 = 0$}, where the proof is straightforward but dizzy.
From this we can say about \textbf{cohomology}.

Now, we call $f \in C^r(\lie{g}; V)$ an \textbf{$r$-cocycle} if $\delta f = 0$, and call $g \in C^r(\lie{g}; V)$ ($r > 0$) an \textbf{$r$-coboundary} if there is $h \in C^{r - 1}(\lie{g}; V)$ such that $g = \delta h$.
We denote the set of all $r$-cocycles by $Z^r(\lie{g}; V)$ and the set of all $r$-coboundaries by $B^r(\lie{g}; V)$ for $r > 0$, and $B^0(\lie{g}; V) = 0$.
Then by definition $Z^r(\lie{g}; V) = \ker{\delta_r}$ and $B^r(\lie{g}; V) = \textrm{Im } \delta_{r - 1}$ if $r > 0$.

Since $\delta^2 = 0$, we have that $B^r(\lie{g}; V) \le Z^r(\lie{g}; V)$.
Then we can define $H^r(\lie{g}; V) = Z^r(\lie{g}; V) / B^r(\lie{g}; V)$, which is called the \textbf{$r$-dimensional cohomology group of $\lie{g}$ relative to $V$}.
Note that $H^0(\lie{g}; V) \cong Z^0(\lie{g}; V)$ can be considered as $I_\lie{g}(V) = \{v \in V \SBar \rho(\lie{g}) v = 0\}$, of which each of elements is called an \textbf{invariant}.
Also note that by skew-symmetry $C^r(\lie{g}; V) = 0$ for any $r > n = \dim{\lie{g}}$, so $H^r(\lie{g}; V) = 0$ for all $r > n$.

\newpage

\textbf{Whitehead's theorem}

In the following we focus on the cohomology of a given finite-dimensional semisimple Lie algebra $\lie{g}$ with an finite-dimensional irreducible representation $\rho : \lie{g} \to \lie{gl}(V)$.
We set two cases for this subject; $\rho$ is non-trivial or trivial.
One might guess that the latter is easier than the former, but actually it does not; we have a theorem that \textit{if $\rho(\lie{g})V \ne 0$, then for all $r \in \NaN_0$ $H^r(\lie{g}; V) = 0$}, called the \textbf{Whitehead's theorem}.
It does not harm any generality to assume that $\rho$ is faithful.
Since for $v \in V$ $\rho(\lie{g})v = 0$ implies $v = 0$ by irreducibility, $H^0(\lie{g}; V) = 0$.
Thus it suffices to show that for each $f \in Z^r(\lie{g}; V)$ ($r > 0$) we can find $g \in C^{r - 1}(\lie{g}; V)$ such that $f = \delta g$.

To find such $g$ we let $(U_i)$ be a basis of $\lie{g}$ and $(U'_i)$ the dual basis of $(U_i)$ relative to the Killing form $\kappa_\lie{g}$ of $\lie{g}$, and $C = \sum_i \rho(U_i) \rho(U'_i)$, the Casimir element, which is in this case invertible.
Also, for $A \in \lie{g}$ we define $a_{ij}(A)$ as $[U_i, A] = \sum_j a_{ij}(A) U_j$ and $b_{ij}(A)$ as $[U'_i, A] = \sum_j a_{ij}(A) U'_j$.
Remind that $b_{ij}(A) = -a_{ji}(A)$ for any $A \in \lie{g}$.
Then, (in this moment we denote $\rho(A)v$ for $A \in \lie{g}$ and $v \in V$ by just $Av$)

{\small
\begin{eqnarray*}
  0 &=& \sum_s U_s (\delta f)(X_0, X_1, \cdots, X_{r - 1}, U'_s) \;\;\; (\delta f = 0) \\
  &=& \sum_s U_s \left( \sum_{i = 0}^{r - 1} (-1)^i X_i f(X_0, \cdots, \hat{X_i}, \cdots, X_{r - 1}, U'_s) + (-1)^r U'_s f(X_0, \cdots, X_{r - 1}) + \right. \\
  & & \left. \sum_{i < j < r} (-1)^{i + j} f([X_i, X_j], X_0, \cdots, \hat{X_i}, \cdots, \hat{X_j}, \cdots, X_{r - 1}, U'_s) + \right. \\
  & & \left. \sum_{i = 0}^{r - 1} (-1)^{i + r} f([X_i, U'_s], X_0, \cdots, \hat{X_i}, \cdots, X_{r - 1}) \right).
\end{eqnarray*}
}

The second term will be $(-1)^r Cf(X_1, \cdots, X_r)$, and the remaining will be 
{\small
\begin{eqnarray*}
  & & \sum_{i = 0}^{r - 1} (-1)^i \sum_s (X_i U_s + [U_s, X_i]) f(X_0, \cdots, \hat{X_i}, \cdots, X_{r - 1}, U'_s) + \\
  & & \sum_{i < j < r} (-1)^{i + j} \sum_s U_s f([X_i, X_j], X_0, \cdots, \hat{X_i}, \cdots, \hat{X_j}, \cdots, X_{r - 1}, U'_s) + \\
  & & \sum_{i = 0}^{r - 1} (-1)^i \sum_s \sum_t b_{st}(X_i) U_s f(X_0, \cdots, \hat{X_i}, \cdots, X_{r - 1}, U'_t) \\
  &=& \sum_{i = 0}^{r - 1} (-1)^i X_i \left( \sum_s U_s f(X_0, \cdots, \hat{X_i}, \cdots, X_{r - 1}, U'_s) \right) + \\ 
  & & \sum_{i < j} (-1)^{i + j} \left( \sum_s U_s f([X_i, X_j], X_0, \cdots, \hat{X_i}, \cdots, \hat{X_j}, \cdots, X_{r - 1}, U'_s) \right). \\
\end{eqnarray*}
}

Now if $g(X_0, \cdots, X_{r - 1}) = (-1)^{r + 1} C^{-1} \sum_s U_s f(X_0, \cdots, X_{r - 1}, U'_s)$, then we have that $g \in C^{r - 1}(\lie{g}; V)$ and $f = \delta g$, which we seek.

\end{document}


